{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Calculus for Machine Learning\n",
    "\n",
    "**Course:** Mathematics for Machine Learning  \n",
    "**Instructor:** Mohammed Alnemari  \n",
    "**Lecture 5**\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook covers the core vector calculus concepts used in machine learning:\n",
    "\n",
    "1. **Numerical Differentiation** - Finite differences and derivative approximation\n",
    "2. **Partial Derivatives and Gradients** - Gradient fields and visualization\n",
    "3. **Jacobians** - Derivatives of vector-valued functions\n",
    "4. **Gradient Descent** - Optimization from scratch\n",
    "5. **Chain Rule and Backpropagation** - The engine behind deep learning\n",
    "6. **Hessian and Second-Order Information** - Newton's method\n",
    "\n",
    "---\n",
    "\n",
    "## Google Colab Ready!\n",
    "\n",
    "This notebook works perfectly in Google Colab. All required libraries are pre-installed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set plotting style\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Numerical Differentiation\n",
    "\n",
    "The derivative of a function $f(x)$ is defined as:\n",
    "\n",
    "$$f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "When we cannot compute derivatives analytically, we approximate them using **finite differences**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Forward and Central Differences\n",
    "\n",
    "- **Forward difference:** $f'(x) \\approx \\frac{f(x+h) - f(x)}{h}$ (first-order accurate)\n",
    "- **Central difference:** $f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}$ (second-order accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_difference(f, x, h=1e-5):\n",
    "    \"\"\"Compute derivative using forward difference.\"\"\"\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def central_difference(f, x, h=1e-5):\n",
    "    \"\"\"Compute derivative using central difference.\"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Define a test function: f(x) = sin(x)\n",
    "def f(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "# Analytical derivative: f'(x) = cos(x)\n",
    "def f_prime_exact(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "# Test at x = pi/4\n",
    "x0 = np.pi / 4\n",
    "exact = f_prime_exact(x0)\n",
    "fwd = forward_difference(f, x0)\n",
    "ctr = central_difference(f, x0)\n",
    "\n",
    "print(f\"Derivative of sin(x) at x = pi/4\")\n",
    "print(f\"{'Method':<20} {'Value':<20} {'Error':<20}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Exact':<20} {exact:<20.12f} {'---':<20}\")\n",
    "print(f\"{'Forward diff':<20} {fwd:<20.12f} {abs(fwd - exact):<20.2e}\")\n",
    "print(f\"{'Central diff':<20} {ctr:<20.12f} {abs(ctr - exact):<20.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Effect of Step Size $h$\n",
    "\n",
    "Choosing the right step size $h$ is important. Too large leads to truncation error; too small leads to floating-point error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study how error changes with h\n",
    "h_values = np.logspace(-15, 0, 100)\n",
    "x0 = 1.0\n",
    "exact = f_prime_exact(x0)\n",
    "\n",
    "fwd_errors = [abs(forward_difference(f, x0, h) - exact) for h in h_values]\n",
    "ctr_errors = [abs(central_difference(f, x0, h) - exact) for h in h_values]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(h_values, fwd_errors, 'b-', label='Forward difference', linewidth=2)\n",
    "plt.loglog(h_values, ctr_errors, 'r-', label='Central difference', linewidth=2)\n",
    "plt.xlabel('Step size h')\n",
    "plt.ylabel('Absolute error')\n",
    "plt.title('Error vs Step Size for Numerical Differentiation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Plotting Function and Its Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more interesting function: f(x) = x^3 - 3x^2 + 2x\n",
    "def g(x):\n",
    "    return x**3 - 3*x**2 + 2*x\n",
    "\n",
    "def g_prime_exact(x):\n",
    "    return 3*x**2 - 6*x + 2\n",
    "\n",
    "x = np.linspace(-1, 4, 300)\n",
    "y = g(x)\n",
    "dy_exact = g_prime_exact(x)\n",
    "dy_numerical = central_difference(g, x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot function\n",
    "axes[0].plot(x, y, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].set_title(r'$f(x) = x^3 - 3x^2 + 2x$')\n",
    "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot derivatives\n",
    "axes[1].plot(x, dy_exact, 'b-', linewidth=2, label='Analytical')\n",
    "axes[1].plot(x[::10], dy_numerical[::10], 'ro', markersize=5, label='Numerical (central)')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel(\"f'(x)\")\n",
    "axes[1].set_title('Derivative Comparison')\n",
    "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Partial Derivatives and Gradients\n",
    "\n",
    "For a function $f(x, y)$, the **gradient** is the vector of partial derivatives:\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$\n",
    "\n",
    "The gradient points in the direction of **steepest ascent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Computing Partial Derivatives Numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_derivative(f, point, var_index, h=1e-5):\n",
    "    \"\"\"Compute partial derivative of f with respect to variable var_index at given point.\"\"\"\n",
    "    point = np.array(point, dtype=float)\n",
    "    point_forward = point.copy()\n",
    "    point_backward = point.copy()\n",
    "    point_forward[var_index] += h\n",
    "    point_backward[var_index] -= h\n",
    "    return (f(point_forward) - f(point_backward)) / (2 * h)\n",
    "\n",
    "def numerical_gradient(f, point, h=1e-5):\n",
    "    \"\"\"Compute gradient of f at given point.\"\"\"\n",
    "    point = np.array(point, dtype=float)\n",
    "    grad = np.zeros_like(point)\n",
    "    for i in range(len(point)):\n",
    "        grad[i] = partial_derivative(f, point, i, h)\n",
    "    return grad\n",
    "\n",
    "# Example: f(x, y) = x^2 + y^2\n",
    "def f_simple(point):\n",
    "    x, y = point\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Analytical gradient: grad f = [2x, 2y]\n",
    "test_point = [3.0, 4.0]\n",
    "numerical_grad = numerical_gradient(f_simple, test_point)\n",
    "analytical_grad = np.array([2 * test_point[0], 2 * test_point[1]])\n",
    "\n",
    "print(f\"f(x, y) = x^2 + y^2 at point ({test_point[0]}, {test_point[1]})\")\n",
    "print(f\"Numerical gradient:  {numerical_grad}\")\n",
    "print(f\"Analytical gradient: {analytical_grad}\")\n",
    "print(f\"Difference: {np.linalg.norm(numerical_grad - analytical_grad):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Gradient of $f(x, y) = x^2 + y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate gradient at several points\n",
    "points = [[1, 0], [0, 1], [-1, 0], [0, -1], [1, 1], [-2, 3]]\n",
    "\n",
    "print(f\"{'Point':<15} {'Gradient':<20} {'Magnitude':<15}\")\n",
    "print(f\"{'-'*50}\")\n",
    "for p in points:\n",
    "    grad = numerical_gradient(f_simple, p)\n",
    "    mag = np.linalg.norm(grad)\n",
    "    print(f\"({p[0]:>3}, {p[1]:>3})     [{grad[0]:>6.2f}, {grad[1]:>6.2f}]     {mag:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Visualizing the Gradient Field with a Quiver Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid\n",
    "x_range = np.linspace(-3, 3, 15)\n",
    "y_range = np.linspace(-3, 3, 15)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# Compute gradient at each grid point\n",
    "# For f(x,y) = x^2 + y^2, grad = [2x, 2y]\n",
    "U = 2 * X  # df/dx\n",
    "V = 2 * Y  # df/dy\n",
    "\n",
    "# Compute function values for contour\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Contour plot with gradient arrows\n",
    "contour = axes[0].contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "axes[0].clabel(contour, inline=True, fontsize=8)\n",
    "axes[0].quiver(X, Y, U, V, color='red', alpha=0.7)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title(r'Gradient field of $f(x,y) = x^2 + y^2$')\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Normalized gradient (direction only)\n",
    "magnitude = np.sqrt(U**2 + V**2)\n",
    "magnitude[magnitude == 0] = 1  # avoid division by zero\n",
    "U_norm = U / magnitude\n",
    "V_norm = V / magnitude\n",
    "\n",
    "contour2 = axes[1].contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(contour2, ax=axes[1], label='f(x, y)')\n",
    "axes[1].quiver(X, Y, U_norm, V_norm, color='white', alpha=0.8)\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Normalized gradient (direction only)')\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Jacobians\n",
    "\n",
    "For a vector-valued function $\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m$, the **Jacobian** is the $m \\times n$ matrix of partial derivatives:\n",
    "\n",
    "$$J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}$$\n",
    "\n",
    "The Jacobian generalizes the gradient to vector-valued functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Computing the Jacobian Numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_jacobian(f, point, h=1e-5):\n",
    "    \"\"\"Compute Jacobian matrix of vector-valued function f at given point.\"\"\"\n",
    "    point = np.array(point, dtype=float)\n",
    "    f0 = np.atleast_1d(f(point))\n",
    "    n = len(point)\n",
    "    m = len(f0)\n",
    "    J = np.zeros((m, n))\n",
    "    \n",
    "    for j in range(n):\n",
    "        point_fwd = point.copy()\n",
    "        point_bwd = point.copy()\n",
    "        point_fwd[j] += h\n",
    "        point_bwd[j] -= h\n",
    "        J[:, j] = (f(point_fwd) - f(point_bwd)) / (2 * h)\n",
    "    \n",
    "    return J\n",
    "\n",
    "# Example: f(x, y) = [x^2*y, 5x + sin(y)]\n",
    "def vector_func(point):\n",
    "    x, y = point\n",
    "    return np.array([x**2 * y, 5*x + np.sin(y)])\n",
    "\n",
    "# Analytical Jacobian:\n",
    "# J = [[2xy,  x^2],\n",
    "#      [5,    cos(y)]]\n",
    "def analytical_jacobian(point):\n",
    "    x, y = point\n",
    "    return np.array([\n",
    "        [2*x*y, x**2],\n",
    "        [5.0,   np.cos(y)]\n",
    "    ])\n",
    "\n",
    "test_point = [1.0, 2.0]\n",
    "J_num = numerical_jacobian(vector_func, test_point)\n",
    "J_exact = analytical_jacobian(test_point)\n",
    "\n",
    "print(\"f(x, y) = [x^2 * y,  5x + sin(y)]\")\n",
    "print(f\"\\nAt point ({test_point[0]}, {test_point[1]}):\\n\")\n",
    "print(\"Numerical Jacobian:\")\n",
    "print(J_num)\n",
    "print(\"\\nAnalytical Jacobian:\")\n",
    "print(J_exact)\n",
    "print(f\"\\nMax absolute error: {np.max(np.abs(J_num - J_exact)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Verifying with SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import approx_fprime\n",
    "\n",
    "# SciPy's approx_fprime computes gradient (one row of Jacobian for scalar functions)\n",
    "# For vector functions, we compute row by row\n",
    "\n",
    "def f1(point):\n",
    "    x, y = point\n",
    "    return x**2 * y\n",
    "\n",
    "def f2(point):\n",
    "    x, y = point\n",
    "    return 5*x + np.sin(y)\n",
    "\n",
    "test_point = np.array([1.0, 2.0])\n",
    "eps = np.sqrt(np.finfo(float).eps)\n",
    "\n",
    "# Compute each row of the Jacobian using SciPy\n",
    "row1 = approx_fprime(test_point, f1, eps)\n",
    "row2 = approx_fprime(test_point, f2, eps)\n",
    "J_scipy = np.array([row1, row2])\n",
    "\n",
    "print(\"SciPy Jacobian:\")\n",
    "print(J_scipy)\n",
    "print(\"\\nOur numerical Jacobian:\")\n",
    "print(J_num)\n",
    "print(f\"\\nMax difference: {np.max(np.abs(J_scipy - J_num)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Gradient Descent\n",
    "\n",
    "**Gradient descent** is the workhorse of machine learning optimization. The update rule is:\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)$$\n",
    "\n",
    "where $\\alpha$ is the **learning rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Gradient Descent from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad_f, x0, learning_rate=0.1, n_iters=100, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to minimize f.\n",
    "    \n",
    "    Parameters:\n",
    "        f:             objective function\n",
    "        grad_f:        gradient of f\n",
    "        x0:            initial point\n",
    "        learning_rate: step size\n",
    "        n_iters:       maximum iterations\n",
    "        tol:           convergence tolerance\n",
    "    \n",
    "    Returns:\n",
    "        path: list of points visited\n",
    "        values: list of function values\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    path = [x.copy()]\n",
    "    values = [f(x)]\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        grad = grad_f(x)\n",
    "        x = x - learning_rate * grad\n",
    "        path.append(x.copy())\n",
    "        values.append(f(x))\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f\"Converged at iteration {i+1}\")\n",
    "            break\n",
    "    \n",
    "    return np.array(path), np.array(values)\n",
    "\n",
    "print(\"Gradient descent function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Minimizing $f(x,y) = x^2 + y^2$ (Simple Quadratic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple quadratic\n",
    "def quadratic(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def grad_quadratic(x):\n",
    "    return np.array([2*x[0], 2*x[1]])\n",
    "\n",
    "# Run gradient descent\n",
    "x0 = [4.0, 3.0]\n",
    "path, values = gradient_descent(quadratic, grad_quadratic, x0, learning_rate=0.1, n_iters=50)\n",
    "\n",
    "print(f\"\\nStarting point: ({x0[0]}, {x0[1]}), f = {quadratic(x0):.4f}\")\n",
    "print(f\"Final point:    ({path[-1][0]:.6f}, {path[-1][1]:.6f}), f = {values[-1]:.8f}\")\n",
    "print(f\"Steps taken:    {len(path) - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize descent path on contour plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Contour plot with path\n",
    "x_range = np.linspace(-5, 5, 100)\n",
    "y_range = np.linspace(-4, 4, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "axes[0].contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.7)\n",
    "axes[0].plot(path[:, 0], path[:, 1], 'ro-', markersize=4, linewidth=1.5, label='GD path')\n",
    "axes[0].plot(path[0, 0], path[0, 1], 'rs', markersize=12, label='Start')\n",
    "axes[0].plot(path[-1, 0], path[-1, 1], 'r*', markersize=15, label='End')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title(r'Gradient Descent on $f(x,y) = x^2 + y^2$')\n",
    "axes[0].legend()\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Convergence plot\n",
    "axes[1].semilogy(values, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('f(x, y) (log scale)')\n",
    "axes[1].set_title('Convergence of Gradient Descent')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Minimizing the Rosenbrock Function (Harder)\n",
    "\n",
    "The Rosenbrock function is a classic test for optimization:\n",
    "\n",
    "$$f(x, y) = (1 - x)^2 + 100(y - x^2)^2$$\n",
    "\n",
    "Minimum is at $(1, 1)$ with $f(1, 1) = 0$. The function has a narrow curved valley that makes optimization difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "def grad_rosenbrock(x):\n",
    "    dfdx = -2*(1 - x[0]) + 100 * 2*(x[1] - x[0]**2) * (-2*x[0])\n",
    "    dfdy = 100 * 2*(x[1] - x[0]**2)\n",
    "    return np.array([dfdx, dfdy])\n",
    "\n",
    "# Gradient descent with small learning rate (Rosenbrock needs careful tuning)\n",
    "x0 = [-1.0, 1.0]\n",
    "path_rosen, values_rosen = gradient_descent(\n",
    "    rosenbrock, grad_rosenbrock, x0, \n",
    "    learning_rate=0.001, n_iters=10000, tol=1e-8\n",
    ")\n",
    "\n",
    "print(f\"Starting point: ({x0[0]}, {x0[1]}), f = {rosenbrock(x0):.4f}\")\n",
    "print(f\"Final point:    ({path_rosen[-1][0]:.6f}, {path_rosen[-1][1]:.6f})\")\n",
    "print(f\"Final value:    f = {values_rosen[-1]:.8f}\")\n",
    "print(f\"Steps taken:    {len(path_rosen) - 1}\")\n",
    "print(f\"Known minimum:  (1.0, 1.0), f = 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Rosenbrock descent\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Contour plot\n",
    "x_range = np.linspace(-2, 2, 200)\n",
    "y_range = np.linspace(-1, 3, 200)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = (1 - X)**2 + 100*(Y - X**2)**2\n",
    "\n",
    "axes[0].contourf(X, Y, np.log10(Z + 1), levels=30, cmap='viridis', alpha=0.7)\n",
    "# Plot every 50th point to avoid clutter\n",
    "step = max(1, len(path_rosen) // 200)\n",
    "axes[0].plot(path_rosen[::step, 0], path_rosen[::step, 1], 'r.-', \n",
    "             markersize=2, linewidth=0.5, alpha=0.8, label='GD path')\n",
    "axes[0].plot(path_rosen[0, 0], path_rosen[0, 1], 'rs', markersize=10, label='Start')\n",
    "axes[0].plot(1, 1, 'g*', markersize=15, label='Global min (1,1)')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Gradient Descent on Rosenbrock Function')\n",
    "axes[0].legend()\n",
    "\n",
    "# Convergence\n",
    "axes[1].semilogy(values_rosen, 'b-', linewidth=1)\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('f(x, y) (log scale)')\n",
    "axes[1].set_title('Convergence on Rosenbrock')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Effect of Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates on the quadratic\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9]\n",
    "x0 = [4.0, 3.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Contour background\n",
    "x_range = np.linspace(-5, 5, 100)\n",
    "y_range = np.linspace(-4, 4, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = X**2 + Y**2\n",
    "axes[0].contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    path_lr, values_lr = gradient_descent(quadratic, grad_quadratic, x0, \n",
    "                                          learning_rate=lr, n_iters=30)\n",
    "    axes[0].plot(path_lr[:, 0], path_lr[:, 1], 'o-', color=color, \n",
    "                 markersize=4, linewidth=1.5, label=f'lr={lr}')\n",
    "    axes[1].semilogy(values_lr, '-', color=color, linewidth=2, label=f'lr={lr}')\n",
    "\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('GD Paths for Different Learning Rates')\n",
    "axes[0].legend()\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('f(x, y) (log scale)')\n",
    "axes[1].set_title('Convergence for Different Learning Rates')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Chain Rule and Backpropagation\n",
    "\n",
    "The **chain rule** is the foundation of backpropagation in neural networks:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "In a computation graph, we propagate gradients backward from the loss to all parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Simple Computation Graph Example\n",
    "\n",
    "Consider the computation: $f(x, y, z) = (x + y) \\cdot z$\n",
    "\n",
    "Let $q = x + y$, then $f = q \\cdot z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x, y, z = 2.0, 3.0, -4.0\n",
    "\n",
    "# Step 1: q = x + y\n",
    "q = x + y\n",
    "print(f\"Forward pass:\")\n",
    "print(f\"  q = x + y = {x} + {y} = {q}\")\n",
    "\n",
    "# Step 2: f = q * z\n",
    "f_val = q * z\n",
    "print(f\"  f = q * z = {q} * {z} = {f_val}\")\n",
    "\n",
    "# Backward pass (backpropagation)\n",
    "print(f\"\\nBackward pass:\")\n",
    "\n",
    "# df/df = 1 (seed gradient)\n",
    "df_df = 1.0\n",
    "print(f\"  df/df = {df_df}\")\n",
    "\n",
    "# df/dq = z, df/dz = q\n",
    "df_dq = z * df_df\n",
    "df_dz = q * df_df\n",
    "print(f\"  df/dq = z = {df_dq}\")\n",
    "print(f\"  df/dz = q = {df_dz}\")\n",
    "\n",
    "# dq/dx = 1, dq/dy = 1\n",
    "# By chain rule: df/dx = df/dq * dq/dx\n",
    "df_dx = df_dq * 1.0\n",
    "df_dy = df_dq * 1.0\n",
    "print(f\"  df/dx = df/dq * dq/dx = {df_dq} * 1 = {df_dx}\")\n",
    "print(f\"  df/dy = df/dq * dq/dy = {df_dq} * 1 = {df_dy}\")\n",
    "\n",
    "# Verify numerically\n",
    "h = 1e-5\n",
    "df_dx_num = ((x+h + y)*z - (x + y)*z) / h\n",
    "df_dy_num = ((x + y+h)*z - (x + y)*z) / h\n",
    "df_dz_num = ((x + y)*(z+h) - (x + y)*z) / h\n",
    "\n",
    "print(f\"\\nNumerical verification:\")\n",
    "print(f\"  df/dx = {df_dx_num:.6f} (analytical: {df_dx})\")\n",
    "print(f\"  df/dy = {df_dy_num:.6f} (analytical: {df_dy})\")\n",
    "print(f\"  df/dz = {df_dz_num:.6f} (analytical: {df_dz})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Backpropagation for a 2-Layer Neural Network\n",
    "\n",
    "We implement a simple 2-layer network for binary classification:\n",
    "\n",
    "- **Forward:** $z_1 = W_1 x + b_1$, $a_1 = \\sigma(z_1)$, $z_2 = W_2 a_1 + b_2$, $\\hat{y} = \\sigma(z_2)$\n",
    "- **Loss:** $L = -(y \\log \\hat{y} + (1-y)\\log(1 - \\hat{y}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid: sigma(z) * (1 - sigma(z)).\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"Initialize a 2-layer neural network.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.randn(hidden_dim, input_dim) * 0.5\n",
    "        self.b1 = np.zeros((hidden_dim, 1))\n",
    "        self.W2 = np.random.randn(output_dim, hidden_dim) * 0.5\n",
    "        self.b2 = np.zeros((output_dim, 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass. X shape: (input_dim, n_samples).\"\"\"\n",
    "        self.X = X\n",
    "        self.z1 = self.W1 @ X + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = self.W2 @ self.a1 + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Binary cross-entropy loss.\"\"\"\n",
    "        m = y_true.shape[1]\n",
    "        eps = 1e-8\n",
    "        loss = -np.mean(y_true * np.log(y_pred + eps) + \n",
    "                        (1 - y_true) * np.log(1 - y_pred + eps))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"Backward pass (backpropagation).\"\"\"\n",
    "        m = y_true.shape[1]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y_true                     # (output_dim, m)\n",
    "        self.dW2 = (1/m) * dz2 @ self.a1.T         # (output_dim, hidden_dim)\n",
    "        self.db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradients (chain rule)\n",
    "        da1 = self.W2.T @ dz2                      # (hidden_dim, m)\n",
    "        dz1 = da1 * sigmoid_derivative(self.z1)     # element-wise\n",
    "        self.dW1 = (1/m) * dz1 @ self.X.T          # (hidden_dim, input_dim)\n",
    "        self.db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"Update parameters using gradient descent.\"\"\"\n",
    "        self.W1 -= learning_rate * self.dW1\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "        self.W2 -= learning_rate * self.dW2\n",
    "        self.b2 -= learning_rate * self.db2\n",
    "\n",
    "print(\"TwoLayerNet class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple 2D dataset (two concentric circles)\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Class 0: inner circle\n",
    "r0 = np.random.randn(n_samples // 2) * 0.3 + 1.0\n",
    "theta0 = np.random.uniform(0, 2*np.pi, n_samples // 2)\n",
    "X0 = np.column_stack([r0 * np.cos(theta0), r0 * np.sin(theta0)])\n",
    "\n",
    "# Class 1: outer circle\n",
    "r1 = np.random.randn(n_samples // 2) * 0.3 + 2.5\n",
    "theta1 = np.random.uniform(0, 2*np.pi, n_samples // 2)\n",
    "X1 = np.column_stack([r1 * np.cos(theta1), r1 * np.sin(theta1)])\n",
    "\n",
    "X = np.vstack([X0, X1]).T  # shape: (2, n_samples)\n",
    "y = np.hstack([np.zeros(n_samples // 2), np.ones(n_samples // 2)]).reshape(1, -1)\n",
    "\n",
    "# Train the network\n",
    "net = TwoLayerNet(input_dim=2, hidden_dim=8, output_dim=1)\n",
    "losses = []\n",
    "n_epochs = 2000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    y_pred = net.forward(X)\n",
    "    loss = net.compute_loss(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward\n",
    "    net.backward(y)\n",
    "    \n",
    "    # Update\n",
    "    net.update(learning_rate=1.0)\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        acc = np.mean((y_pred > 0.5).astype(float) == y) * 100\n",
    "        print(f\"Epoch {epoch+1:4d}: loss = {loss:.4f}, accuracy = {acc:.1f}%\")\n",
    "\n",
    "# Final accuracy\n",
    "y_pred_final = net.forward(X)\n",
    "accuracy = np.mean((y_pred_final > 0.5).astype(float) == y) * 100\n",
    "print(f\"\\nFinal accuracy: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Training data\n",
    "axes[0].scatter(X[0, y[0]==0], X[1, y[0]==0], c='blue', label='Class 0', alpha=0.6)\n",
    "axes[0].scatter(X[0, y[0]==1], X[1, y[0]==1], c='red', label='Class 1', alpha=0.6)\n",
    "axes[0].set_xlabel('x1')\n",
    "axes[0].set_ylabel('x2')\n",
    "axes[0].set_title('Training Data')\n",
    "axes[0].legend()\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary\n",
    "xx = np.linspace(-4, 4, 200)\n",
    "yy = np.linspace(-4, 4, 200)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "grid_points = np.vstack([XX.ravel(), YY.ravel()])\n",
    "ZZ = net.forward(grid_points).reshape(XX.shape)\n",
    "\n",
    "axes[1].contourf(XX, YY, ZZ, levels=20, cmap='RdBu_r', alpha=0.7)\n",
    "axes[1].contour(XX, YY, ZZ, levels=[0.5], colors='black', linewidths=2)\n",
    "axes[1].scatter(X[0, y[0]==0], X[1, y[0]==0], c='blue', edgecolors='k', s=20)\n",
    "axes[1].scatter(X[0, y[0]==1], X[1, y[0]==1], c='red', edgecolors='k', s=20)\n",
    "axes[1].set_xlabel('x1')\n",
    "axes[1].set_ylabel('x2')\n",
    "axes[1].set_title('Decision Boundary')\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "# Loss curve\n",
    "axes[2].plot(losses, 'b-', linewidth=1.5)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_title('Training Loss')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Hessian and Second-Order Information\n",
    "\n",
    "The **Hessian matrix** contains all second-order partial derivatives:\n",
    "\n",
    "$$H = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix}$$\n",
    "\n",
    "The Hessian tells us about the **curvature** of the function and enables second-order optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Computing the Hessian Numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_hessian(f, point, h=1e-5):\n",
    "    \"\"\"Compute the Hessian matrix numerically using central differences.\"\"\"\n",
    "    point = np.array(point, dtype=float)\n",
    "    n = len(point)\n",
    "    H = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Use central difference for second derivatives\n",
    "            ei = np.zeros(n)\n",
    "            ej = np.zeros(n)\n",
    "            ei[i] = h\n",
    "            ej[j] = h\n",
    "            \n",
    "            H[i, j] = (\n",
    "                f(point + ei + ej) \n",
    "                - f(point + ei - ej) \n",
    "                - f(point - ei + ej) \n",
    "                + f(point - ei - ej)\n",
    "            ) / (4 * h * h)\n",
    "    \n",
    "    return H\n",
    "\n",
    "# Example: f(x, y) = x^4 + x^2*y + y^2\n",
    "def f_hess(point):\n",
    "    x, y = point\n",
    "    return x**4 + x**2 * y + y**2\n",
    "\n",
    "# Analytical Hessian:\n",
    "# H = [[12x^2 + 2y, 2x],\n",
    "#      [2x,          2 ]]\n",
    "def analytical_hessian(point):\n",
    "    x, y = point\n",
    "    return np.array([\n",
    "        [12*x**2 + 2*y, 2*x],\n",
    "        [2*x,           2.0]\n",
    "    ])\n",
    "\n",
    "test_point = [1.0, 2.0]\n",
    "H_num = numerical_hessian(f_hess, test_point)\n",
    "H_exact = analytical_hessian(test_point)\n",
    "\n",
    "print(f\"f(x, y) = x^4 + x^2*y + y^2\")\n",
    "print(f\"At point ({test_point[0]}, {test_point[1]}):\\n\")\n",
    "print(\"Numerical Hessian:\")\n",
    "print(H_num)\n",
    "print(\"\\nAnalytical Hessian:\")\n",
    "print(H_exact)\n",
    "print(f\"\\nMax absolute error: {np.max(np.abs(H_num - H_exact)):.2e}\")\n",
    "\n",
    "# Eigenvalues tell us about curvature\n",
    "eigenvalues = np.linalg.eigvals(H_exact)\n",
    "print(f\"\\nEigenvalues of Hessian: {eigenvalues}\")\n",
    "print(f\"All positive (convex at this point): {np.all(eigenvalues > 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Newton's Method vs Gradient Descent\n",
    "\n",
    "**Newton's method** uses the Hessian to take better steps:\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_k - H^{-1} \\nabla f(\\mathbf{x}_k)$$\n",
    "\n",
    "It converges much faster (quadratically) for well-behaved functions, but each step is more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_method(f, grad_f, hess_f, x0, n_iters=50, tol=1e-10):\n",
    "    \"\"\"Newton's method for optimization.\"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    path = [x.copy()]\n",
    "    values = [f(x)]\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        grad = grad_f(x)\n",
    "        hess = hess_f(x)\n",
    "        \n",
    "        # Newton step: x = x - H^{-1} * grad\n",
    "        try:\n",
    "            step = np.linalg.solve(hess, grad)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(f\"Hessian singular at iteration {i+1}\")\n",
    "            break\n",
    "        \n",
    "        x = x - step\n",
    "        path.append(x.copy())\n",
    "        values.append(f(x))\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f\"Converged at iteration {i+1}\")\n",
    "            break\n",
    "    \n",
    "    return np.array(path), np.array(values)\n",
    "\n",
    "print(\"Newton's method function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on a quadratic with different curvatures: f(x,y) = 5x^2 + y^2\n",
    "def f_elliptic(point):\n",
    "    x, y = point\n",
    "    return 5*x**2 + y**2\n",
    "\n",
    "def grad_elliptic(point):\n",
    "    x, y = point\n",
    "    return np.array([10*x, 2*y])\n",
    "\n",
    "def hess_elliptic(point):\n",
    "    return np.array([[10.0, 0.0],\n",
    "                     [0.0,  2.0]])\n",
    "\n",
    "x0 = [4.0, 4.0]\n",
    "\n",
    "# Gradient descent\n",
    "path_gd, values_gd = gradient_descent(\n",
    "    f_elliptic, grad_elliptic, x0, learning_rate=0.08, n_iters=50\n",
    ")\n",
    "\n",
    "# Newton's method\n",
    "path_newton, values_newton = newtons_method(\n",
    "    f_elliptic, grad_elliptic, hess_elliptic, x0, n_iters=50\n",
    ")\n",
    "\n",
    "print(f\"\\nGradient Descent:\")\n",
    "print(f\"  Final point: ({path_gd[-1][0]:.8f}, {path_gd[-1][1]:.8f})\")\n",
    "print(f\"  Final value: {values_gd[-1]:.10f}\")\n",
    "print(f\"  Steps: {len(path_gd) - 1}\")\n",
    "\n",
    "print(f\"\\nNewton's Method:\")\n",
    "print(f\"  Final point: ({path_newton[-1][0]:.8f}, {path_newton[-1][1]:.8f})\")\n",
    "print(f\"  Final value: {values_newton[-1]:.10f}\")\n",
    "print(f\"  Steps: {len(path_newton) - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Contour plot with both paths\n",
    "x_range = np.linspace(-5, 5, 100)\n",
    "y_range = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = 5*X**2 + Y**2\n",
    "\n",
    "axes[0].contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.6)\n",
    "axes[0].contour(X, Y, Z, levels=15, colors='grey', alpha=0.3, linewidths=0.5)\n",
    "\n",
    "axes[0].plot(path_gd[:, 0], path_gd[:, 1], 'ro-', markersize=5, linewidth=1.5, \n",
    "             label=f'Gradient Descent ({len(path_gd)-1} steps)')\n",
    "axes[0].plot(path_newton[:, 0], path_newton[:, 1], 'bs-', markersize=8, linewidth=2, \n",
    "             label=f'Newton ({len(path_newton)-1} steps)')\n",
    "axes[0].plot(0, 0, 'g*', markersize=15, label='Minimum')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title(r'GD vs Newton on $f(x,y) = 5x^2 + y^2$')\n",
    "axes[0].legend()\n",
    "\n",
    "# Convergence comparison\n",
    "axes[1].semilogy(range(len(values_gd)), values_gd, 'r-', linewidth=2, label='Gradient Descent')\n",
    "axes[1].semilogy(range(len(values_newton)), values_newton, 'b-', linewidth=2, label=\"Newton's Method\")\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('f(x, y) (log scale)')\n",
    "axes[1].set_title('Convergence Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 When Does Newton's Method Shine?\n",
    "\n",
    "Newton's method is especially powerful when the Hessian has a large **condition number** (ratio of largest to smallest eigenvalue), which means the function has very different curvatures in different directions. Gradient descent struggles in this case because it oscillates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ill-conditioned quadratic: f(x,y) = 50x^2 + y^2 (condition number = 50)\n",
    "def f_ill(point):\n",
    "    x, y = point\n",
    "    return 50*x**2 + y**2\n",
    "\n",
    "def grad_ill(point):\n",
    "    x, y = point\n",
    "    return np.array([100*x, 2*y])\n",
    "\n",
    "def hess_ill(point):\n",
    "    return np.array([[100.0, 0.0],\n",
    "                     [0.0,   2.0]])\n",
    "\n",
    "x0 = [3.0, 3.0]\n",
    "\n",
    "# Gradient descent (needs small learning rate due to large curvature)\n",
    "path_gd_ill, values_gd_ill = gradient_descent(\n",
    "    f_ill, grad_ill, x0, learning_rate=0.009, n_iters=200\n",
    ")\n",
    "\n",
    "# Newton's method\n",
    "path_newton_ill, values_newton_ill = newtons_method(\n",
    "    f_ill, grad_ill, hess_ill, x0, n_iters=50\n",
    ")\n",
    "\n",
    "print(f\"Condition number of Hessian: {100/2:.0f}\")\n",
    "print(f\"\\nGradient Descent: {len(path_gd_ill)-1} steps, final f = {values_gd_ill[-1]:.6e}\")\n",
    "print(f\"Newton's Method:  {len(path_newton_ill)-1} steps, final f = {values_newton_ill[-1]:.6e}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_range = np.linspace(-4, 4, 100)\n",
    "y_range = np.linspace(-4, 4, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = 50*X**2 + Y**2\n",
    "\n",
    "ax.contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.6)\n",
    "ax.plot(path_gd_ill[:, 0], path_gd_ill[:, 1], 'r.-', markersize=3, linewidth=0.8, \n",
    "        alpha=0.8, label=f'GD ({len(path_gd_ill)-1} steps)')\n",
    "ax.plot(path_newton_ill[:, 0], path_newton_ill[:, 1], 'bs-', markersize=10, linewidth=2, \n",
    "        label=f'Newton ({len(path_newton_ill)-1} steps)')\n",
    "ax.plot(0, 0, 'g*', markersize=15, label='Minimum')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title(r'Ill-conditioned problem: $f(x,y) = 50x^2 + y^2$ (condition = 50)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Practice Exercises\n",
    "\n",
    "Try these on your own:\n",
    "\n",
    "**Exercise 1: Numerical Differentiation**  \n",
    "Compute the derivative of $f(x) = e^{-x^2}$ using both forward and central differences. Compare with the analytical derivative $f'(x) = -2x e^{-x^2}$ at $x = 1$. Plot the function and its derivative over $[-3, 3]$.\n",
    "\n",
    "**Exercise 2: Gradient Visualization**  \n",
    "Compute and visualize the gradient field of $f(x, y) = \\sin(x) \\cdot \\cos(y)$ using a quiver plot on the domain $[-\\pi, \\pi] \\times [-\\pi, \\pi]$. Overlay contour lines on the same plot.\n",
    "\n",
    "**Exercise 3: Gradient Descent with Momentum**  \n",
    "Implement gradient descent **with momentum** using the update rule:  \n",
    "$v_{k+1} = \\beta \\, v_k + \\alpha \\, \\nabla f(x_k)$  \n",
    "$x_{k+1} = x_k - v_{k+1}$  \n",
    "Test it on the Rosenbrock function with $\\alpha = 0.001$ and $\\beta = 0.9$. Compare convergence with standard gradient descent.\n",
    "\n",
    "**Exercise 4: Hessian Analysis**  \n",
    "For the function $f(x, y) = x^3 - 3xy^2$ (the \"monkey saddle\"), compute the Hessian at the origin $(0, 0)$. What do the eigenvalues tell you about the nature of this critical point? Visualize the surface using a 3D plot.\n",
    "\n",
    "---\n",
    "\n",
    "**Course:** Mathematics for Machine Learning  \n",
    "**Instructor:** Mohammed Alnemari"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}