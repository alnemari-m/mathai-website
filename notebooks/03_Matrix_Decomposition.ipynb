{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Decomposition\n",
    "\n",
    "**Course:** Mathematics for Machine Learning  \n",
    "**Instructor:** Mohammed Alnemari  \n",
    "**Lecture 4**\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook covers the essential matrix decomposition techniques used in machine learning:\n",
    "\n",
    "1. **Determinants and Trace** - Fundamental matrix properties\n",
    "2. **Eigenvalues and Eigenvectors** - Understanding matrix transformations\n",
    "3. **Cholesky Decomposition** - Decomposing positive definite matrices\n",
    "4. **Eigendecomposition** - Diagonalization of matrices\n",
    "5. **Singular Value Decomposition (SVD)** - The most general decomposition\n",
    "6. **Matrix Approximation** - Low-rank approximations and the Eckart-Young theorem\n",
    "\n",
    "---\n",
    "\n",
    "## Google Colab Ready!\n",
    "\n",
    "This notebook works perfectly in Google Colab. All required libraries are pre-installed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Determinants and Trace\n",
    "\n",
    "The **determinant** and **trace** are scalar values associated with a square matrix that reveal important properties about the matrix.\n",
    "\n",
    "- **Determinant** $\\det(A)$: Measures the volume scaling factor of the linear transformation\n",
    "- **Trace** $\\text{tr}(A)$: The sum of the diagonal elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Computing Determinants and Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a square matrix\n",
    "A = np.array([[2, 1, 3],\n",
    "              [1, 4, 2],\n",
    "              [3, 2, 5]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "# Compute the determinant\n",
    "det_A = np.linalg.det(A)\n",
    "print(\"Determinant of A:\", det_A)\n",
    "print()\n",
    "\n",
    "# Compute the trace\n",
    "trace_A = np.trace(A)\n",
    "print(\"Trace of A:\", trace_A)\n",
    "print(\"(Sum of diagonal: 2 + 4 + 5 =\", 2 + 4 + 5, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Property: det(AB) = det(A) * det(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two square matrices\n",
    "A = np.array([[2, 1],\n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print()\n",
    "print(\"Matrix B:\")\n",
    "print(B)\n",
    "print()\n",
    "\n",
    "# Compute det(AB)\n",
    "AB = A @ B\n",
    "det_AB = np.linalg.det(AB)\n",
    "print(\"det(AB) =\", det_AB)\n",
    "\n",
    "# Compute det(A) * det(B)\n",
    "det_A = np.linalg.det(A)\n",
    "det_B = np.linalg.det(B)\n",
    "product = det_A * det_B\n",
    "print(\"det(A) * det(B) =\", det_A, \"*\", det_B, \"=\", product)\n",
    "print()\n",
    "\n",
    "# Verify they are equal\n",
    "print(\"Are they equal?\", np.isclose(det_AB, product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Property: tr(AB) = tr(BA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same matrices A and B from above\n",
    "AB = A @ B\n",
    "BA = B @ A\n",
    "\n",
    "print(\"AB:\")\n",
    "print(AB)\n",
    "print()\n",
    "print(\"BA:\")\n",
    "print(BA)\n",
    "print()\n",
    "\n",
    "# Compute traces\n",
    "trace_AB = np.trace(AB)\n",
    "trace_BA = np.trace(BA)\n",
    "\n",
    "print(\"tr(AB) =\", trace_AB)\n",
    "print(\"tr(BA) =\", trace_BA)\n",
    "print()\n",
    "print(\"Are they equal?\", np.isclose(trace_AB, trace_BA))\n",
    "print()\n",
    "\n",
    "# Additional property: trace equals sum of eigenvalues\n",
    "eigenvalues_A = np.linalg.eigvals(A)\n",
    "print(\"Eigenvalues of A:\", eigenvalues_A)\n",
    "print(\"Sum of eigenvalues:\", np.sum(eigenvalues_A).real)\n",
    "print(\"Trace of A:\", np.trace(A))\n",
    "print(\"Are they equal?\", np.isclose(np.sum(eigenvalues_A).real, np.trace(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 More Determinant Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property: det(A^T) = det(A)\n",
    "print(\"det(A) =\", np.linalg.det(A))\n",
    "print(\"det(A^T) =\", np.linalg.det(A.T))\n",
    "print(\"det(A) == det(A^T)?\", np.isclose(np.linalg.det(A), np.linalg.det(A.T)))\n",
    "print()\n",
    "\n",
    "# Property: det(cA) = c^n * det(A) for n x n matrix\n",
    "c = 3\n",
    "n = A.shape[0]\n",
    "print(\"det(3A) =\", np.linalg.det(c * A))\n",
    "print(\"3^2 * det(A) =\", c**n * np.linalg.det(A))\n",
    "print(\"Are they equal?\", np.isclose(np.linalg.det(c * A), c**n * np.linalg.det(A)))\n",
    "print()\n",
    "\n",
    "# Property: det(A^{-1}) = 1 / det(A)\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(\"det(A^{-1}) =\", np.linalg.det(A_inv))\n",
    "print(\"1 / det(A) =\", 1.0 / np.linalg.det(A))\n",
    "print(\"Are they equal?\", np.isclose(np.linalg.det(A_inv), 1.0 / np.linalg.det(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Eigenvalues and Eigenvectors\n",
    "\n",
    "For a square matrix $A$, an **eigenvector** $\\mathbf{v}$ and **eigenvalue** $\\lambda$ satisfy:\n",
    "\n",
    "$$A\\mathbf{v} = \\lambda \\mathbf{v}$$\n",
    "\n",
    "The eigenvectors represent directions that are only scaled (not rotated) by the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Computing Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix\n",
    "A = np.array([[4, 2],\n",
    "              [1, 3]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print()\n",
    "print(\"Eigenvectors (columns):\")\n",
    "print(eigenvectors)\n",
    "print()\n",
    "\n",
    "# Each column of eigenvectors is an eigenvector\n",
    "for i in range(len(eigenvalues)):\n",
    "    lam = eigenvalues[i]\n",
    "    v = eigenvectors[:, i]\n",
    "    print(f\"Eigenvalue {i+1}: lambda = {lam:.4f}\")\n",
    "    print(f\"Eigenvector {i+1}: v = {v}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Verify Av = lambda * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Av = lambda * v for each eigenpair\n",
    "for i in range(len(eigenvalues)):\n",
    "    lam = eigenvalues[i]\n",
    "    v = eigenvectors[:, i]\n",
    "    \n",
    "    # Compute A @ v\n",
    "    Av = A @ v\n",
    "    \n",
    "    # Compute lambda * v\n",
    "    lambda_v = lam * v\n",
    "    \n",
    "    print(f\"Eigenpair {i+1}:\")\n",
    "    print(f\"  A @ v     = {Av}\")\n",
    "    print(f\"  lambda*v  = {lambda_v}\")\n",
    "    print(f\"  Equal?    {np.allclose(Av, lambda_v)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Visualize Eigenvectors for a 2x2 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how A transforms the eigenvectors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Colors for eigenvectors\n",
    "colors = ['#e74c3c', '#2980b9']\n",
    "\n",
    "# Plot 1: Original eigenvectors and their transformations\n",
    "ax = axes[0]\n",
    "ax.set_title('Eigenvectors and Transformations', fontsize=14)\n",
    "\n",
    "for i in range(2):\n",
    "    v = eigenvectors[:, i]\n",
    "    Av = A @ v\n",
    "    \n",
    "    # Plot original eigenvector\n",
    "    ax.arrow(0, 0, v[0], v[1], head_width=0.1, head_length=0.08,\n",
    "             fc=colors[i], ec=colors[i], linewidth=2, label=f'v{i+1}')\n",
    "    \n",
    "    # Plot transformed vector (Av = lambda * v)\n",
    "    ax.arrow(0, 0, Av[0], Av[1], head_width=0.1, head_length=0.08,\n",
    "             fc=colors[i], ec=colors[i], linewidth=2, linestyle='dashed',\n",
    "             alpha=0.5, label=f'Av{i+1} = {eigenvalues[i]:.2f}*v{i+1}')\n",
    "\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Plot 2: How A transforms a unit circle\n",
    "ax = axes[1]\n",
    "ax.set_title('Unit Circle Transformation by A', fontsize=14)\n",
    "\n",
    "# Generate points on a unit circle\n",
    "theta = np.linspace(0, 2 * np.pi, 100)\n",
    "circle = np.array([np.cos(theta), np.sin(theta)])\n",
    "\n",
    "# Transform the circle\n",
    "transformed = A @ circle\n",
    "\n",
    "# Plot original circle\n",
    "ax.plot(circle[0], circle[1], 'b-', linewidth=2, label='Unit circle')\n",
    "\n",
    "# Plot transformed circle (ellipse)\n",
    "ax.plot(transformed[0], transformed[1], 'r-', linewidth=2, label='A * (unit circle)')\n",
    "\n",
    "# Plot eigenvectors scaled by eigenvalues\n",
    "for i in range(2):\n",
    "    v = eigenvectors[:, i]\n",
    "    lam = eigenvalues[i]\n",
    "    ax.arrow(0, 0, lam * v[0], lam * v[1], head_width=0.15, head_length=0.1,\n",
    "             fc=colors[i], ec=colors[i], linewidth=2,\n",
    "             label=f'lambda{i+1}*v{i+1} (lambda={lam:.2f})')\n",
    "\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(-6, 6)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Eigenvalues of a 3x3 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 symmetric matrix (always has real eigenvalues)\n",
    "S = np.array([[6, 2, 1],\n",
    "              [2, 3, 1],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "print(\"Symmetric matrix S:\")\n",
    "print(S)\n",
    "print()\n",
    "\n",
    "eigenvalues_S, eigenvectors_S = np.linalg.eig(S)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues_S)\n",
    "print()\n",
    "\n",
    "# For symmetric matrices, eigenvectors are orthogonal\n",
    "print(\"Eigenvectors (columns):\")\n",
    "print(eigenvectors_S)\n",
    "print()\n",
    "\n",
    "# Verify orthogonality: V^T V should be identity\n",
    "print(\"V^T @ V (should be identity):\")\n",
    "print(eigenvectors_S.T @ eigenvectors_S)\n",
    "print()\n",
    "\n",
    "# Verify: product of eigenvalues = determinant\n",
    "print(\"Product of eigenvalues:\", np.prod(eigenvalues_S))\n",
    "print(\"Determinant of S:\", np.linalg.det(S))\n",
    "print(\"Equal?\", np.isclose(np.prod(eigenvalues_S), np.linalg.det(S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Cholesky Decomposition\n",
    "\n",
    "For a **symmetric positive definite** matrix $A$, the Cholesky decomposition factors it as:\n",
    "\n",
    "$$A = LL^T$$\n",
    "\n",
    "where $L$ is a lower triangular matrix with positive diagonal entries.\n",
    "\n",
    "This decomposition is:\n",
    "- About **twice as fast** as LU decomposition\n",
    "- **Numerically stable**\n",
    "- Used extensively in machine learning (e.g., Gaussian processes, sampling from multivariate normals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Checking Positive Definiteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A matrix is positive definite if all eigenvalues are positive\n",
    "# An easy way to create one: A = M^T M (for any full-rank M)\n",
    "\n",
    "M = np.array([[2, 1, 0],\n",
    "              [1, 3, 1],\n",
    "              [0, 1, 2]])\n",
    "\n",
    "# Create a symmetric positive definite matrix\n",
    "A = M.T @ M\n",
    "\n",
    "print(\"Matrix A = M^T @ M:\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "# Check symmetry\n",
    "print(\"Is A symmetric?\", np.allclose(A, A.T))\n",
    "print()\n",
    "\n",
    "# Check positive definiteness via eigenvalues\n",
    "eigenvalues = np.linalg.eigvals(A)\n",
    "print(\"Eigenvalues of A:\", eigenvalues)\n",
    "print(\"All positive?\", np.all(eigenvalues > 0))\n",
    "print(\"=> A is positive definite!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Computing Cholesky Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cholesky decomposition\n",
    "L = np.linalg.cholesky(A)\n",
    "\n",
    "print(\"Lower triangular matrix L:\")\n",
    "print(L)\n",
    "print()\n",
    "\n",
    "# Verify L is lower triangular\n",
    "print(\"L is lower triangular:\", np.allclose(L, np.tril(L)))\n",
    "print()\n",
    "\n",
    "# Verify A = L @ L^T\n",
    "A_reconstructed = L @ L.T\n",
    "print(\"L @ L^T:\")\n",
    "print(A_reconstructed)\n",
    "print()\n",
    "print(\"A == L @ L^T?\", np.allclose(A, A_reconstructed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Solving Linear Systems with Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Ax = b using Cholesky decomposition\n",
    "# A = L L^T, so L L^T x = b\n",
    "# Step 1: Solve L y = b (forward substitution)\n",
    "# Step 2: Solve L^T x = y (backward substitution)\n",
    "\n",
    "b = np.array([1, 2, 3], dtype=float)\n",
    "\n",
    "# Using Cholesky\n",
    "L = np.linalg.cholesky(A)\n",
    "y = linalg.solve_triangular(L, b, lower=True)\n",
    "x_chol = linalg.solve_triangular(L.T, y, lower=False)\n",
    "\n",
    "# Direct solve for comparison\n",
    "x_direct = np.linalg.solve(A, b)\n",
    "\n",
    "print(\"Solution via Cholesky:  x =\", x_chol)\n",
    "print(\"Solution via direct:    x =\", x_direct)\n",
    "print(\"Solutions match?\", np.allclose(x_chol, x_direct))\n",
    "print()\n",
    "\n",
    "# Verify: A @ x should equal b\n",
    "print(\"Verification: A @ x =\", A @ x_chol)\n",
    "print(\"Original b            =\", b)\n",
    "print(\"Match?\", np.allclose(A @ x_chol, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 What Happens with Non-Positive-Definite Matrices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A matrix that is NOT positive definite\n",
    "not_pd = np.array([[1, 2],\n",
    "                   [2, 1]])  # eigenvalues: 3 and -1\n",
    "\n",
    "print(\"Matrix:\")\n",
    "print(not_pd)\n",
    "print(\"Eigenvalues:\", np.linalg.eigvals(not_pd))\n",
    "print()\n",
    "\n",
    "# Try Cholesky - this should fail\n",
    "try:\n",
    "    L = np.linalg.cholesky(not_pd)\n",
    "    print(\"Cholesky succeeded (unexpected)\")\n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(\"Cholesky failed (as expected)!\")\n",
    "    print(\"Error:\", e)\n",
    "    print()\n",
    "    print(\"Reason: The matrix has a negative eigenvalue,\")\n",
    "    print(\"so it is not positive definite.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Eigendecomposition\n",
    "\n",
    "If a matrix $A$ has $n$ linearly independent eigenvectors, it can be **diagonalized** as:\n",
    "\n",
    "$$A = P D P^{-1}$$\n",
    "\n",
    "where:\n",
    "- $P$ is the matrix whose columns are eigenvectors of $A$\n",
    "- $D$ is a diagonal matrix of eigenvalues\n",
    "\n",
    "This is extremely useful for computing **matrix powers**: $A^k = P D^k P^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Diagonalization: A = P D P^{-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a diagonalizable matrix\n",
    "A = np.array([[4, 1],\n",
    "              [2, 3]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "# Compute eigendecomposition\n",
    "eigenvalues, P = np.linalg.eig(A)\n",
    "\n",
    "# Create diagonal matrix D\n",
    "D = np.diag(eigenvalues)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print()\n",
    "print(\"P (eigenvectors as columns):\")\n",
    "print(P)\n",
    "print()\n",
    "print(\"D (diagonal matrix of eigenvalues):\")\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Verify Reconstruction A = P D P^{-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute P^{-1}\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Reconstruct A\n",
    "A_reconstructed = P @ D @ P_inv\n",
    "\n",
    "print(\"P @ D @ P^{-1}:\")\n",
    "print(A_reconstructed)\n",
    "print()\n",
    "print(\"Original A:\")\n",
    "print(A)\n",
    "print()\n",
    "print(\"A == P D P^{-1}?\", np.allclose(A, A_reconstructed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Matrix Powers via Eigendecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing A^k is easy with eigendecomposition:\n",
    "# A^k = P D^k P^{-1}\n",
    "# D^k is simply raising each diagonal element to the k-th power\n",
    "\n",
    "k = 5\n",
    "\n",
    "# Method 1: Direct matrix power\n",
    "A_power_direct = np.linalg.matrix_power(A, k)\n",
    "\n",
    "# Method 2: Via eigendecomposition\n",
    "D_power = np.diag(eigenvalues ** k)\n",
    "A_power_eigen = P @ D_power @ P_inv\n",
    "\n",
    "print(f\"A^{k} via direct computation:\")\n",
    "print(A_power_direct)\n",
    "print()\n",
    "print(f\"A^{k} via eigendecomposition (P D^{k} P^{{-1}}):\")\n",
    "print(A_power_eigen.real)\n",
    "print()\n",
    "print(\"Results match?\", np.allclose(A_power_direct, A_power_eigen.real))\n",
    "print()\n",
    "\n",
    "# Show the advantage: D^k is trivial to compute\n",
    "print(f\"D^{k} (just raise diagonal elements to power {k}):\")\n",
    "print(D_power)\n",
    "print()\n",
    "print(f\"lambda_1^{k} = {eigenvalues[0]}^{k} = {eigenvalues[0]**k:.4f}\")\n",
    "print(f\"lambda_2^{k} = {eigenvalues[1]}^{k} = {eigenvalues[1]**k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Eigendecomposition of a Symmetric Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For symmetric matrices: A = Q D Q^T (Q is orthogonal, so Q^{-1} = Q^T)\n",
    "S = np.array([[5, 2, 0],\n",
    "              [2, 3, 1],\n",
    "              [0, 1, 4]])\n",
    "\n",
    "print(\"Symmetric matrix S:\")\n",
    "print(S)\n",
    "print(\"Is symmetric?\", np.allclose(S, S.T))\n",
    "print()\n",
    "\n",
    "# Use eigh for symmetric matrices (more efficient and numerically stable)\n",
    "eigenvalues_S, Q = np.linalg.eigh(S)\n",
    "D_S = np.diag(eigenvalues_S)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues_S)\n",
    "print()\n",
    "\n",
    "# Verify Q is orthogonal: Q^T Q = I\n",
    "print(\"Q^T @ Q (should be identity):\")\n",
    "print(Q.T @ Q)\n",
    "print()\n",
    "\n",
    "# Reconstruct: S = Q D Q^T\n",
    "S_reconstructed = Q @ D_S @ Q.T\n",
    "print(\"Q @ D @ Q^T:\")\n",
    "print(S_reconstructed)\n",
    "print()\n",
    "print(\"S == Q D Q^T?\", np.allclose(S, S_reconstructed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Singular Value Decomposition (SVD)\n",
    "\n",
    "The SVD is the most general matrix decomposition. For **any** matrix $A$ (even non-square):\n",
    "\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "where:\n",
    "- $U$ is an $m \\times m$ orthogonal matrix (left singular vectors)\n",
    "- $\\Sigma$ is an $m \\times n$ diagonal matrix (singular values)\n",
    "- $V^T$ is an $n \\times n$ orthogonal matrix (right singular vectors)\n",
    "\n",
    "SVD is fundamental in:\n",
    "- **PCA** (Principal Component Analysis)\n",
    "- **Image compression**\n",
    "- **Recommender systems**\n",
    "- **Pseudoinverse** computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Computing SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix (non-square to show generality)\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12]])\n",
    "\n",
    "print(\"Matrix A (4x3):\")\n",
    "print(A)\n",
    "print(\"Shape:\", A.shape)\n",
    "print()\n",
    "\n",
    "# Compute SVD\n",
    "U, sigma, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "print(\"U (4x4):\")\n",
    "print(U)\n",
    "print(\"Shape:\", U.shape)\n",
    "print()\n",
    "\n",
    "print(\"Singular values:\", sigma)\n",
    "print()\n",
    "\n",
    "print(\"V^T (3x3):\")\n",
    "print(Vt)\n",
    "print(\"Shape:\", Vt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Verify A = U Sigma V^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the full Sigma matrix (m x n)\n",
    "m, n = A.shape\n",
    "Sigma = np.zeros((m, n))\n",
    "np.fill_diagonal(Sigma, sigma)\n",
    "\n",
    "print(\"Full Sigma matrix (4x3):\")\n",
    "print(Sigma)\n",
    "print()\n",
    "\n",
    "# Reconstruct A\n",
    "A_reconstructed = U @ Sigma @ Vt\n",
    "\n",
    "print(\"U @ Sigma @ V^T:\")\n",
    "print(A_reconstructed)\n",
    "print()\n",
    "print(\"Original A:\")\n",
    "print(A)\n",
    "print()\n",
    "print(\"A == U Sigma V^T?\", np.allclose(A, A_reconstructed))\n",
    "print()\n",
    "\n",
    "# Verify orthogonality of U and V\n",
    "print(\"U^T @ U is identity?\", np.allclose(U.T @ U, np.eye(m)))\n",
    "print(\"V^T @ V is identity?\", np.allclose(Vt @ Vt.T, np.eye(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 SVD and Matrix Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of non-zero singular values equals the rank of the matrix\n",
    "print(\"Singular values of A:\", sigma)\n",
    "print()\n",
    "\n",
    "# Count non-zero singular values (using a threshold)\n",
    "tolerance = 1e-10\n",
    "rank = np.sum(sigma > tolerance)\n",
    "print(f\"Number of singular values > {tolerance}: {rank}\")\n",
    "print(f\"Matrix rank (np.linalg.matrix_rank): {np.linalg.matrix_rank(A)}\")\n",
    "print()\n",
    "\n",
    "# Note: A is rank 2 because the rows are linearly dependent\n",
    "# Row 3 = 2*Row2 - Row1, Row 4 = 3*Row2 - 2*Row1\n",
    "print(\"Check: Row3 - 2*Row2 + Row1 =\", A[2] - 2*A[1] + A[0])\n",
    "print(\"The rows are linearly dependent, confirming rank 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Image Compression with Truncated SVD\n",
    "\n",
    "One of the most intuitive applications of SVD is image compression. We represent an image as a matrix, compute its SVD, and keep only the top-$k$ singular values to create an approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic grayscale image (a gradient with some patterns)\n",
    "# This avoids needing to load an external file\n",
    "rows, cols = 200, 300\n",
    "\n",
    "# Create a base gradient\n",
    "x = np.linspace(0, 1, cols)\n",
    "y = np.linspace(0, 1, rows)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Create an interesting image with patterns\n",
    "image = (\n",
    "    0.3 * np.sin(2 * np.pi * X * 3) * np.cos(2 * np.pi * Y * 2) +\n",
    "    0.3 * np.exp(-((X - 0.5)**2 + (Y - 0.5)**2) / 0.1) +\n",
    "    0.2 * X +\n",
    "    0.1 * Y +\n",
    "    0.1 * np.sin(2 * np.pi * X * 8) * np.sin(2 * np.pi * Y * 6)\n",
    ")\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Image dtype:\", image.dtype)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Original Synthetic Image', fontsize=14)\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SVD of the image\n",
    "U, sigma, Vt = np.linalg.svd(image, full_matrices=False)\n",
    "\n",
    "print(f\"U shape: {U.shape}\")\n",
    "print(f\"Sigma shape: {sigma.shape}\")\n",
    "print(f\"V^T shape: {Vt.shape}\")\n",
    "print()\n",
    "print(f\"Total singular values: {len(sigma)}\")\n",
    "print(f\"First 10 singular values: {sigma[:10]}\")\n",
    "print()\n",
    "\n",
    "# Compress with different ranks\n",
    "ranks = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, k in enumerate(ranks):\n",
    "    # Truncated SVD: keep only top-k singular values\n",
    "    compressed = U[:, :k] @ np.diag(sigma[:k]) @ Vt[:k, :]\n",
    "    \n",
    "    # Calculate compression ratio\n",
    "    original_size = rows * cols\n",
    "    compressed_size = k * (rows + cols + 1)\n",
    "    ratio = original_size / compressed_size\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    error = np.linalg.norm(image - compressed, 'fro') / np.linalg.norm(image, 'fro')\n",
    "    \n",
    "    axes[idx].imshow(compressed, cmap='gray')\n",
    "    axes[idx].set_title(f'Rank {k} (ratio: {ratio:.1f}x, error: {error:.4f})', fontsize=11)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Image Compression via Truncated SVD', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Singular Value Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the singular value spectrum\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot singular values\n",
    "axes[0].plot(sigma, 'b-o', markersize=3)\n",
    "axes[0].set_xlabel('Index', fontsize=12)\n",
    "axes[0].set_ylabel('Singular Value', fontsize=12)\n",
    "axes[0].set_title('Singular Values', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot cumulative energy (fraction of total variance captured)\n",
    "cumulative_energy = np.cumsum(sigma**2) / np.sum(sigma**2)\n",
    "axes[1].plot(cumulative_energy, 'r-o', markersize=3)\n",
    "axes[1].axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
    "axes[1].axhline(y=0.99, color='orange', linestyle='--', label='99% threshold')\n",
    "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Energy', fontsize=12)\n",
    "axes[1].set_title('Cumulative Energy Captured', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Find how many components for 95% and 99%\n",
    "k_95 = np.argmax(cumulative_energy >= 0.95) + 1\n",
    "k_99 = np.argmax(cumulative_energy >= 0.99) + 1\n",
    "print(f\"Components for 95% energy: {k_95} out of {len(sigma)}\")\n",
    "print(f\"Components for 99% energy: {k_99} out of {len(sigma)}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Matrix Approximation\n",
    "\n",
    "The **Eckart-Young theorem** states that the best rank-$k$ approximation of a matrix (in terms of Frobenius norm or spectral norm) is given by the **truncated SVD**:\n",
    "\n",
    "$$A_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$$\n",
    "\n",
    "The approximation error is:\n",
    "$$\\|A - A_k\\|_F = \\sqrt{\\sum_{i=k+1}^{r} \\sigma_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Truncated SVD for Different Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix with known rank structure\n",
    "np.random.seed(42)\n",
    "m, n = 50, 40\n",
    "\n",
    "# Create a rank-5 matrix with added noise\n",
    "true_rank = 5\n",
    "U_true = np.random.randn(m, true_rank)\n",
    "V_true = np.random.randn(true_rank, n)\n",
    "A_clean = U_true @ V_true\n",
    "\n",
    "# Add some noise\n",
    "noise_level = 0.5\n",
    "A_noisy = A_clean + noise_level * np.random.randn(m, n)\n",
    "\n",
    "print(f\"Matrix shape: {A_noisy.shape}\")\n",
    "print(f\"True underlying rank: {true_rank}\")\n",
    "print(f\"Numerical rank of noisy matrix: {np.linalg.matrix_rank(A_noisy)}\")\n",
    "print()\n",
    "\n",
    "# Compute SVD\n",
    "U, sigma, Vt = np.linalg.svd(A_noisy, full_matrices=False)\n",
    "\n",
    "# Show singular values - there should be a gap after the 5th\n",
    "print(\"First 10 singular values:\")\n",
    "for i in range(10):\n",
    "    print(f\"  sigma_{i+1} = {sigma[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Reconstruction Error vs. Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstruction error for different ranks\n",
    "max_rank = min(m, n)\n",
    "ranks = range(1, max_rank + 1)\n",
    "errors_frobenius = []\n",
    "errors_vs_clean = []\n",
    "\n",
    "for k in ranks:\n",
    "    # Truncated SVD approximation\n",
    "    A_k = U[:, :k] @ np.diag(sigma[:k]) @ Vt[:k, :]\n",
    "    \n",
    "    # Frobenius norm error (relative to noisy matrix)\n",
    "    error_frob = np.linalg.norm(A_noisy - A_k, 'fro')\n",
    "    errors_frobenius.append(error_frob)\n",
    "    \n",
    "    # Error relative to clean matrix\n",
    "    error_clean = np.linalg.norm(A_clean - A_k, 'fro')\n",
    "    errors_vs_clean.append(error_clean)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Error vs rank\n",
    "axes[0].plot(list(ranks), errors_frobenius, 'b-o', markersize=3, label='||A_noisy - A_k||_F')\n",
    "axes[0].axvline(x=true_rank, color='r', linestyle='--', label=f'True rank = {true_rank}')\n",
    "axes[0].set_xlabel('Rank k', fontsize=12)\n",
    "axes[0].set_ylabel('Frobenius Norm Error', fontsize=12)\n",
    "axes[0].set_title('Reconstruction Error vs. Rank', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error vs clean signal\n",
    "axes[1].plot(list(ranks), errors_vs_clean, 'g-o', markersize=3, label='||A_clean - A_k||_F')\n",
    "axes[1].axvline(x=true_rank, color='r', linestyle='--', label=f'True rank = {true_rank}')\n",
    "axes[1].set_xlabel('Rank k', fontsize=12)\n",
    "axes[1].set_ylabel('Error vs. Clean Matrix', fontsize=12)\n",
    "axes[1].set_title('Denoising: Error vs. True Signal', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMinimum error vs clean matrix at rank {np.argmin(errors_vs_clean)+1}\")\n",
    "print(f\"(True rank was {true_rank})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Eckart-Young Theorem Demonstration\n",
    "\n",
    "The theorem guarantees that the truncated SVD gives the **optimal** low-rank approximation. Let's verify this by comparing it to random low-rank matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eckart-Young Theorem: truncated SVD is the BEST rank-k approximation\n",
    "# Let's verify by comparing with random rank-k approximations\n",
    "\n",
    "# Use a test matrix\n",
    "np.random.seed(42)\n",
    "A_test = np.random.randn(20, 15)\n",
    "U_t, sigma_t, Vt_t = np.linalg.svd(A_test, full_matrices=False)\n",
    "\n",
    "test_rank = 3\n",
    "\n",
    "# Best rank-k approximation (truncated SVD)\n",
    "A_best = U_t[:, :test_rank] @ np.diag(sigma_t[:test_rank]) @ Vt_t[:test_rank, :]\n",
    "best_error = np.linalg.norm(A_test - A_best, 'fro')\n",
    "\n",
    "# Theoretical error from Eckart-Young:\n",
    "# ||A - A_k||_F = sqrt(sum of sigma_i^2 for i > k)\n",
    "theoretical_error = np.sqrt(np.sum(sigma_t[test_rank:]**2))\n",
    "\n",
    "print(f\"Rank-{test_rank} approximation errors:\")\n",
    "print(f\"  Truncated SVD error:  {best_error:.6f}\")\n",
    "print(f\"  Theoretical (E-Y):    {theoretical_error:.6f}\")\n",
    "print(f\"  Match: {np.isclose(best_error, theoretical_error)}\")\n",
    "print()\n",
    "\n",
    "# Compare with random rank-k matrices\n",
    "n_trials = 1000\n",
    "random_errors = []\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    # Create a random rank-k matrix\n",
    "    R1 = np.random.randn(20, test_rank)\n",
    "    R2 = np.random.randn(test_rank, 15)\n",
    "    A_random = R1 @ R2\n",
    "    \n",
    "    error = np.linalg.norm(A_test - A_random, 'fro')\n",
    "    random_errors.append(error)\n",
    "\n",
    "print(f\"Random rank-{test_rank} matrix errors ({n_trials} trials):\")\n",
    "print(f\"  Minimum: {min(random_errors):.6f}\")\n",
    "print(f\"  Mean:    {np.mean(random_errors):.6f}\")\n",
    "print(f\"  Maximum: {max(random_errors):.6f}\")\n",
    "print()\n",
    "print(f\"SVD error ({best_error:.6f}) <= All random errors? \"\n",
    "      f\"{best_error <= min(random_errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Eckart-Young theorem\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(random_errors, bins=50, alpha=0.7, color='steelblue',\n",
    "         edgecolor='black', label=f'Random rank-{test_rank} matrices')\n",
    "plt.axvline(x=best_error, color='red', linewidth=3, linestyle='--',\n",
    "            label=f'Truncated SVD (optimal) = {best_error:.4f}')\n",
    "\n",
    "plt.xlabel('Frobenius Norm Error ||A - A_k||_F', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title(f'Eckart-Young Theorem: Truncated SVD Gives the Best Rank-{test_rank} Approximation',\n",
    "          fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Error Bounds for Each Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how the Eckart-Young error bound works for each rank\n",
    "print(\"Eckart-Young Error Bounds\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Rank k':<10} {'||A-A_k||_F':<18} {'Theoretical':<18} {'Match':<8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for k in range(1, min(10, len(sigma_t)) + 1):\n",
    "    # Compute rank-k approximation\n",
    "    A_k = U_t[:, :k] @ np.diag(sigma_t[:k]) @ Vt_t[:k, :]\n",
    "    actual_error = np.linalg.norm(A_test - A_k, 'fro')\n",
    "    \n",
    "    # Theoretical error\n",
    "    theory_error = np.sqrt(np.sum(sigma_t[k:]**2))\n",
    "    \n",
    "    match = np.isclose(actual_error, theory_error)\n",
    "    print(f\"{k:<10} {actual_error:<18.6f} {theory_error:<18.6f} {str(match):<8}\")\n",
    "\n",
    "print()\n",
    "print(\"The truncated SVD error exactly matches the theoretical bound\")\n",
    "print(\"from the Eckart-Young theorem for every rank.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this notebook, we covered the following matrix decomposition techniques:\n",
    "\n",
    "## Determinants and Trace\n",
    "- Computing det(A) and tr(A)\n",
    "- Key properties: det(AB) = det(A)det(B), tr(AB) = tr(BA)\n",
    "\n",
    "## Eigenvalues and Eigenvectors\n",
    "- Solving Av = lambda * v\n",
    "- Geometric interpretation: directions preserved under transformation\n",
    "\n",
    "## Cholesky Decomposition\n",
    "- A = LL^T for positive definite matrices\n",
    "- Efficient for solving linear systems\n",
    "\n",
    "## Eigendecomposition\n",
    "- A = P D P^{-1} (diagonalization)\n",
    "- Efficient matrix powers: A^k = P D^k P^{-1}\n",
    "\n",
    "## Singular Value Decomposition\n",
    "- A = U Sigma V^T for any matrix\n",
    "- Image compression application\n",
    "\n",
    "## Matrix Approximation\n",
    "- Truncated SVD for low-rank approximation\n",
    "- Eckart-Young theorem: optimality of truncated SVD\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Exercises\n",
    "\n",
    "Try these on your own:\n",
    "\n",
    "**Exercise 1: Determinant and Eigenvalue Relationship**  \n",
    "Create a random 4x4 matrix. Verify that the determinant equals the product of its eigenvalues, and that the trace equals the sum of its eigenvalues.\n",
    "\n",
    "**Exercise 2: Cholesky and Covariance Matrices**  \n",
    "Generate a random 3x3 covariance matrix (hint: create a random matrix M and compute M^T M). Perform Cholesky decomposition and use L to generate samples from a multivariate normal distribution with that covariance (hint: if z ~ N(0, I), then Lz ~ N(0, LL^T)).\n",
    "\n",
    "**Exercise 3: Power Iteration Method**  \n",
    "Implement the power iteration algorithm to find the largest eigenvalue and its corresponding eigenvector of a symmetric matrix. Compare your result with np.linalg.eig. The algorithm is: start with a random vector, repeatedly multiply by A and normalize, until convergence.\n",
    "\n",
    "**Exercise 4: SVD for Data Denoising**  \n",
    "Create a rank-3 matrix of size 30x20 and add Gaussian noise with standard deviation 0.1. Use truncated SVD (ranks 1 through 10) to denoise it. Plot the reconstruction error vs. the true clean matrix for each rank. At which rank do you get the best denoising?\n",
    "\n",
    "---\n",
    "\n",
    "**Course:** Mathematics for Machine Learning  \n",
    "**Instructor:** Mohammed Alnemari"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 0,
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}