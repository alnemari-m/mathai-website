{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MATHEMATICS FOR MACHINE LEARNING \u00b6 Graduate Course \u2022 Spring 2026 Mathematical foundations of artificial intelligence and machine learning - from theory to implementation. Prepared by: Mohammed Alnemari \ud83d\udce2 ANNOUNCEMENTS \u00b6 Important Updates \u00b6 Week 1: Course begins January 20, 2026 - Welcome! Office Hours: Remember to schedule appointments via email First Quiz: Scheduled for Week 3 - Linear Algebra fundamentals Notebooks: All Python implementations available in the Notebooks section TEXTBOOK AND REFERENCES \u00b6 PRIMARY \u00b6 Mathematics for Machine Learning Deisenroth, Faisal, and Ong Cambridge University Press mml-book.github.io Convex Optimization Boyd and Vandenberghe Cambridge University Press stanford.edu/~boyd/cvxbook Introduction to Probability Bertsekas and Tsitsiklis Athena Scientific (2nd Ed.) COURSE MATERIALS \u00b6 \ud83d\udcc4 LECTURES - Lecture slides and notes in PDF format \ud83d\udcd0 MATH TUTORIALS - Detailed mathematical derivations and notes \ud83d\udcbb NOTEBOOKS - Python/Jupyter implementations and exercises INSTRUCTOR & COURSE INFORMATION \u00b6 INSTRUCTOR Mohammed Alnemari TEACHING APPROACH - Part 1: Concepts & Explanation - Part 2: Mathematical Examples & Tutorials - Part 3: Python Implementation PHILOSOPHY Building strong mathematical foundations combined with practical coding skills to prepare students for real-world machine learning applications. OFFICE HOURS Monday & Wednesday 11:00 AM \u2013 1:00 PM BY APPOINTMENT To schedule a meeting outside regular office hours, please contact via email. EMAIL: mnemari@gmail.com COURSE WEBSITE: ut.edu.sa/mathml COURSE STRUCTURE & ASSESSMENT \u00b6 6 Core Chapters (Focus) \u00b6 # Topic Description 1 Linear Algebra Vectors, matrices, and operations 2 Analytic Geometry Geometric interpretations 3 Matrix Decomposition Eigendecomposition & SVD 4 Vector Calculus Gradients & optimization 5 Probability & Distributions Statistical foundations 6 Optimization Model training & parameter estimation Assessment Breakdown \u00b6 Component Weight Details Quizzes 40% 4-5 quizzes throughout course Midterm Examination 20% - Final Examination 30% - Reading & Review Papers 10% 4-5 papers TOTAL 100% - EXAM QUESTION PHILOSOPHY & EXAMPLES \u00b6 Deep Understanding Through Challenging Problems \u00b6 Exam questions are designed to test deep conceptual understanding rather than memorization. Each question builds progressively, connecting theory, computation, and insight. Question Structure: - Multi-part problems that build on each other - Require synthesis of multiple concepts - Mix analytical work with computational techniques - Based strictly on material covered in lectures Sample Exam Question \u00b6 This question exemplifies the depth and rigor expected in exams, requiring synthesis of multiple concepts and progressive problem-solving. Linear Independence, Basis, and Rank \u00b6 Consider the matrix \\(\\mathbf{A} \\in \\mathbb{R}^{4 \\times 4}\\) with column vectors: $ \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 & 5 \\\\ 2 & 1 & 3 & 4 \\\\ 1 & 0 & 1 & 1 \\\\ 0 & 1 & 1 & 2 \\end{bmatrix}\\) $ (a) Use Gaussian elimination to determine \\(\\text{rank}(\\mathbf{A})\\) by reducing to row echelon form. Identify which columns form a basis for the column space \\(\\mathcal{C}(\\mathbf{A})\\) and express each remaining column as a linear combination of these basis columns. (b) Find a basis for the null space \\(\\mathcal{N}(\\mathbf{A}) = \\{\\mathbf{x} \\in \\mathbb{R}^4 : \\mathbf{Ax} = \\mathbf{0}\\}\\) by solving the homogeneous system. Verify that \\(\\text{rank}(\\mathbf{A}) + \\dim(\\mathcal{N}(\\mathbf{A})) = 4\\) . (c) Prove the following general statement: If \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) has rank \\(r\\) , then any set of \\(r+1\\) columns must be linearly dependent. Apply this to show that columns 1, 2, and 4 of your matrix cannot all be part of a linearly independent set if \\(r < 3\\) . (d) Compute the row space \\(\\mathcal{R}(\\mathbf{A})\\) by finding a basis for the row space from the row echelon form. Show that \\(\\dim(\\mathcal{R}(\\mathbf{A})) = \\dim(\\mathcal{C}(\\mathbf{A}))\\) . (e) Demonstrate that \\(\\mathcal{N}(\\mathbf{A})\\) and \\(\\mathcal{R}(\\mathbf{A})\\) are orthogonal subspaces by verifying that every vector in \\(\\mathcal{N}(\\mathbf{A})\\) is orthogonal to every row of \\(\\mathbf{A}\\) . What does this tell you about the decomposition \\(\\mathbb{R}^4 = \\mathcal{R}(\\mathbf{A}^T) \\oplus \\mathcal{N}(\\mathbf{A})\\) ? (f) Consider the augmented matrix \\([\\mathbf{A} | \\mathbf{b}]\\) where \\(\\mathbf{b} = [1, 1, 0, 1]^T\\) . Without solving the system, use your knowledge of \\(\\mathcal{C}(\\mathbf{A})\\) to determine whether \\(\\mathbf{b} \\in \\mathcal{C}(\\mathbf{A})\\) . If \\(\\mathbf{b} \\notin \\mathcal{C}(\\mathbf{A})\\) , decompose \\(\\mathbf{b}\\) as \\(\\mathbf{b} = \\mathbf{b}_{\\parallel} + \\mathbf{b}_{\\perp}\\) where \\(\\mathbf{b}_{\\parallel} \\in \\mathcal{C}(\\mathbf{A})\\) and \\(\\mathbf{b}_{\\perp} \\perp \\mathcal{C}(\\mathbf{A})\\) . LECTURE STRUCTURE: THREE-PART APPROACH \u00b6 PART 1: Concepts & Explanation \u00b6 Theoretical foundations and intuitive understanding of the topic. - Key definitions - Conceptual framework - Intuitive explanations - Real-world context PART 2: Mathematical Examples & Tutorials \u00b6 Hands-on mathematical work with students through guided examples. - Worked examples - Step-by-step solutions - Interactive tutorials - Mathematical derivations PART 3: Python Implementation \u00b6 Practical coding examples implementing concepts from the lecture. - Code examples - Implementation details - Practical applications - Coding exercises NOTATION CONVENTIONS \u00b6 Category Notation SCALARS \\(a, b, c, \\alpha, \\beta, \\gamma\\) VECTORS \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z}\\) MATRICES \\(\\mathbf{X}, \\mathbf{Y}, \\mathbf{Z}\\) SETS \\(A, B, C\\) NUMBER SYSTEMS \\(\\mathbb{R}, \\mathbb{C}, \\mathbb{Z}, \\mathbb{N}, \\mathbb{R}^n\\) PROBABILITY \\(p(\\cdot), P[\\cdot]\\) MOHAMMED ALNEMARI MATHEMATICS FOR MACHINE LEARNING \u2022 SPRING 2026 ENJOY YOUR LEARNING JOURNEY Welcome to Mathematics for Machine Learning. Building the foundation for your future algorithms. Last Updated: February 10, 2026","title":"Home"},{"location":"#mathematics-for-machine-learning","text":"Graduate Course \u2022 Spring 2026 Mathematical foundations of artificial intelligence and machine learning - from theory to implementation. Prepared by: Mohammed Alnemari","title":"MATHEMATICS FOR MACHINE LEARNING"},{"location":"#announcements","text":"","title":"\ud83d\udce2 ANNOUNCEMENTS"},{"location":"#important-updates","text":"Week 1: Course begins January 20, 2026 - Welcome! Office Hours: Remember to schedule appointments via email First Quiz: Scheduled for Week 3 - Linear Algebra fundamentals Notebooks: All Python implementations available in the Notebooks section","title":"Important Updates"},{"location":"#textbook-and-references","text":"","title":"TEXTBOOK AND REFERENCES"},{"location":"#primary","text":"Mathematics for Machine Learning Deisenroth, Faisal, and Ong Cambridge University Press mml-book.github.io Convex Optimization Boyd and Vandenberghe Cambridge University Press stanford.edu/~boyd/cvxbook Introduction to Probability Bertsekas and Tsitsiklis Athena Scientific (2nd Ed.)","title":"PRIMARY"},{"location":"#course-materials","text":"\ud83d\udcc4 LECTURES - Lecture slides and notes in PDF format \ud83d\udcd0 MATH TUTORIALS - Detailed mathematical derivations and notes \ud83d\udcbb NOTEBOOKS - Python/Jupyter implementations and exercises","title":"COURSE MATERIALS"},{"location":"#instructor-course-information","text":"INSTRUCTOR Mohammed Alnemari TEACHING APPROACH - Part 1: Concepts & Explanation - Part 2: Mathematical Examples & Tutorials - Part 3: Python Implementation PHILOSOPHY Building strong mathematical foundations combined with practical coding skills to prepare students for real-world machine learning applications. OFFICE HOURS Monday & Wednesday 11:00 AM \u2013 1:00 PM BY APPOINTMENT To schedule a meeting outside regular office hours, please contact via email. EMAIL: mnemari@gmail.com COURSE WEBSITE: ut.edu.sa/mathml","title":"INSTRUCTOR &amp; COURSE INFORMATION"},{"location":"#course-structure-assessment","text":"","title":"COURSE STRUCTURE &amp; ASSESSMENT"},{"location":"#6-core-chapters-focus","text":"# Topic Description 1 Linear Algebra Vectors, matrices, and operations 2 Analytic Geometry Geometric interpretations 3 Matrix Decomposition Eigendecomposition & SVD 4 Vector Calculus Gradients & optimization 5 Probability & Distributions Statistical foundations 6 Optimization Model training & parameter estimation","title":"6 Core Chapters (Focus)"},{"location":"#assessment-breakdown","text":"Component Weight Details Quizzes 40% 4-5 quizzes throughout course Midterm Examination 20% - Final Examination 30% - Reading & Review Papers 10% 4-5 papers TOTAL 100% -","title":"Assessment Breakdown"},{"location":"#exam-question-philosophy-examples","text":"","title":"EXAM QUESTION PHILOSOPHY &amp; EXAMPLES"},{"location":"#deep-understanding-through-challenging-problems","text":"Exam questions are designed to test deep conceptual understanding rather than memorization. Each question builds progressively, connecting theory, computation, and insight. Question Structure: - Multi-part problems that build on each other - Require synthesis of multiple concepts - Mix analytical work with computational techniques - Based strictly on material covered in lectures","title":"Deep Understanding Through Challenging Problems"},{"location":"#sample-exam-question","text":"This question exemplifies the depth and rigor expected in exams, requiring synthesis of multiple concepts and progressive problem-solving.","title":"Sample Exam Question"},{"location":"#linear-independence-basis-and-rank","text":"Consider the matrix \\(\\mathbf{A} \\in \\mathbb{R}^{4 \\times 4}\\) with column vectors: $ \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 & 5 \\\\ 2 & 1 & 3 & 4 \\\\ 1 & 0 & 1 & 1 \\\\ 0 & 1 & 1 & 2 \\end{bmatrix}\\) $ (a) Use Gaussian elimination to determine \\(\\text{rank}(\\mathbf{A})\\) by reducing to row echelon form. Identify which columns form a basis for the column space \\(\\mathcal{C}(\\mathbf{A})\\) and express each remaining column as a linear combination of these basis columns. (b) Find a basis for the null space \\(\\mathcal{N}(\\mathbf{A}) = \\{\\mathbf{x} \\in \\mathbb{R}^4 : \\mathbf{Ax} = \\mathbf{0}\\}\\) by solving the homogeneous system. Verify that \\(\\text{rank}(\\mathbf{A}) + \\dim(\\mathcal{N}(\\mathbf{A})) = 4\\) . (c) Prove the following general statement: If \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) has rank \\(r\\) , then any set of \\(r+1\\) columns must be linearly dependent. Apply this to show that columns 1, 2, and 4 of your matrix cannot all be part of a linearly independent set if \\(r < 3\\) . (d) Compute the row space \\(\\mathcal{R}(\\mathbf{A})\\) by finding a basis for the row space from the row echelon form. Show that \\(\\dim(\\mathcal{R}(\\mathbf{A})) = \\dim(\\mathcal{C}(\\mathbf{A}))\\) . (e) Demonstrate that \\(\\mathcal{N}(\\mathbf{A})\\) and \\(\\mathcal{R}(\\mathbf{A})\\) are orthogonal subspaces by verifying that every vector in \\(\\mathcal{N}(\\mathbf{A})\\) is orthogonal to every row of \\(\\mathbf{A}\\) . What does this tell you about the decomposition \\(\\mathbb{R}^4 = \\mathcal{R}(\\mathbf{A}^T) \\oplus \\mathcal{N}(\\mathbf{A})\\) ? (f) Consider the augmented matrix \\([\\mathbf{A} | \\mathbf{b}]\\) where \\(\\mathbf{b} = [1, 1, 0, 1]^T\\) . Without solving the system, use your knowledge of \\(\\mathcal{C}(\\mathbf{A})\\) to determine whether \\(\\mathbf{b} \\in \\mathcal{C}(\\mathbf{A})\\) . If \\(\\mathbf{b} \\notin \\mathcal{C}(\\mathbf{A})\\) , decompose \\(\\mathbf{b}\\) as \\(\\mathbf{b} = \\mathbf{b}_{\\parallel} + \\mathbf{b}_{\\perp}\\) where \\(\\mathbf{b}_{\\parallel} \\in \\mathcal{C}(\\mathbf{A})\\) and \\(\\mathbf{b}_{\\perp} \\perp \\mathcal{C}(\\mathbf{A})\\) .","title":"Linear Independence, Basis, and Rank"},{"location":"#lecture-structure-three-part-approach","text":"","title":"LECTURE STRUCTURE: THREE-PART APPROACH"},{"location":"#part-1-concepts-explanation","text":"Theoretical foundations and intuitive understanding of the topic. - Key definitions - Conceptual framework - Intuitive explanations - Real-world context","title":"PART 1: Concepts &amp; Explanation"},{"location":"#part-2-mathematical-examples-tutorials","text":"Hands-on mathematical work with students through guided examples. - Worked examples - Step-by-step solutions - Interactive tutorials - Mathematical derivations","title":"PART 2: Mathematical Examples &amp; Tutorials"},{"location":"#part-3-python-implementation","text":"Practical coding examples implementing concepts from the lecture. - Code examples - Implementation details - Practical applications - Coding exercises","title":"PART 3: Python Implementation"},{"location":"#notation-conventions","text":"Category Notation SCALARS \\(a, b, c, \\alpha, \\beta, \\gamma\\) VECTORS \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z}\\) MATRICES \\(\\mathbf{X}, \\mathbf{Y}, \\mathbf{Z}\\) SETS \\(A, B, C\\) NUMBER SYSTEMS \\(\\mathbb{R}, \\mathbb{C}, \\mathbb{Z}, \\mathbb{N}, \\mathbb{R}^n\\) PROBABILITY \\(p(\\cdot), P[\\cdot]\\) MOHAMMED ALNEMARI MATHEMATICS FOR MACHINE LEARNING \u2022 SPRING 2026 ENJOY YOUR LEARNING JOURNEY Welcome to Mathematics for Machine Learning. Building the foundation for your future algorithms. Last Updated: February 10, 2026","title":"NOTATION CONVENTIONS"},{"location":"animations/","text":"\ud83d\udd12 Protected Content Enter password to access mathematical animations and visualizations Unlock Content \u274c Incorrect password. Please try again. Mathematical Animations & Visualizations \u00b6 Advanced Learning Tools for Deep Mathematical Understanding \ud83c\udfac About This Section \u00b6 This section provides interactive mathematical animations and visualizations to help you develop intuitive understanding of abstract concepts covered in the course. Tools Used: - Manim (Mathematical Animation Engine) - Created by Grant Sanderson (3Blue1Brown) - GeoGebra - Interactive geometry and algebra - Desmos - Graphing calculator - Python Visualizations - Custom interactive plots \ud83d\udcda Why Mathematical Animations? \u00b6 The Power of Visualization \u00b6 Mathematical animations help you: - See abstract concepts in action - Understand geometric interpretations - Remember complex theorems through visual memory - Build intuition before diving into proofs - Connect different mathematical concepts \"Mathematics is not about numbers, equations, or algorithms: it is about understanding.\" - William Paul Thurston \ud83c\udfaf Available Animations \u00b6 Topic 1: Linear Algebra Fundamentals \u00b6 Animation 1.1: Vector Addition and Scaling \u00b6 Visualizing vector addition (tip-to-tail method) Scalar multiplication and direction Linear combinations in 2D and 3D \ud83d\udcf9 Watch: Vector Operations Animation \ud83d\udcbb Code: Manim Source Code \ud83d\udcdd Exercise: Try different scalars and see what happens! Animation 1.2: Matrix Transformations \u00b6 Matrices as linear transformations Understanding det(A) geometrically (area scaling) Rotation, scaling, shearing, and reflection matrices \ud83d\udcf9 Watch: Matrix Transformations \ud83d\udcbb Interactive: GeoGebra - Matrix Transformations Animation 1.3: Column Space and Null Space \u00b6 Visualizing column space as \"reachable\" vectors Null space as vectors that get squashed to zero Rank-nullity theorem in action \ud83d\udcf9 Watch: Column Space & Null Space Topic 2: Analytic Geometry \u00b6 Animation 2.1: Inner Products and Projections \u00b6 Geometric interpretation of dot product Vector projections animated Orthogonality visualization \ud83d\udcf9 Watch: Inner Products Animation 2.2: Norms and Distances \u00b6 Different norms (L1, L2, L\u221e) visualized Unit balls in different norms Distance metrics comparison \ud83d\udcf9 Watch: Norms and Metrics Topic 3: Matrix Decomposition \u00b6 Animation 3.1: Eigenvalues and Eigenvectors \u00b6 What eigenvalues and eigenvectors really mean Visualizing Av = \u03bbv Diagonalization process animated \ud83d\udcf9 Watch: Eigendecomposition \ud83c\udf1f Featured: This is one of the most important visualizations in the course! Animation 3.2: Singular Value Decomposition (SVD) \u00b6 Breaking down any matrix into rotation + scaling + rotation Geometric interpretation of SVD Applications to image compression \ud83d\udcf9 Watch: SVD Visualization \ud83d\udcbb Interactive: SVD Image Compression Demo Topic 4: Vector Calculus \u00b6 Animation 4.1: Gradients and Directional Derivatives \u00b6 Gradient as \"direction of steepest ascent\" Visualizing gradient descent Contour plots and gradient fields \ud83d\udcf9 Watch: Gradient Visualization Animation 4.2: The Chain Rule \u00b6 Chain rule in action with function composition Backpropagation visualization Computational graphs animated \ud83d\udcf9 Watch: Chain Rule & Backprop Topic 5: Probability & Distributions \u00b6 Animation 5.1: Common Probability Distributions \u00b6 Gaussian distribution evolving Central Limit Theorem animated Comparing different distributions \ud83d\udcf9 Watch: Probability Distributions Animation 5.2: Bayesian Inference \u00b6 Prior to posterior animation Bayes' theorem visualized Updating beliefs with evidence \ud83d\udcf9 Watch: Bayesian Inference Topic 6: Optimization \u00b6 Animation 6.1: Gradient Descent Variants \u00b6 Batch, mini-batch, and stochastic gradient descent Momentum and acceleration visualized Adam optimizer animation \ud83d\udcf9 Watch: Optimization Algorithms \ud83d\udcbb Interactive: Gradient Descent Playground Animation 6.2: Convex vs Non-Convex Optimization \u00b6 Convex functions and global minima Non-convex landscapes and local minima Saddle points visualization \ud83d\udcf9 Watch: Loss Landscapes \ud83d\udee0\ufe0f Create Your Own Animations \u00b6 Getting Started with Manim \u00b6 Manim is a powerful Python library for creating mathematical animations. Installation \u00b6 # Install Manim Community Edition pip install manim # Install dependencies pip install numpy scipy matplotlib Your First Animation \u00b6 from manim import * class VectorScene(Scene): def construct(self): # Create coordinate plane plane = NumberPlane() # Create vectors v1 = Vector([2, 1], color=BLUE) v2 = Vector([1, 2], color=RED) v3 = Vector([3, 3], color=GREEN) # Labels v1_label = MathTex(\"\\\\vec{v}\", color=BLUE).next_to(v1, RIGHT) v2_label = MathTex(\"\\\\vec{w}\", color=RED).next_to(v2, LEFT) v3_label = MathTex(\"\\\\vec{v} + \\\\vec{w}\", color=GREEN).next_to(v3, UP) # Animate self.add(plane) self.play(Create(v1), Write(v1_label)) self.play(Create(v2), Write(v2_label)) self.wait() self.play(Transform(v1.copy(), v3), Create(v3), Write(v3_label)) self.wait(2) # Render with: manim -pql your_file.py VectorScene Manim Resources \u00b6 Official Docs: https://docs.manim.community/ 3Blue1Brown Channel: YouTube Manim Tutorial: Community Guide Alternative Tools \u00b6 1. GeoGebra (Interactive Geometry) \u00b6 Great for exploring transformations No coding required Download GeoGebra 2. Desmos (Graphing Calculator) \u00b6 Excellent for function visualization Web-based, no installation Desmos Calculator 3. Python + Matplotlib (Custom Visualizations) \u00b6 import numpy as np import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation # Example: Visualizing matrix transformation def plot_transformation(A): # Create unit square square = np.array([[0, 1, 1, 0, 0], [0, 0, 1, 1, 0]]) # Transform transformed = A @ square # Plot fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5)) ax1.plot(square[0], square[1], 'b-', linewidth=2) ax1.set_title('Original') ax1.grid(True) ax1.axis('equal') ax2.plot(transformed[0], transformed[1], 'r-', linewidth=2) ax2.set_title('After Transformation') ax2.grid(True) ax2.axis('equal') plt.show() # Example usage A = np.array([[2, 0], [0, 1]]) # Scaling matrix plot_transformation(A) \ud83d\udcd6 Recommended Video Series \u00b6 3Blue1Brown - Essence of Linear Algebra \u00b6 The best visual introduction to linear algebra ever made. Vectors, what even are they? - Watch Linear combinations, span, and basis vectors - Watch Linear transformations and matrices - Watch Matrix multiplication as composition - Watch The determinant - Watch Inverse matrices, column space and null space - Watch Dot products and duality - Watch Cross products - Watch Change of basis - Watch Eigenvectors and eigenvalues - Watch 3Blue1Brown - Essence of Calculus \u00b6 Visual approach to understanding derivatives, integrals, and gradients. StatQuest - Statistics and ML Concepts \u00b6 Great for probability and statistical learning visualizations. \ud83d\udca1 How to Use This Section \u00b6 Study Strategy: \u00b6 Before Lecture: Watch the relevant animation to build intuition During Lecture: Connect the visuals to mathematical definitions After Lecture: Create your own animations to test understanding Before Exam: Review animations for quick intuitive recall Active Learning Tips: \u00b6 Pause and Predict: Stop the animation and guess what happens next Vary Parameters: Change values and observe the effects Code Your Own: Try recreating animations from scratch Teach Someone: Explain the visual to a classmate \ud83c\udf93 Student Projects \u00b6 Want to earn bonus points? Create your own mathematical animation! Project Ideas: \u00b6 Visualize the Jacobi or Gauss-Seidel iteration process Animate the convergence of gradient descent on different functions Show how PCA finds principal components Visualize the EM algorithm for Gaussian Mixture Models Animate SVM decision boundaries with different kernels Submission: \u00b6 Email your Manim code to: mnemari@gmail.com Include a 1-page explanation of what you visualized Best submissions will be featured in this section! Bonus: +3% on final grade for exceptional work \ud83d\udcc1 Download All Animations \u00b6 Full Animation Pack (2.5 GB): \ud83d\udce5 Download ZIP Individual Topics: - Topic 1: Linear Algebra - 450 MB - Topic 2: Analytic Geometry - 320 MB - Topic 3: Matrix Decomposition - 380 MB - Topic 4: Vector Calculus - 290 MB - Topic 5: Probability - 340 MB - Topic 6: Optimization - 420 MB \ud83d\udd10 Access Information \u00b6 This section is restricted to enrolled students only. If you need the password: - Attend the first lecture - Check your email (sent after enrollment) - Contact during office hours Keep the password confidential - sharing it violates academic integrity policies. MOHAMMED ALNEMARI MATHEMATICS FOR MACHINE LEARNING \u2022 SPRING 2026 Visualize. Understand. Master. Last Updated: January 26, 2026 Next Animation Upload: Week 3 (SVD Deep Dive) (function() { // Simple password protection const CORRECT_PASSWORD = \"malak2024\"; window.checkPassword = function() { const input = document.getElementById('password-input'); const errorMsg = document.getElementById('error-message'); if (!input) return; if (input.value === CORRECT_PASSWORD) { // Store authentication in session sessionStorage.setItem('animations_auth', 'true'); showContent(); } else { errorMsg.style.display = 'block'; input.value = ''; input.focus(); } }; function showContent() { const gate = document.getElementById('password-gate'); const content = document.getElementById('protected-content'); if (gate) gate.style.display = 'none'; if (content) content.style.display = 'block'; } function checkAuth() { if (sessionStorage.getItem('animations_auth') === 'true') { showContent(); } } // Run immediately checkAuth(); // Also run on DOM ready if (document.readyState === 'loading') { document.addEventListener('DOMContentLoaded', function() { checkAuth(); const input = document.getElementById('password-input'); if (input) { input.addEventListener('keypress', function(e) { if (e.key === 'Enter') { window.checkPassword(); } }); // Focus input after a small delay setTimeout(function() { input.focus(); }, 100); } }); } else { checkAuth(); const input = document.getElementById('password-input'); if (input) { input.addEventListener('keypress', function(e) { if (e.key === 'Enter') { window.checkPassword(); } }); input.focus(); } } })(); #password-input:focus { outline: none; border-color: #00D9A3; box-shadow: 0 0 0 3px rgba(0, 217, 163, 0.2); } button:hover { background: #E64A2E !important; transform: translateY(-2px); box-shadow: 0 4px 12px rgba(255, 87, 51, 0.3); }","title":"\ud83d\udd12 Animations"},{"location":"animations/#mathematical-animations-visualizations","text":"Advanced Learning Tools for Deep Mathematical Understanding","title":"Mathematical Animations &amp; Visualizations"},{"location":"animations/#about-this-section","text":"This section provides interactive mathematical animations and visualizations to help you develop intuitive understanding of abstract concepts covered in the course. Tools Used: - Manim (Mathematical Animation Engine) - Created by Grant Sanderson (3Blue1Brown) - GeoGebra - Interactive geometry and algebra - Desmos - Graphing calculator - Python Visualizations - Custom interactive plots","title":"\ud83c\udfac About This Section"},{"location":"animations/#why-mathematical-animations","text":"","title":"\ud83d\udcda Why Mathematical Animations?"},{"location":"animations/#the-power-of-visualization","text":"Mathematical animations help you: - See abstract concepts in action - Understand geometric interpretations - Remember complex theorems through visual memory - Build intuition before diving into proofs - Connect different mathematical concepts \"Mathematics is not about numbers, equations, or algorithms: it is about understanding.\" - William Paul Thurston","title":"The Power of Visualization"},{"location":"animations/#available-animations","text":"","title":"\ud83c\udfaf Available Animations"},{"location":"animations/#topic-1-linear-algebra-fundamentals","text":"","title":"Topic 1: Linear Algebra Fundamentals"},{"location":"animations/#animation-11-vector-addition-and-scaling","text":"Visualizing vector addition (tip-to-tail method) Scalar multiplication and direction Linear combinations in 2D and 3D \ud83d\udcf9 Watch: Vector Operations Animation \ud83d\udcbb Code: Manim Source Code \ud83d\udcdd Exercise: Try different scalars and see what happens!","title":"Animation 1.1: Vector Addition and Scaling"},{"location":"animations/#animation-12-matrix-transformations","text":"Matrices as linear transformations Understanding det(A) geometrically (area scaling) Rotation, scaling, shearing, and reflection matrices \ud83d\udcf9 Watch: Matrix Transformations \ud83d\udcbb Interactive: GeoGebra - Matrix Transformations","title":"Animation 1.2: Matrix Transformations"},{"location":"animations/#animation-13-column-space-and-null-space","text":"Visualizing column space as \"reachable\" vectors Null space as vectors that get squashed to zero Rank-nullity theorem in action \ud83d\udcf9 Watch: Column Space & Null Space","title":"Animation 1.3: Column Space and Null Space"},{"location":"animations/#topic-2-analytic-geometry","text":"","title":"Topic 2: Analytic Geometry"},{"location":"animations/#animation-21-inner-products-and-projections","text":"Geometric interpretation of dot product Vector projections animated Orthogonality visualization \ud83d\udcf9 Watch: Inner Products","title":"Animation 2.1: Inner Products and Projections"},{"location":"animations/#animation-22-norms-and-distances","text":"Different norms (L1, L2, L\u221e) visualized Unit balls in different norms Distance metrics comparison \ud83d\udcf9 Watch: Norms and Metrics","title":"Animation 2.2: Norms and Distances"},{"location":"animations/#topic-3-matrix-decomposition","text":"","title":"Topic 3: Matrix Decomposition"},{"location":"animations/#animation-31-eigenvalues-and-eigenvectors","text":"What eigenvalues and eigenvectors really mean Visualizing Av = \u03bbv Diagonalization process animated \ud83d\udcf9 Watch: Eigendecomposition \ud83c\udf1f Featured: This is one of the most important visualizations in the course!","title":"Animation 3.1: Eigenvalues and Eigenvectors"},{"location":"animations/#animation-32-singular-value-decomposition-svd","text":"Breaking down any matrix into rotation + scaling + rotation Geometric interpretation of SVD Applications to image compression \ud83d\udcf9 Watch: SVD Visualization \ud83d\udcbb Interactive: SVD Image Compression Demo","title":"Animation 3.2: Singular Value Decomposition (SVD)"},{"location":"animations/#topic-4-vector-calculus","text":"","title":"Topic 4: Vector Calculus"},{"location":"animations/#animation-41-gradients-and-directional-derivatives","text":"Gradient as \"direction of steepest ascent\" Visualizing gradient descent Contour plots and gradient fields \ud83d\udcf9 Watch: Gradient Visualization","title":"Animation 4.1: Gradients and Directional Derivatives"},{"location":"animations/#animation-42-the-chain-rule","text":"Chain rule in action with function composition Backpropagation visualization Computational graphs animated \ud83d\udcf9 Watch: Chain Rule & Backprop","title":"Animation 4.2: The Chain Rule"},{"location":"animations/#topic-5-probability-distributions","text":"","title":"Topic 5: Probability &amp; Distributions"},{"location":"animations/#animation-51-common-probability-distributions","text":"Gaussian distribution evolving Central Limit Theorem animated Comparing different distributions \ud83d\udcf9 Watch: Probability Distributions","title":"Animation 5.1: Common Probability Distributions"},{"location":"animations/#animation-52-bayesian-inference","text":"Prior to posterior animation Bayes' theorem visualized Updating beliefs with evidence \ud83d\udcf9 Watch: Bayesian Inference","title":"Animation 5.2: Bayesian Inference"},{"location":"animations/#topic-6-optimization","text":"","title":"Topic 6: Optimization"},{"location":"animations/#animation-61-gradient-descent-variants","text":"Batch, mini-batch, and stochastic gradient descent Momentum and acceleration visualized Adam optimizer animation \ud83d\udcf9 Watch: Optimization Algorithms \ud83d\udcbb Interactive: Gradient Descent Playground","title":"Animation 6.1: Gradient Descent Variants"},{"location":"animations/#animation-62-convex-vs-non-convex-optimization","text":"Convex functions and global minima Non-convex landscapes and local minima Saddle points visualization \ud83d\udcf9 Watch: Loss Landscapes","title":"Animation 6.2: Convex vs Non-Convex Optimization"},{"location":"animations/#create-your-own-animations","text":"","title":"\ud83d\udee0\ufe0f Create Your Own Animations"},{"location":"animations/#getting-started-with-manim","text":"Manim is a powerful Python library for creating mathematical animations.","title":"Getting Started with Manim"},{"location":"animations/#installation","text":"# Install Manim Community Edition pip install manim # Install dependencies pip install numpy scipy matplotlib","title":"Installation"},{"location":"animations/#your-first-animation","text":"from manim import * class VectorScene(Scene): def construct(self): # Create coordinate plane plane = NumberPlane() # Create vectors v1 = Vector([2, 1], color=BLUE) v2 = Vector([1, 2], color=RED) v3 = Vector([3, 3], color=GREEN) # Labels v1_label = MathTex(\"\\\\vec{v}\", color=BLUE).next_to(v1, RIGHT) v2_label = MathTex(\"\\\\vec{w}\", color=RED).next_to(v2, LEFT) v3_label = MathTex(\"\\\\vec{v} + \\\\vec{w}\", color=GREEN).next_to(v3, UP) # Animate self.add(plane) self.play(Create(v1), Write(v1_label)) self.play(Create(v2), Write(v2_label)) self.wait() self.play(Transform(v1.copy(), v3), Create(v3), Write(v3_label)) self.wait(2) # Render with: manim -pql your_file.py VectorScene","title":"Your First Animation"},{"location":"animations/#manim-resources","text":"Official Docs: https://docs.manim.community/ 3Blue1Brown Channel: YouTube Manim Tutorial: Community Guide","title":"Manim Resources"},{"location":"animations/#alternative-tools","text":"","title":"Alternative Tools"},{"location":"animations/#1-geogebra-interactive-geometry","text":"Great for exploring transformations No coding required Download GeoGebra","title":"1. GeoGebra (Interactive Geometry)"},{"location":"animations/#2-desmos-graphing-calculator","text":"Excellent for function visualization Web-based, no installation Desmos Calculator","title":"2. Desmos (Graphing Calculator)"},{"location":"animations/#3-python-matplotlib-custom-visualizations","text":"import numpy as np import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation # Example: Visualizing matrix transformation def plot_transformation(A): # Create unit square square = np.array([[0, 1, 1, 0, 0], [0, 0, 1, 1, 0]]) # Transform transformed = A @ square # Plot fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5)) ax1.plot(square[0], square[1], 'b-', linewidth=2) ax1.set_title('Original') ax1.grid(True) ax1.axis('equal') ax2.plot(transformed[0], transformed[1], 'r-', linewidth=2) ax2.set_title('After Transformation') ax2.grid(True) ax2.axis('equal') plt.show() # Example usage A = np.array([[2, 0], [0, 1]]) # Scaling matrix plot_transformation(A)","title":"3. Python + Matplotlib (Custom Visualizations)"},{"location":"animations/#recommended-video-series","text":"","title":"\ud83d\udcd6 Recommended Video Series"},{"location":"animations/#3blue1brown-essence-of-linear-algebra","text":"The best visual introduction to linear algebra ever made. Vectors, what even are they? - Watch Linear combinations, span, and basis vectors - Watch Linear transformations and matrices - Watch Matrix multiplication as composition - Watch The determinant - Watch Inverse matrices, column space and null space - Watch Dot products and duality - Watch Cross products - Watch Change of basis - Watch Eigenvectors and eigenvalues - Watch","title":"3Blue1Brown - Essence of Linear Algebra"},{"location":"animations/#3blue1brown-essence-of-calculus","text":"Visual approach to understanding derivatives, integrals, and gradients.","title":"3Blue1Brown - Essence of Calculus"},{"location":"animations/#statquest-statistics-and-ml-concepts","text":"Great for probability and statistical learning visualizations.","title":"StatQuest - Statistics and ML Concepts"},{"location":"animations/#how-to-use-this-section","text":"","title":"\ud83d\udca1 How to Use This Section"},{"location":"animations/#study-strategy","text":"Before Lecture: Watch the relevant animation to build intuition During Lecture: Connect the visuals to mathematical definitions After Lecture: Create your own animations to test understanding Before Exam: Review animations for quick intuitive recall","title":"Study Strategy:"},{"location":"animations/#active-learning-tips","text":"Pause and Predict: Stop the animation and guess what happens next Vary Parameters: Change values and observe the effects Code Your Own: Try recreating animations from scratch Teach Someone: Explain the visual to a classmate","title":"Active Learning Tips:"},{"location":"animations/#student-projects","text":"Want to earn bonus points? Create your own mathematical animation!","title":"\ud83c\udf93 Student Projects"},{"location":"animations/#project-ideas","text":"Visualize the Jacobi or Gauss-Seidel iteration process Animate the convergence of gradient descent on different functions Show how PCA finds principal components Visualize the EM algorithm for Gaussian Mixture Models Animate SVM decision boundaries with different kernels","title":"Project Ideas:"},{"location":"animations/#submission","text":"Email your Manim code to: mnemari@gmail.com Include a 1-page explanation of what you visualized Best submissions will be featured in this section! Bonus: +3% on final grade for exceptional work","title":"Submission:"},{"location":"animations/#download-all-animations","text":"Full Animation Pack (2.5 GB): \ud83d\udce5 Download ZIP Individual Topics: - Topic 1: Linear Algebra - 450 MB - Topic 2: Analytic Geometry - 320 MB - Topic 3: Matrix Decomposition - 380 MB - Topic 4: Vector Calculus - 290 MB - Topic 5: Probability - 340 MB - Topic 6: Optimization - 420 MB","title":"\ud83d\udcc1 Download All Animations"},{"location":"animations/#access-information","text":"This section is restricted to enrolled students only. If you need the password: - Attend the first lecture - Check your email (sent after enrollment) - Contact during office hours Keep the password confidential - sharing it violates academic integrity policies. MOHAMMED ALNEMARI MATHEMATICS FOR MACHINE LEARNING \u2022 SPRING 2026 Visualize. Understand. Master. Last Updated: January 26, 2026 Next Animation Upload: Week 3 (SVD Deep Dive) (function() { // Simple password protection const CORRECT_PASSWORD = \"malak2024\"; window.checkPassword = function() { const input = document.getElementById('password-input'); const errorMsg = document.getElementById('error-message'); if (!input) return; if (input.value === CORRECT_PASSWORD) { // Store authentication in session sessionStorage.setItem('animations_auth', 'true'); showContent(); } else { errorMsg.style.display = 'block'; input.value = ''; input.focus(); } }; function showContent() { const gate = document.getElementById('password-gate'); const content = document.getElementById('protected-content'); if (gate) gate.style.display = 'none'; if (content) content.style.display = 'block'; } function checkAuth() { if (sessionStorage.getItem('animations_auth') === 'true') { showContent(); } } // Run immediately checkAuth(); // Also run on DOM ready if (document.readyState === 'loading') { document.addEventListener('DOMContentLoaded', function() { checkAuth(); const input = document.getElementById('password-input'); if (input) { input.addEventListener('keypress', function(e) { if (e.key === 'Enter') { window.checkPassword(); } }); // Focus input after a small delay setTimeout(function() { input.focus(); }, 100); } }); } else { checkAuth(); const input = document.getElementById('password-input'); if (input) { input.addEventListener('keypress', function(e) { if (e.key === 'Enter') { window.checkPassword(); } }); input.focus(); } } })(); #password-input:focus { outline: none; border-color: #00D9A3; box-shadow: 0 0 0 3px rgba(0, 217, 163, 0.2); } button:hover { background: #E64A2E !important; transform: translateY(-2px); box-shadow: 0 4px 12px rgba(255, 87, 51, 0.3); }","title":"\ud83d\udd10 Access Information"},{"location":"lectures/","text":"Lectures \u00b6 Complete lecture materials with slides, review questions, theory connections, and PhD-level insights \u2014 all in PDF format. Course Administration \u00b6 Assessment Breakdown \u00b6 Component Weight Details Quizzes 40% 4-5 quizzes throughout the course Midterm Examination 20% Covers Part I: Mathematical Foundations Final Examination 30% Comprehensive exam covering all topics Reading & Review Papers 10% 4-5 research papers to review TOTAL 100% - Course Level & Prerequisites \u00b6 Target Audience: Undergraduate/Graduate Level Flexible Prerequisites: Basic linear algebra and vector calculus recommended Course structure allows for adjustment based on student background Mathematical concepts presented with varying levels of rigor Lecture Structure \u00b6 Every lecture follows a consistent four-part approach designed to build deep understanding: Part What It Covers Concepts & Explanation Key definitions, intuitive explanations, \"Think of it as...\" plain-English descriptions Mathematical Examples & Tutorials Step-by-step worked examples with detailed solutions Review & Practice 10 structured review questions with hints, common mistakes, and concepts-at-a-glance tables Advanced Perspectives 5 Theory Connection slides (AI/ML applications) + 5 PhD View slides (research-level topics) Part I: Mathematical Foundations \u00b6 Lecture 0: Introduction & Course Overview \u00b6 Course organization, assessment structure, textbooks, teaching philosophy, notation conventions, and what to expect. Download Lecture 0 (PDF) Lecture 2: Linear Algebra \u00b6 Foundation of Everything in AI Linear algebra is the computational engine behind all of machine learning. Every dataset is a matrix, every data point is a vector, and every neural network layer computes a linear transformation. Topics covered (8 sections): Systems of Linear Equations Matrices Solving Systems of Linear Equations Vector Spaces Linear Independence Basis and Rank Linear Mappings Affine Spaces What's inside the slides: Definitions with plain-English \"Think of it as...\" intuitions Full Math Tutorial section with 15 worked examples 10 Review Questions with hints (systems, matrices, inverse/transpose, Gaussian elimination, vector spaces, linear independence, basis/dimension, rank, linear mappings, affine spaces) Common Mistakes to Avoid (5 key pitfalls) Concepts at a Glance comparison table Key Takeaways summary 5 Theory Connection slides (ML, Computer Vision, Deep Learning, Optimization, Data Science) 5 PhD View slides (Functional Analysis, Matrix Decompositions, Tensor Algebra, Numerical LA, Spectral Theory) Materials: Download Lecture 2: Linear Algebra (PDF) Tutorial: Linear Algebra Notebook: NumPy Basics Lecture 3: Analytic Geometry \u00b6 The Geometry Behind Machine Learning Analytic geometry gives us the tools to measure similarity, distance, and angles in high-dimensional spaces \u2014 the foundation of recommendation systems, NLP embeddings, and PCA. Topics covered (9 sections): Norms Inner Products Lengths and Distances Angles and Orthogonality Orthonormal Basis Orthogonal Complement Inner Product of Functions Orthogonal Projections Rotations What's inside the slides: Complete concept chain: Inner Product \u2192 Norm \u2192 Distance \u2192 Angle \u2192 Orthogonality \u2192 Projection Projection formulas for 1D lines and general subspaces Gram-Schmidt orthogonalization with worked examples 10 Review Questions with hints (norms, inner products, Cauchy-Schwarz, angles, orthogonal matrices, Gram-Schmidt, orthogonal complement, projections onto lines, general projections, rotations) Common Mistakes to Avoid (5 key pitfalls) Concepts at a Glance comparison table 5 Theory Connection slides (ML, NLP, Deep Learning, Computer Vision, Data Science) 5 PhD View slides (Hilbert Spaces, RKHS, Compressed Sensing, Riemannian Geometry, Random Projections) Materials: Download Lecture 3: Analytic Geometry (PDF) Tutorial: Analytic Geometry Notebook: Analytic Geometry in Python Lecture 4: Matrix Decomposition \u00b6 Revealing Hidden Structure in Data Matrix decompositions like eigendecomposition and SVD are the workhorses of data science \u2014 they power PCA, recommender systems, image compression, and Google's PageRank. Topics covered (6 sections): Determinants and Trace Eigenvalues and Eigenvectors Cholesky Decomposition Eigendecomposition and Diagonalization Singular Value Decomposition (SVD) Matrix Approximation What's inside the slides: Eigenvalue computation with characteristic polynomials SVD: geometric interpretation and step-by-step examples Matrix approximation via truncated SVD 10 Review Questions with hints Common Mistakes to Avoid 5 Theory Connection slides (ML applications of decompositions) 5 PhD View slides (advanced decomposition theory) Materials: Download Lecture 4: Matrix Decomposition (PDF) Tutorial: Matrix Decomposition Notebook: Matrix Decomposition in Python Lecture 5: Vector Calculus \u00b6 The Mathematics of Learning Vector calculus provides the gradient and chain rule \u2014 without these, there is no backpropagation, no gradient descent, and no training of neural networks. Topics covered (6 sections): Differentiation of Univariate Functions Partial Differentiation and Gradients Gradients of Vector-Valued Functions Gradients of Matrices Useful Identities for Computing Gradients Backpropagation and Automatic Differentiation What's inside the slides: From single-variable derivatives to Jacobians and gradients Chain rule for vector and matrix functions Backpropagation derivation with computation graphs 10 Review Questions with hints Common Mistakes to Avoid 5 Theory Connection slides (optimization, neural networks, physics) 5 PhD View slides (differential geometry, automatic differentiation theory) Materials: Download Lecture 5: Vector Calculus (PDF) Tutorial: Vector Calculus Notebook: Vector Calculus in Python Lecture 6: Probability & Distributions \u00b6 Reasoning Under Uncertainty Probability theory is the language of uncertainty \u2014 it underpins Bayesian inference, generative models, statistical testing, and every probabilistic ML algorithm. Topics covered (8 sections): Construction of a Probability Space Discrete and Continuous Probabilities Sum Rule, Product Rule, and Bayes' Theorem Summary Statistics and Independence Gaussian Distribution Conjugacy and Exponential Family Change of Variables / Inverse Transform Probability in Machine Learning What's inside the slides: From sample spaces to random variables to distributions Discrete (Bernoulli, Binomial, Geometric) and Continuous (Uniform, Exponential, Gaussian) distributions Bayes' Theorem with real-world examples Marginal and conditional distributions Covariance, correlation, and independence 10 Review Questions with hints Common Mistakes to Avoid 5 Theory Connection slides (Bayesian ML, generative models, information theory) 5 PhD View slides (measure theory, stochastic processes, information geometry) Materials: Download Lecture 6: Probability & Distributions (PDF) Tutorial: Probability and Distributions Notebook: Probability and Distributions in Python Part II: Machine Learning Applications \u00b6 Application 1: When Models Meet Data \u00b6 Description: Introduction to the practical aspects of applying mathematical models to real-world datasets and the challenges that arise. What you'll learn: Data preprocessing and feature engineering Model selection and evaluation Overfitting and regularization Bias-variance tradeoff Materials: Lecture Slides (Complete PDF) Application 2: Dimensionality Reduction (PCA) \u00b6 Description: Using Principal Component Analysis to reduce data complexity while preserving essential information for efficient learning. What you'll learn: PCA theory: eigenvalues meet data variance Step-by-step PCA computation Variance explained and choosing dimensions Eigenfaces and image compression Materials: Lecture Slides (Complete PDF) Application 3: Density Estimation (GMM) \u00b6 Description: Probabilistic approaches to understanding data distributions and clustering using Gaussian Mixture Models. What you'll learn: Probability density estimation Gaussian Mixture Models Expectation-Maximization (EM) algorithm Clustering applications Materials: Lecture Slides (Complete PDF) Application 4: Classification (SVM) \u00b6 Description: Geometric and optimization-based methods for supervised learning and decision boundary determination using Support Vector Machines. What you'll learn: Support Vector Machines (SVM) Kernel methods and the kernel trick Margin maximization Multi-class classification Materials: Lecture Slides (Complete PDF) Quick Reference: All Lecture Downloads \u00b6 Lecture Topic Download Lecture 0 Introduction & Course Overview PDF Lecture 2 Linear Algebra PDF Lecture 3 Analytic Geometry PDF Lecture 4 Matrix Decomposition PDF Lecture 5 Vector Calculus PDF Lecture 6 Probability & Distributions PDF Applications ML Applications (all 4) PDF Primary Textbooks \u00b6 Mathematics for Machine Learning \u00b6 Authors: Deisenroth, Faisal, and Ong Publisher: Cambridge University Press Website: mml-book.github.io Convex Optimization \u00b6 Authors: Boyd and Vandenberghe Publisher: Cambridge University Press Website: stanford.edu/~boyd/cvxbook Introduction to Probability \u00b6 Authors: Bertsekas and Tsitsiklis Publisher: Athena Scientific (2nd Ed.) Learning Tips \u00b6 Before Each Lecture: Download and review the lecture slides Complete prerequisite readings Review previous lecture concepts During Lectures: Follow the four-part structure Take notes during explanations Work through examples actively Try coding exercises immediately After Lectures: Work through the 10 Review Questions (use hints if stuck) Study the Common Mistakes to Avoid Explore Theory Connection slides for AI/ML context Review PhD View slides for deeper understanding Complete Python notebooks Lectures prepared by Mohammed Alnemari Mathematics of AI \u2022 Spring 2026 Last Updated: February 8, 2026","title":"Lectures"},{"location":"lectures/#lectures","text":"Complete lecture materials with slides, review questions, theory connections, and PhD-level insights \u2014 all in PDF format.","title":"Lectures"},{"location":"lectures/#course-administration","text":"","title":"Course Administration"},{"location":"lectures/#assessment-breakdown","text":"Component Weight Details Quizzes 40% 4-5 quizzes throughout the course Midterm Examination 20% Covers Part I: Mathematical Foundations Final Examination 30% Comprehensive exam covering all topics Reading & Review Papers 10% 4-5 research papers to review TOTAL 100% -","title":"Assessment Breakdown"},{"location":"lectures/#course-level-prerequisites","text":"Target Audience: Undergraduate/Graduate Level Flexible Prerequisites: Basic linear algebra and vector calculus recommended Course structure allows for adjustment based on student background Mathematical concepts presented with varying levels of rigor","title":"Course Level &amp; Prerequisites"},{"location":"lectures/#lecture-structure","text":"Every lecture follows a consistent four-part approach designed to build deep understanding: Part What It Covers Concepts & Explanation Key definitions, intuitive explanations, \"Think of it as...\" plain-English descriptions Mathematical Examples & Tutorials Step-by-step worked examples with detailed solutions Review & Practice 10 structured review questions with hints, common mistakes, and concepts-at-a-glance tables Advanced Perspectives 5 Theory Connection slides (AI/ML applications) + 5 PhD View slides (research-level topics)","title":"Lecture Structure"},{"location":"lectures/#part-i-mathematical-foundations","text":"","title":"Part I: Mathematical Foundations"},{"location":"lectures/#lecture-0-introduction-course-overview","text":"Course organization, assessment structure, textbooks, teaching philosophy, notation conventions, and what to expect. Download Lecture 0 (PDF)","title":"Lecture 0: Introduction &amp; Course Overview"},{"location":"lectures/#lecture-2-linear-algebra","text":"Foundation of Everything in AI Linear algebra is the computational engine behind all of machine learning. Every dataset is a matrix, every data point is a vector, and every neural network layer computes a linear transformation. Topics covered (8 sections): Systems of Linear Equations Matrices Solving Systems of Linear Equations Vector Spaces Linear Independence Basis and Rank Linear Mappings Affine Spaces What's inside the slides: Definitions with plain-English \"Think of it as...\" intuitions Full Math Tutorial section with 15 worked examples 10 Review Questions with hints (systems, matrices, inverse/transpose, Gaussian elimination, vector spaces, linear independence, basis/dimension, rank, linear mappings, affine spaces) Common Mistakes to Avoid (5 key pitfalls) Concepts at a Glance comparison table Key Takeaways summary 5 Theory Connection slides (ML, Computer Vision, Deep Learning, Optimization, Data Science) 5 PhD View slides (Functional Analysis, Matrix Decompositions, Tensor Algebra, Numerical LA, Spectral Theory) Materials: Download Lecture 2: Linear Algebra (PDF) Tutorial: Linear Algebra Notebook: NumPy Basics","title":"Lecture 2: Linear Algebra"},{"location":"lectures/#lecture-3-analytic-geometry","text":"The Geometry Behind Machine Learning Analytic geometry gives us the tools to measure similarity, distance, and angles in high-dimensional spaces \u2014 the foundation of recommendation systems, NLP embeddings, and PCA. Topics covered (9 sections): Norms Inner Products Lengths and Distances Angles and Orthogonality Orthonormal Basis Orthogonal Complement Inner Product of Functions Orthogonal Projections Rotations What's inside the slides: Complete concept chain: Inner Product \u2192 Norm \u2192 Distance \u2192 Angle \u2192 Orthogonality \u2192 Projection Projection formulas for 1D lines and general subspaces Gram-Schmidt orthogonalization with worked examples 10 Review Questions with hints (norms, inner products, Cauchy-Schwarz, angles, orthogonal matrices, Gram-Schmidt, orthogonal complement, projections onto lines, general projections, rotations) Common Mistakes to Avoid (5 key pitfalls) Concepts at a Glance comparison table 5 Theory Connection slides (ML, NLP, Deep Learning, Computer Vision, Data Science) 5 PhD View slides (Hilbert Spaces, RKHS, Compressed Sensing, Riemannian Geometry, Random Projections) Materials: Download Lecture 3: Analytic Geometry (PDF) Tutorial: Analytic Geometry Notebook: Analytic Geometry in Python","title":"Lecture 3: Analytic Geometry"},{"location":"lectures/#lecture-4-matrix-decomposition","text":"Revealing Hidden Structure in Data Matrix decompositions like eigendecomposition and SVD are the workhorses of data science \u2014 they power PCA, recommender systems, image compression, and Google's PageRank. Topics covered (6 sections): Determinants and Trace Eigenvalues and Eigenvectors Cholesky Decomposition Eigendecomposition and Diagonalization Singular Value Decomposition (SVD) Matrix Approximation What's inside the slides: Eigenvalue computation with characteristic polynomials SVD: geometric interpretation and step-by-step examples Matrix approximation via truncated SVD 10 Review Questions with hints Common Mistakes to Avoid 5 Theory Connection slides (ML applications of decompositions) 5 PhD View slides (advanced decomposition theory) Materials: Download Lecture 4: Matrix Decomposition (PDF) Tutorial: Matrix Decomposition Notebook: Matrix Decomposition in Python","title":"Lecture 4: Matrix Decomposition"},{"location":"lectures/#lecture-5-vector-calculus","text":"The Mathematics of Learning Vector calculus provides the gradient and chain rule \u2014 without these, there is no backpropagation, no gradient descent, and no training of neural networks. Topics covered (6 sections): Differentiation of Univariate Functions Partial Differentiation and Gradients Gradients of Vector-Valued Functions Gradients of Matrices Useful Identities for Computing Gradients Backpropagation and Automatic Differentiation What's inside the slides: From single-variable derivatives to Jacobians and gradients Chain rule for vector and matrix functions Backpropagation derivation with computation graphs 10 Review Questions with hints Common Mistakes to Avoid 5 Theory Connection slides (optimization, neural networks, physics) 5 PhD View slides (differential geometry, automatic differentiation theory) Materials: Download Lecture 5: Vector Calculus (PDF) Tutorial: Vector Calculus Notebook: Vector Calculus in Python","title":"Lecture 5: Vector Calculus"},{"location":"lectures/#lecture-6-probability-distributions","text":"Reasoning Under Uncertainty Probability theory is the language of uncertainty \u2014 it underpins Bayesian inference, generative models, statistical testing, and every probabilistic ML algorithm. Topics covered (8 sections): Construction of a Probability Space Discrete and Continuous Probabilities Sum Rule, Product Rule, and Bayes' Theorem Summary Statistics and Independence Gaussian Distribution Conjugacy and Exponential Family Change of Variables / Inverse Transform Probability in Machine Learning What's inside the slides: From sample spaces to random variables to distributions Discrete (Bernoulli, Binomial, Geometric) and Continuous (Uniform, Exponential, Gaussian) distributions Bayes' Theorem with real-world examples Marginal and conditional distributions Covariance, correlation, and independence 10 Review Questions with hints Common Mistakes to Avoid 5 Theory Connection slides (Bayesian ML, generative models, information theory) 5 PhD View slides (measure theory, stochastic processes, information geometry) Materials: Download Lecture 6: Probability & Distributions (PDF) Tutorial: Probability and Distributions Notebook: Probability and Distributions in Python","title":"Lecture 6: Probability &amp; Distributions"},{"location":"lectures/#part-ii-machine-learning-applications","text":"","title":"Part II: Machine Learning Applications"},{"location":"lectures/#application-1-when-models-meet-data","text":"Description: Introduction to the practical aspects of applying mathematical models to real-world datasets and the challenges that arise. What you'll learn: Data preprocessing and feature engineering Model selection and evaluation Overfitting and regularization Bias-variance tradeoff Materials: Lecture Slides (Complete PDF)","title":"Application 1: When Models Meet Data"},{"location":"lectures/#application-2-dimensionality-reduction-pca","text":"Description: Using Principal Component Analysis to reduce data complexity while preserving essential information for efficient learning. What you'll learn: PCA theory: eigenvalues meet data variance Step-by-step PCA computation Variance explained and choosing dimensions Eigenfaces and image compression Materials: Lecture Slides (Complete PDF)","title":"Application 2: Dimensionality Reduction (PCA)"},{"location":"lectures/#application-3-density-estimation-gmm","text":"Description: Probabilistic approaches to understanding data distributions and clustering using Gaussian Mixture Models. What you'll learn: Probability density estimation Gaussian Mixture Models Expectation-Maximization (EM) algorithm Clustering applications Materials: Lecture Slides (Complete PDF)","title":"Application 3: Density Estimation (GMM)"},{"location":"lectures/#application-4-classification-svm","text":"Description: Geometric and optimization-based methods for supervised learning and decision boundary determination using Support Vector Machines. What you'll learn: Support Vector Machines (SVM) Kernel methods and the kernel trick Margin maximization Multi-class classification Materials: Lecture Slides (Complete PDF)","title":"Application 4: Classification (SVM)"},{"location":"lectures/#quick-reference-all-lecture-downloads","text":"Lecture Topic Download Lecture 0 Introduction & Course Overview PDF Lecture 2 Linear Algebra PDF Lecture 3 Analytic Geometry PDF Lecture 4 Matrix Decomposition PDF Lecture 5 Vector Calculus PDF Lecture 6 Probability & Distributions PDF Applications ML Applications (all 4) PDF","title":"Quick Reference: All Lecture Downloads"},{"location":"lectures/#primary-textbooks","text":"","title":"Primary Textbooks"},{"location":"lectures/#mathematics-for-machine-learning","text":"Authors: Deisenroth, Faisal, and Ong Publisher: Cambridge University Press Website: mml-book.github.io","title":"Mathematics for Machine Learning"},{"location":"lectures/#convex-optimization","text":"Authors: Boyd and Vandenberghe Publisher: Cambridge University Press Website: stanford.edu/~boyd/cvxbook","title":"Convex Optimization"},{"location":"lectures/#introduction-to-probability","text":"Authors: Bertsekas and Tsitsiklis Publisher: Athena Scientific (2nd Ed.)","title":"Introduction to Probability"},{"location":"lectures/#learning-tips","text":"Before Each Lecture: Download and review the lecture slides Complete prerequisite readings Review previous lecture concepts During Lectures: Follow the four-part structure Take notes during explanations Work through examples actively Try coding exercises immediately After Lectures: Work through the 10 Review Questions (use hints if stuck) Study the Common Mistakes to Avoid Explore Theory Connection slides for AI/ML context Review PhD View slides for deeper understanding Complete Python notebooks Lectures prepared by Mohammed Alnemari Mathematics of AI \u2022 Spring 2026 Last Updated: February 8, 2026","title":"Learning Tips"},{"location":"notebooks/","text":"Python Notebooks \u00b6 Jupyter notebooks for hands-on implementation and computational exercises. Getting Started \u00b6 Notebook 0: Python Libraries Basics Introduction to NumPy, SciPy, and Pandas \u2014 Essential libraries for this course. Open in GitHub | Open in Google Colab | Download Lecture Notebooks \u00b6 Notebook 2: Analytic Geometry \u00b6 Norms, inner products, projections, Gram-Schmidt, and rotations \u2014 all implemented in Python with visualizations. Open in GitHub | Open in Google Colab | Download What's inside: Compute and visualize \\(\\ell_1\\) , \\(\\ell_2\\) , \\(\\ell_\\infty\\) norms and unit balls Inner products, cosine similarity, and angle computation Projection onto lines and general subspaces with 2D visualization Gram-Schmidt implementation from scratch (compare with scipy.linalg.qr ) 2D rotation matrices and vector transformations Notebook 3: Matrix Decomposition \u00b6 Eigenvalues, SVD, and matrix approximation \u2014 the computational workhorses of data science. Open in GitHub | Open in Google Colab | Download What's inside: Determinants, trace, and property verification Eigenvalue computation and eigenvector visualization Cholesky decomposition for positive definite matrices Eigendecomposition: \\(A = PDP^{-1}\\) reconstruction SVD: \\(A = U\\Sigma V^T\\) with image compression demo Matrix approximation: reconstruction error vs rank Notebook 4: Vector Calculus \u00b6 Gradients, Jacobians, and backpropagation \u2014 the mathematics behind training neural networks. Open in GitHub | Open in Google Colab | Download What's inside: Numerical differentiation (forward and central differences) Gradient computation and gradient field visualization Gradient descent from scratch (simple and Rosenbrock functions) Descent path visualization on contour plots Chain rule and backpropagation for a 2-layer neural network Hessian computation and Newton's method comparison Notebook 5: Probability and Distributions \u00b6 From coin flips to Gaussians \u2014 simulate, visualize, and understand probability distributions. Open in GitHub | Open in Google Colab | Download What's inside: Discrete distributions (Bernoulli, Binomial, Geometric) with PMF plots Continuous distributions (Uniform, Exponential, Gaussian) with PDF/CDF plots Bayes' Theorem: medical test example with prior vs posterior Joint distributions and marginalization with heatmaps Covariance and correlation with scatter plots Multivariate Gaussian: contour plots and sampling Central Limit Theorem demonstration Quick Reference: All Notebooks \u00b6 # Topic Colab Download 0 Python Libraries Basics Open ipynb 2 Analytic Geometry Open ipynb 3 Matrix Decomposition Open ipynb 4 Vector Calculus Open ipynb 5 Probability & Distributions Open ipynb Setup & Usage \u00b6 Option 1: Google Colab (Recommended) \u00b6 Click any \"Open in Google Colab\" link above \u2014 no setup required! Option 2: Local Jupyter \u00b6 # Install dependencies pip install jupyter numpy scipy matplotlib pandas # Start Jupyter jupyter notebook Option 3: VS Code \u00b6 Install Python and Jupyter extensions, then open .ipynb files directly. Notebook Structure \u00b6 Each notebook includes: Learning Objectives - What you'll learn Theory Review - Key concepts from lectures Implementation - Step-by-step code with explanations Visualizations - Plots and graphs for intuition Practice Exercises - 4 exercises per notebook Best Practices \u00b6 Before Starting: Review corresponding lecture slides Read the matching math tutorial Understand the theory before coding While Working: Run cells in order Experiment with parameters Add your own test cases Write notes in markdown cells After Completing: Compare your implementations with library functions (SciPy, scikit-learn) Test edge cases Try the practice exercises Notebooks developed by Mohammed Alnemari Mathematics of AI \u2022 Spring 2026 Last Updated: February 8, 2026","title":"Notebooks"},{"location":"notebooks/#python-notebooks","text":"Jupyter notebooks for hands-on implementation and computational exercises.","title":"Python Notebooks"},{"location":"notebooks/#getting-started","text":"Notebook 0: Python Libraries Basics Introduction to NumPy, SciPy, and Pandas \u2014 Essential libraries for this course. Open in GitHub | Open in Google Colab | Download","title":"Getting Started"},{"location":"notebooks/#lecture-notebooks","text":"","title":"Lecture Notebooks"},{"location":"notebooks/#notebook-2-analytic-geometry","text":"Norms, inner products, projections, Gram-Schmidt, and rotations \u2014 all implemented in Python with visualizations. Open in GitHub | Open in Google Colab | Download What's inside: Compute and visualize \\(\\ell_1\\) , \\(\\ell_2\\) , \\(\\ell_\\infty\\) norms and unit balls Inner products, cosine similarity, and angle computation Projection onto lines and general subspaces with 2D visualization Gram-Schmidt implementation from scratch (compare with scipy.linalg.qr ) 2D rotation matrices and vector transformations","title":"Notebook 2: Analytic Geometry"},{"location":"notebooks/#notebook-3-matrix-decomposition","text":"Eigenvalues, SVD, and matrix approximation \u2014 the computational workhorses of data science. Open in GitHub | Open in Google Colab | Download What's inside: Determinants, trace, and property verification Eigenvalue computation and eigenvector visualization Cholesky decomposition for positive definite matrices Eigendecomposition: \\(A = PDP^{-1}\\) reconstruction SVD: \\(A = U\\Sigma V^T\\) with image compression demo Matrix approximation: reconstruction error vs rank","title":"Notebook 3: Matrix Decomposition"},{"location":"notebooks/#notebook-4-vector-calculus","text":"Gradients, Jacobians, and backpropagation \u2014 the mathematics behind training neural networks. Open in GitHub | Open in Google Colab | Download What's inside: Numerical differentiation (forward and central differences) Gradient computation and gradient field visualization Gradient descent from scratch (simple and Rosenbrock functions) Descent path visualization on contour plots Chain rule and backpropagation for a 2-layer neural network Hessian computation and Newton's method comparison","title":"Notebook 4: Vector Calculus"},{"location":"notebooks/#notebook-5-probability-and-distributions","text":"From coin flips to Gaussians \u2014 simulate, visualize, and understand probability distributions. Open in GitHub | Open in Google Colab | Download What's inside: Discrete distributions (Bernoulli, Binomial, Geometric) with PMF plots Continuous distributions (Uniform, Exponential, Gaussian) with PDF/CDF plots Bayes' Theorem: medical test example with prior vs posterior Joint distributions and marginalization with heatmaps Covariance and correlation with scatter plots Multivariate Gaussian: contour plots and sampling Central Limit Theorem demonstration","title":"Notebook 5: Probability and Distributions"},{"location":"notebooks/#quick-reference-all-notebooks","text":"# Topic Colab Download 0 Python Libraries Basics Open ipynb 2 Analytic Geometry Open ipynb 3 Matrix Decomposition Open ipynb 4 Vector Calculus Open ipynb 5 Probability & Distributions Open ipynb","title":"Quick Reference: All Notebooks"},{"location":"notebooks/#setup-usage","text":"","title":"Setup &amp; Usage"},{"location":"notebooks/#option-1-google-colab-recommended","text":"Click any \"Open in Google Colab\" link above \u2014 no setup required!","title":"Option 1: Google Colab (Recommended)"},{"location":"notebooks/#option-2-local-jupyter","text":"# Install dependencies pip install jupyter numpy scipy matplotlib pandas # Start Jupyter jupyter notebook","title":"Option 2: Local Jupyter"},{"location":"notebooks/#option-3-vs-code","text":"Install Python and Jupyter extensions, then open .ipynb files directly.","title":"Option 3: VS Code"},{"location":"notebooks/#notebook-structure","text":"Each notebook includes: Learning Objectives - What you'll learn Theory Review - Key concepts from lectures Implementation - Step-by-step code with explanations Visualizations - Plots and graphs for intuition Practice Exercises - 4 exercises per notebook","title":"Notebook Structure"},{"location":"notebooks/#best-practices","text":"Before Starting: Review corresponding lecture slides Read the matching math tutorial Understand the theory before coding While Working: Run cells in order Experiment with parameters Add your own test cases Write notes in markdown cells After Completing: Compare your implementations with library functions (SciPy, scikit-learn) Test edge cases Try the practice exercises Notebooks developed by Mohammed Alnemari Mathematics of AI \u2022 Spring 2026 Last Updated: February 8, 2026","title":"Best Practices"},{"location":"papers/","text":"Reading & Review Papers \u00b6 Essential papers covering the mathematical foundations of machine learning and AI. Assignment Requirements \u00b6 Component Weight: 10% of final grade Requirements: Read and review the 5 required papers below throughout the semester Submit written reviews (2-3 pages each) Focus on mathematical concepts and techniques Connect paper content to course topics Review Structure: Summary of main mathematical concepts Key theorems, proofs, or derivations Connections to course material Critical analysis of mathematical approach Personal insights and questions BONUS OPPORTUNITY \u00b6 Want to go deeper? Read additional papers from the Optional Papers section below! Discuss any optional paper with me during office hours Gain deeper understanding of mathematical foundations Earn bonus points towards your final grade More papers = deeper expertise in Mathematics of AI The more you read, the stronger your mathematical foundation will become. Students who engage with optional papers consistently demonstrate superior understanding and research capability. REQUIRED PAPERS (5 Papers - Must Read) \u00b6 All papers are available on arXiv and are \u226420 pages Required Paper 1: Linear Algebra & Matrix Decomposition \u00b6 A Tutorial on Principal Component Analysis \u00b6 Authors: Shlens (2014) Pages: 12 pages arXiv: 1404.1100 Topics Covered: Covariance matrices and their eigenstructure (Topics 1-2) Eigendecomposition and Singular Value Decomposition (Topic 3) Variance maximization and projection (Topic 2) Dimensionality reduction from first principles (Application 2) Why This Paper is Required: This tutorial derives PCA step-by-step from first principles, demonstrating exactly how linear algebra (Topics 1-3) powers one of the most important tools in data science. You'll see eigenvalues, eigenvectors, covariance matrices, and SVD working together in a concrete application. The paper assumes only basic linear algebra, making it the perfect bridge between course theory and practice. What Makes It Excellent: Short, clear, and self-contained Complete mathematical derivation from scratch Connects abstract linear algebra to real data analysis Directly supports Application 2 (Dimensionality Reduction) Course Topics: Topics 1, 2, 3, Application 2 Required Paper 2: Optimization Theory \u00b6 An Overview of Gradient Descent Optimization Algorithms \u00b6 Authors: Ruder (2017) Pages: 14 pages arXiv: 1609.04747 Topics Covered: Gradient descent and its variants (Topic 6) Momentum, Nesterov accelerated gradient Adaptive learning rate methods (Adagrad, RMSprop, Adam) Challenges in optimization: saddle points, local minima Why This Paper is Required: This paper provides a clear, accessible overview of all the major gradient descent optimization algorithms used in modern deep learning. It explains the mathematical formulation of each optimizer, compares their properties, and discusses when to use each one. Understanding these optimizers is essential for training any neural network. What Makes It Excellent: Covers every major optimizer in one place Clear mathematical formulations with intuitive explanations Practical guidance on choosing optimizers One of the most cited optimization overviews in deep learning Course Topics: Topics 4, 6, Application 1 Required Paper 3: Probability & Generative Models \u00b6 Auto-Encoding Variational Bayes \u00b6 Authors: Kingma, Welling (2014) Pages: 14 pages arXiv: 1312.6114 Topics Covered: Probability distributions and latent variables (Topic 5) Variational inference and the ELBO Reparameterization trick (Topics 4 + 5) Gaussian distributions in generative models Why This Paper is Required: The VAE paper is a landmark that elegantly combines probability theory (Topic 5) with optimization (Topic 6) and calculus (Topic 4). It shows how to learn probability distributions from data using neural networks. The mathematical derivation of the Evidence Lower Bound (ELBO) is a masterclass in applying Bayes' theorem, KL divergence, and expectations \u2014 all key concepts from Topic 5. What Makes It Excellent: Foundational paper for modern generative AI Elegant mathematical derivation using course concepts Demonstrates probability + optimization working together Directly connects to Application 3 (Density Estimation) Course Topics: Topics 4, 5, 6, Application 3 Required Paper 4: Gradient-Based Learning \u00b6 Adam: A Method for Stochastic Optimization \u00b6 Authors: Kingma, Ba (2015) Pages: 15 pages arXiv: 1412.6980 Topics Covered: First and second moment estimation of gradients (Topics 4 + 5) Bias correction in running averages Convergence analysis for convex objectives (Topic 6) Gradient computation and adaptive learning rates Why This Paper is Required: Adam is the most widely used optimizer in deep learning. This paper derives it mathematically, showing how gradient computation (Topic 4), expected values and variance (Topic 5), and optimization theory (Topic 6) come together. The convergence proof provides a rigorous example of mathematical analysis applied to a practical algorithm. Every ML practitioner should understand this derivation. What Makes It Excellent: The default optimizer in most deep learning frameworks Clean mathematical derivation with convergence proof Combines concepts from calculus, probability, and optimization Directly applicable to all neural network training Course Topics: Topics 4, 5, 6 Required Paper 5: Kernel Methods & SVM \u00b6 Learning Theory and Support Vector Machines - A Primer \u00b6 Authors: Banf (2019) Pages: ~15 pages arXiv: 1902.04622 Topics Covered: Statistical learning theory fundamentals Empirical vs structural risk minimization Support Vector Machines (Application 4) Kernel trick and feature spaces (Topic 2) Why This Paper is Required: This primer introduces statistical learning theory and SVMs in an accessible way. It covers the mathematical foundations of why learning from data works (risk minimization), then shows how SVMs use geometric concepts (margins, hyperplanes) from our course to build powerful classifiers. It directly supports Application 4 and ties together linear algebra (Topics 1-2) and optimization (Topic 6). What Makes It Excellent: Accessible introduction to learning theory Connects mathematical foundations to practical classification Covers both theory (risk minimization) and practice (SVM) Short and focused \u2014 ideal for course-level reading Course Topics: Topics 1, 2, 6, Application 4 Why These 5 Papers? \u00b6 Comprehensive Coverage: Paper 1 covers linear algebra and matrix decomposition (Topics 1-3) Paper 2 covers optimization algorithms (Topic 6) Paper 3 covers probability and generative models (Topic 5) Paper 4 covers gradient computation and adaptive methods (Topic 4) Paper 5 synthesizes multiple topics in SVM classification (Application 4) Student-Friendly: All papers are \u226420 pages Accessible mathematical rigor without being overwhelming Build progressively on course material Mix of classical foundations and modern perspectives Practical Relevance: Each paper connects directly to ML practice Cover the three main course applications (PCA, GMM, SVM) Provide foundation for understanding modern deep learning Suggested Reading Schedule \u00b6 Week Paper Topic Review Due 3-4 Paper 1 (PCA Tutorial) Topics 1-3 Week 5 5-6 Paper 4 (Adam Optimizer) Topic 4 Week 7 7-8 Paper 2 (Gradient Descent Overview) Topic 6 Week 9 9-10 Paper 3 (VAE) Topic 5 Week 11 11-12 Paper 5 (SVM Primer) Application 4 Week 13 OPTIONAL PAPERS (15 Papers - For Bonus) \u00b6 Want to earn bonus points and gain deeper understanding? Read any of these papers and discuss them with me during office hours! All papers available on arXiv and are \u226420 pages Linear Algebra & Matrix Theory (4 papers) \u00b6 Optional 1: Linear Algebra in Transformers \u00b6 Title: \"Attention Is All You Need\" Authors: Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin (2017) | Pages: 15 | arXiv:1706.03762 Focus: Matrix multiplications, linear projections, scaled dot-product attention Why Read: See how linear algebra (matrix operations, projections, inner products) powers the architecture behind ChatGPT and modern AI Optional 2: Matrix Factorization in Recommender Systems \u00b6 Title: \"Neural Collaborative Filtering\" Authors: He, Liao, Zhang, Nie, Hu, Chua (2017) | Pages: 10 | arXiv:1708.05031 Focus: Matrix factorization extended with neural networks for recommendation Why Read: Shows how matrix decomposition concepts (Topic 3) are applied in real recommendation systems Optional 3: Vector Spaces in NLP \u00b6 Title: \"Efficient Estimation of Word Representations in Vector Space\" Authors: Mikolov, Chen, Corrado, Dean (2013) | Pages: 12 | arXiv:1301.3781 Focus: Word embeddings as vectors, linear relationships in semantic space Why Read: Demonstrates that words can be represented as vectors where linear algebra operations capture meaning (king - man + woman = queen) Optional 4: Normalization and Matrix Statistics \u00b6 Title: \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\" Authors: Ioffe, Szegedy (2015) | Pages: 11 | arXiv:1502.03167 Focus: Statistics of activations, normalization transformations, gradient flow Why Read: Shows how matrix statistics (mean, variance) and normalization improve neural network training Optimization Methods (4 papers) \u00b6 Optional 5: Escaping Saddle Points \u00b6 Title: \"How to Escape Saddle Points Efficiently\" Authors: Jin, Ge, Netrapalli, Kakade, Jordan (2017) | Pages: 9 | arXiv:1703.00887 Focus: Non-convex optimization, perturbed gradient descent, convergence to second-order stationary points Why Read: Explains mathematically why gradient descent succeeds in non-convex deep learning landscapes Optional 6: Loss Landscape Visualization \u00b6 Title: \"Visualizing the Loss Landscape of Neural Nets\" Authors: Li, Xu, Taylor, Studer, Goldstein (2018) | Pages: 14 | arXiv:1712.09913 Focus: Loss surface geometry, filter normalization, effect of skip connections Why Read: Visual and geometric understanding of why some networks are easier to optimize than others Optional 7: Bayesian Hyperparameter Optimization \u00b6 Title: \"Practical Bayesian Optimization of Machine Learning Algorithms\" Authors: Snoek, Larochelle, Adams (2012) | Pages: 9 | arXiv:1206.2944 Focus: Gaussian processes for hyperparameter tuning, acquisition functions Why Read: Elegant application of probability (Gaussian processes) to the practical problem of tuning ML models Optional 8: Adversarial Optimization \u00b6 Title: \"Generative Adversarial Nets\" Authors: Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, Bengio (2014) | Pages: 9 | arXiv:1406.2661 Focus: Minimax optimization, game theory, probability distribution matching Why Read: Foundational paper for generative AI, shows optimization as a two-player game between neural networks Probability & Statistics (3 papers) \u00b6 Optional 9: EM Algorithm \u00b6 Title: \"EM Algorithm and Variants: An Informal Tutorial\" Authors: Roche (2012) | Pages: 20 | arXiv:1105.1476 Focus: Expectation-Maximization algorithm, mixture models, convergence properties Why Read: Clear tutorial on the EM algorithm, directly relevant to GMM (Application 3) Optional 10: Information Theory & Deep Learning \u00b6 Title: \"Deep Learning and the Information Bottleneck Principle\" Authors: Tishby, Zaslavsky (2015) | Pages: 5 | arXiv:1503.02406 Focus: Information bottleneck, mutual information, deep network layers as compression Why Read: Connects information theory to understanding why deep learning works \u2014 layers progressively compress information Optional 11: Uncertainty in Deep Learning \u00b6 Title: \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\" Authors: Gal, Ghahramani (2016) | Pages: 12 | arXiv:1506.02142 Focus: Bayesian inference, model uncertainty, dropout as variational inference Why Read: Beautiful connection between a practical technique (dropout) and Bayesian probability theory Deep Learning Theory (4 papers) \u00b6 Optional 12: Neural Network Approximation Theory \u00b6 Title: \"Deep vs. Shallow Networks: An Approximation Theory Perspective\" Authors: Mhaskar, Poggio (2016) | Pages: 8 | arXiv:1608.03287 Focus: Function approximation, depth vs width, compositional functions Why Read: Rigorous analysis of why deep networks outperform shallow ones \u2014 depth enables exponential efficiency Optional 13: Sparse Networks \u00b6 Title: \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\" Authors: Frankle, Carlin (2019) | Pages: ~15 | arXiv:1803.03635 Focus: Network pruning, sparse subnetworks, weight initialization Why Read: Reveals that small subnetworks within large networks can match full network performance \u2014 implications for efficiency Optional 14: Knowledge Distillation \u00b6 Title: \"Distilling the Knowledge in a Neural Network\" Authors: Hinton, Vinyals, Dean (2015) | Pages: 9 | arXiv:1503.02531 Focus: Model compression, soft targets, temperature scaling of probability distributions Why Read: Shows how probability distributions (softmax outputs) can transfer knowledge between networks Optional 15: Rethinking Generalization \u00b6 Title: \"Understanding Deep Learning Requires Rethinking Generalization\" Authors: Zhang, Bengio, Hardt, Recht, Vinyals (2017) | Pages: ~15 | arXiv:1611.03530 Focus: Generalization theory, memorization vs learning, role of regularization Why Read: Challenges classical learning theory by showing neural networks can memorize random labels \u2014 a foundational puzzle in ML theory How to Earn Bonus Points \u00b6 Read any optional paper(s) from the list above Schedule office hours to discuss the paper with me Come prepared with: Summary of main mathematical concepts Connections to course material Questions or insights How it extends your understanding Bonus Structure: 1 optional paper discussed = +2% bonus on final grade 2 optional papers discussed = +4% bonus on final grade 3+ optional papers discussed = +5% bonus on final grade (maximum) Quality matters: Superficial reading won't earn bonus points. Show deep engagement with the mathematical content! Paper-Topic Coverage Matrix \u00b6 Required Papers \u00b6 Paper Topics 1-3 (LA) Topic 4 (Calc) Topic 5 (Prob) Topic 6 (Opt) Applications 1. PCA Tutorial \u2713\u2713\u2713 App 2 2. Gradient Descent Overview \u2713\u2713 \u2713\u2713\u2713 Training 3. VAE \u2713 \u2713\u2713\u2713 \u2713\u2713 App 3 4. Adam Optimizer \u2713\u2713\u2713 \u2713 \u2713\u2713 Training 5. SVM Primer \u2713\u2713 \u2713\u2713 App 4 Optional Papers Coverage \u00b6 Linear Algebra Deep Dives: Optional 1-4 (Transformers, Recommenders, Word Vectors, Normalization) Optimization Deep Dives: Optional 5-8 (Saddle Points, Loss Landscapes, Bayesian Opt, GANs) Probability Deep Dives: Optional 9-11 (EM Algorithm, Information Theory, Bayesian Dropout) Deep Learning Theory: Optional 12-15 (Approximation, Lottery Tickets, Distillation, Generalization) PhD-Level Papers (Advanced Reading) \u00b6 For graduate students and researchers seeking deeper mathematical foundations. These are comprehensive, longer papers (20-160 pages) that provide rigorous treatments with full proofs and advanced theory. Foundational Theory (5 papers) \u00b6 PhD 1: Mathematical Foundations of Deep Learning \u00b6 Title: \"The Modern Mathematics of Deep Learning\" Authors: Berner, Grohs, Kutyniok, Petersen (2021) | Pages: ~60 | arXiv:2105.04026 Focus: Complete mathematical treatment of deep learning theory \u2014 vector spaces, function approximation, optimization, and generalization Why Read: The most comprehensive mathematical survey connecting linear algebra, analysis, and approximation theory to deep learning PhD 2: Large-Scale Optimization Theory \u00b6 Title: \"Optimization Methods for Large-Scale Machine Learning\" Authors: Bottou, Curtis, Nocedal (2018) | Pages: ~90 | arXiv:1606.04838 Focus: Complete convergence analysis, noise reduction methods, second-order methods, stochastic gradient theory Why Read: The definitive reference on optimization for ML with full mathematical proofs of convergence rates PhD 3: Variational Inference \u00b6 Title: \"Variational Inference: A Review for Statisticians\" Authors: Blei, Kucukelbir, McAuliffe (2017) | Pages: ~33 | arXiv:1601.00670 Focus: Complete variational inference framework, ELBO derivation, mean-field methods, stochastic variational inference Why Read: Rigorous treatment of how optimization approximates Bayesian inference \u2014 connects probability theory to practical algorithms PhD 4: Gradient-Based Training \u00b6 Title: \"Practical Recommendations for Gradient-Based Training of Deep Architectures\" Authors: Bengio (2012) | Pages: ~35 | arXiv:1206.5533 Focus: Learning rate theory, momentum analysis, weight initialization, preprocessing, hyperparameter optimization Why Read: Written by Turing Award winner Yoshua Bengio \u2014 comprehensive guide to training neural networks with mathematical justification PhD 5: Kernel Methods and RKHS Theory \u00b6 Title: \"Kernel Methods in Machine Learning\" Authors: Hofmann, Sch\u00f6lkopf, Smola (2008) | Pages: ~50 | arXiv:math/0701907 Focus: Reproducing Kernel Hilbert Spaces, Mercer's theorem, kernel design, representer theorem, SVM duality Why Read: Published in Annals of Statistics \u2014 the definitive mathematical treatment of kernel methods by the field's pioneers Advanced Topics (8 papers) \u00b6 PhD 6: Randomized Numerical Linear Algebra \u00b6 Title: \"Randomized Numerical Linear Algebra: Foundations and Algorithms\" Authors: Martinsson, Tropp (2020) | Pages: ~90 | arXiv:2002.01387 Focus: Randomized SVD, matrix sketching, Johnson-Lindenstrauss, low-rank approximation algorithms Why Read: Comprehensive survey of probabilistic approaches to large-scale matrix computations PhD 7: Random Matrix Theory for Neural Networks \u00b6 Title: \"A Random Matrix Approach to Neural Networks\" Authors: Louart, Liao, Couillet (2017) | Pages: ~30 | arXiv:1702.05419 Focus: Spectral analysis of Gram matrices, deterministic equivalents, concentration inequalities Why Read: Applies random matrix theory to rigorously analyze neural network behavior PhD 8: Convex Optimization \u00b6 Title: \"Convex Optimization: Algorithms and Complexity\" Authors: Bubeck (2015) | Pages: ~130 | arXiv:1405.4980 Focus: Black-box optimization, accelerated methods, mirror descent, interior point methods, stochastic methods Why Read: Self-contained monograph covering all of convex optimization theory \u2014 the mathematical backbone of ML PhD 9: Non-Convex Optimization \u00b6 Title: \"Non-convex Optimization for Machine Learning\" Authors: Jain, Kar (2017) | Pages: ~160 | arXiv:1712.07897 Focus: Alternating minimization, EM convergence, matrix completion, phase retrieval, non-convex landscape analysis Why Read: Comprehensive treatment of why and when non-convex optimization succeeds in machine learning PhD 10: Bayesian Optimization \u00b6 Title: \"A Tutorial on Bayesian Optimization\" Authors: Frazier (2018) | Pages: ~35 | arXiv:1807.02811 Focus: Gaussian process regression, acquisition functions, theoretical guarantees, practical considerations Why Read: Complete mathematical framework for Bayesian optimization with full derivations PhD 11: Statistical Learning Theory \u00b6 Title: \"Statistical Learning Theory: Models, Concepts, and Results\" Authors: von Luxburg, Sch\u00f6lkopf (2008) | Pages: ~26 | arXiv:0810.4752 Focus: PAC learning, VC dimension, Rademacher complexity, generalization bounds, model selection Why Read: Rigorous overview of the theoretical foundations of learning \u2014 when and why algorithms generalize PhD 12: SVM Theory \u00b6 Title: \"Support Vector Machines with Applications\" Authors: Moguerza, Munoz (2006) | Pages: ~28 | arXiv:math/0612817 Focus: SVM duality theory, margin theory, kernel trick proofs, multi-class extensions Why Read: Complete mathematical treatment of SVMs with real-world application examples PhD 13: Dimensionality Reduction \u00b6 Title: \"A Survey of Dimensionality Reduction Techniques\" Authors: Sorzano, Vargas, Pascual Montano (2014) | Pages: ~35 | arXiv:1403.2877 Focus: PCA, MDS, Isomap, LLE, t-SNE, kernel PCA, manifold learning theory Why Read: Comprehensive mathematical survey covering both linear and nonlinear dimensionality reduction methods Research Frontiers (10 papers) \u00b6 These papers represent active research frontiers that every PhD student in mathematical ML should understand. PhD 14: Neural Tangent Kernel \u00b6 Title: \"Neural Tangent Kernel: Convergence and Generalization in Neural Networks\" Authors: Jacot, Gabriel, Hongler (2018) | arXiv:1806.07572 Focus: Infinite-width neural networks, kernel regime, convergence of gradient descent, lazy training Why Read: Foundational breakthrough showing that infinitely wide neural networks behave as kernel methods \u2014 bridges classical kernel theory with deep learning and provides convergence guarantees for gradient descent PhD 15: Computational Optimal Transport \u00b6 Title: \"Computational Optimal Transport\" Authors: Peyr\u00e9, Cuturi (2019) | arXiv:1803.00567 Focus: Wasserstein distances, Sinkhorn algorithm, entropy regularization, optimal transport theory Why Read: Optimal transport provides the mathematical framework behind Wasserstein GANs, distribution matching, and domain adaptation \u2014 essential for modern generative AI and understanding geometry of probability distributions PhD 16: Matrix Concentration Inequalities \u00b6 Title: \"An Introduction to Matrix Concentration Inequalities\" Authors: Tropp (2015) | arXiv:1501.01571 Focus: Matrix Bernstein, matrix Chernoff, spectral norm bounds, applications to randomized algorithms Why Read: Essential mathematical toolkit for proving guarantees about randomized matrix algorithms, random graphs, and high-dimensional statistics \u2014 the probability theory that makes modern ML theory rigorous PhD 17: Neural Ordinary Differential Equations \u00b6 Title: \"Neural Ordinary Differential Equations\" Authors: Chen, Rubanova, Bettencourt, Duvenaud (2018) | arXiv:1806.07366 Focus: Continuous-depth networks, adjoint method for backpropagation, ODE solvers as network layers Why Read: NeurIPS Best Paper \u2014 reframes neural networks as continuous dynamical systems, connecting differential equations (Topic 4) to deep learning architecture design. Shows how the adjoint method provides memory-efficient backpropagation through ODE solvers PhD 18: Diffusion Models \u00b6 Title: \"Denoising Diffusion Probabilistic Models\" Authors: Ho, Jain, Abbeel (2020) | arXiv:2006.11239 Focus: Forward/reverse diffusion processes, variational lower bound, score matching, Markov chains Why Read: The mathematical foundation of modern image generation (DALL-E, Stable Diffusion). Combines probability theory (Markov chains, Gaussian distributions), calculus (score functions, denoising), and optimization in one elegant framework PhD 19: Normalizing Flows \u00b6 Title: \"Normalizing Flows for Probabilistic Modeling and Inference\" Authors: Papamakarios, Nalisnick, Rezende, Mohamed, Lakshminarayanan (2021) | arXiv:1912.02762 Focus: Change of variables formula, invertible transformations, Jacobian computation, density estimation Why Read: Comprehensive review connecting the change-of-variables theorem from calculus (Topic 4) and probability distributions (Topic 5) to powerful generative models \u2014 demonstrates how mathematical foundations enable practical density estimation PhD 20: Graph Neural Networks \u00b6 Title: \"Semi-Supervised Classification with Graph Convolutional Networks\" Authors: Kipf, Welling (2017) | arXiv:1609.02907 Focus: Spectral graph theory, graph Laplacian, Chebyshev polynomials, message passing Why Read: Bridges spectral graph theory (eigenvalues of the Laplacian) with deep learning \u2014 shows how linear algebra on graphs enables learning from relational data (social networks, molecules, knowledge graphs) PhD 21: Representation Learning \u00b6 Title: \"Representation Learning: A Review and New Perspectives\" Authors: Bengio, Courville, Vincent (2014) | arXiv:1206.5538 Focus: Feature learning, autoencoders, manifold hypothesis, disentangled representations, deep architectures Why Read: Written by Turing Award winner Yoshua Bengio \u2014 comprehensive review of how neural networks learn useful representations from data. Connects information theory, manifold geometry, and probabilistic models to understanding what deep networks actually learn PhD 22: Double Descent and Modern Generalization \u00b6 Title: \"Reconciling Modern Machine Learning Practice and the Bias-Variance Trade-off\" Authors: Belkin, Hsu, Ma, Mandal (2019) | arXiv:1812.11118 Focus: Double descent curve, interpolation threshold, over-parameterization, classical vs modern learning theory Why Read: Challenges the classical bias-variance trade-off by showing that going beyond the interpolation threshold can improve generalization \u2014 a fundamental puzzle that reshaped our understanding of why over-parameterized neural networks generalize PhD 23: Matrix Calculus for Deep Learning \u00b6 Title: \"The Matrix Calculus You Need For Deep Learning\" Authors: Parr, Howard (2018) | arXiv:1802.01528 Focus: Jacobian matrices, chain rule for vectors/matrices, gradient computation, backpropagation derivation Why Read: The most accessible and complete reference for matrix calculus in deep learning \u2014 derives all the gradient rules needed for understanding backpropagation, from scalar derivatives to full Jacobian computations. Essential reference for implementing and understanding any neural network Reading Tips \u00b6 Before Reading: Review relevant course topics first Have the textbook nearby for reference Each paper is \u226420 pages \u2014 plan 2-3 hours for focused reading During Reading: Work through all mathematical derivations yourself Write down questions as they arise Make notes connecting to course material Don't skip the proofs \u2014 they build understanding After Reading: Summarize the main mathematical results Identify connections to other course topics Attempt practice problems if provided Prepare your written review For Bonus Papers: Choose papers that interest you or align with research goals Focus on areas where you want deeper expertise Prepare specific questions for office hour discussion Paper selection curated by Mohammed Alnemari Mathematics of AI \u2022 Spring 2026 Last Updated: February 8, 2026","title":"Papers"},{"location":"papers/#reading-review-papers","text":"Essential papers covering the mathematical foundations of machine learning and AI.","title":"Reading &amp; Review Papers"},{"location":"papers/#assignment-requirements","text":"Component Weight: 10% of final grade Requirements: Read and review the 5 required papers below throughout the semester Submit written reviews (2-3 pages each) Focus on mathematical concepts and techniques Connect paper content to course topics Review Structure: Summary of main mathematical concepts Key theorems, proofs, or derivations Connections to course material Critical analysis of mathematical approach Personal insights and questions","title":"Assignment Requirements"},{"location":"papers/#bonus-opportunity","text":"Want to go deeper? Read additional papers from the Optional Papers section below! Discuss any optional paper with me during office hours Gain deeper understanding of mathematical foundations Earn bonus points towards your final grade More papers = deeper expertise in Mathematics of AI The more you read, the stronger your mathematical foundation will become. Students who engage with optional papers consistently demonstrate superior understanding and research capability.","title":"BONUS OPPORTUNITY"},{"location":"papers/#required-papers-5-papers-must-read","text":"All papers are available on arXiv and are \u226420 pages","title":"REQUIRED PAPERS (5 Papers - Must Read)"},{"location":"papers/#required-paper-1-linear-algebra-matrix-decomposition","text":"","title":"Required Paper 1: Linear Algebra &amp; Matrix Decomposition"},{"location":"papers/#a-tutorial-on-principal-component-analysis","text":"Authors: Shlens (2014) Pages: 12 pages arXiv: 1404.1100 Topics Covered: Covariance matrices and their eigenstructure (Topics 1-2) Eigendecomposition and Singular Value Decomposition (Topic 3) Variance maximization and projection (Topic 2) Dimensionality reduction from first principles (Application 2) Why This Paper is Required: This tutorial derives PCA step-by-step from first principles, demonstrating exactly how linear algebra (Topics 1-3) powers one of the most important tools in data science. You'll see eigenvalues, eigenvectors, covariance matrices, and SVD working together in a concrete application. The paper assumes only basic linear algebra, making it the perfect bridge between course theory and practice. What Makes It Excellent: Short, clear, and self-contained Complete mathematical derivation from scratch Connects abstract linear algebra to real data analysis Directly supports Application 2 (Dimensionality Reduction) Course Topics: Topics 1, 2, 3, Application 2","title":"A Tutorial on Principal Component Analysis"},{"location":"papers/#required-paper-2-optimization-theory","text":"","title":"Required Paper 2: Optimization Theory"},{"location":"papers/#an-overview-of-gradient-descent-optimization-algorithms","text":"Authors: Ruder (2017) Pages: 14 pages arXiv: 1609.04747 Topics Covered: Gradient descent and its variants (Topic 6) Momentum, Nesterov accelerated gradient Adaptive learning rate methods (Adagrad, RMSprop, Adam) Challenges in optimization: saddle points, local minima Why This Paper is Required: This paper provides a clear, accessible overview of all the major gradient descent optimization algorithms used in modern deep learning. It explains the mathematical formulation of each optimizer, compares their properties, and discusses when to use each one. Understanding these optimizers is essential for training any neural network. What Makes It Excellent: Covers every major optimizer in one place Clear mathematical formulations with intuitive explanations Practical guidance on choosing optimizers One of the most cited optimization overviews in deep learning Course Topics: Topics 4, 6, Application 1","title":"An Overview of Gradient Descent Optimization Algorithms"},{"location":"papers/#required-paper-3-probability-generative-models","text":"","title":"Required Paper 3: Probability &amp; Generative Models"},{"location":"papers/#auto-encoding-variational-bayes","text":"Authors: Kingma, Welling (2014) Pages: 14 pages arXiv: 1312.6114 Topics Covered: Probability distributions and latent variables (Topic 5) Variational inference and the ELBO Reparameterization trick (Topics 4 + 5) Gaussian distributions in generative models Why This Paper is Required: The VAE paper is a landmark that elegantly combines probability theory (Topic 5) with optimization (Topic 6) and calculus (Topic 4). It shows how to learn probability distributions from data using neural networks. The mathematical derivation of the Evidence Lower Bound (ELBO) is a masterclass in applying Bayes' theorem, KL divergence, and expectations \u2014 all key concepts from Topic 5. What Makes It Excellent: Foundational paper for modern generative AI Elegant mathematical derivation using course concepts Demonstrates probability + optimization working together Directly connects to Application 3 (Density Estimation) Course Topics: Topics 4, 5, 6, Application 3","title":"Auto-Encoding Variational Bayes"},{"location":"papers/#required-paper-4-gradient-based-learning","text":"","title":"Required Paper 4: Gradient-Based Learning"},{"location":"papers/#adam-a-method-for-stochastic-optimization","text":"Authors: Kingma, Ba (2015) Pages: 15 pages arXiv: 1412.6980 Topics Covered: First and second moment estimation of gradients (Topics 4 + 5) Bias correction in running averages Convergence analysis for convex objectives (Topic 6) Gradient computation and adaptive learning rates Why This Paper is Required: Adam is the most widely used optimizer in deep learning. This paper derives it mathematically, showing how gradient computation (Topic 4), expected values and variance (Topic 5), and optimization theory (Topic 6) come together. The convergence proof provides a rigorous example of mathematical analysis applied to a practical algorithm. Every ML practitioner should understand this derivation. What Makes It Excellent: The default optimizer in most deep learning frameworks Clean mathematical derivation with convergence proof Combines concepts from calculus, probability, and optimization Directly applicable to all neural network training Course Topics: Topics 4, 5, 6","title":"Adam: A Method for Stochastic Optimization"},{"location":"papers/#required-paper-5-kernel-methods-svm","text":"","title":"Required Paper 5: Kernel Methods &amp; SVM"},{"location":"papers/#learning-theory-and-support-vector-machines-a-primer","text":"Authors: Banf (2019) Pages: ~15 pages arXiv: 1902.04622 Topics Covered: Statistical learning theory fundamentals Empirical vs structural risk minimization Support Vector Machines (Application 4) Kernel trick and feature spaces (Topic 2) Why This Paper is Required: This primer introduces statistical learning theory and SVMs in an accessible way. It covers the mathematical foundations of why learning from data works (risk minimization), then shows how SVMs use geometric concepts (margins, hyperplanes) from our course to build powerful classifiers. It directly supports Application 4 and ties together linear algebra (Topics 1-2) and optimization (Topic 6). What Makes It Excellent: Accessible introduction to learning theory Connects mathematical foundations to practical classification Covers both theory (risk minimization) and practice (SVM) Short and focused \u2014 ideal for course-level reading Course Topics: Topics 1, 2, 6, Application 4","title":"Learning Theory and Support Vector Machines - A Primer"},{"location":"papers/#why-these-5-papers","text":"Comprehensive Coverage: Paper 1 covers linear algebra and matrix decomposition (Topics 1-3) Paper 2 covers optimization algorithms (Topic 6) Paper 3 covers probability and generative models (Topic 5) Paper 4 covers gradient computation and adaptive methods (Topic 4) Paper 5 synthesizes multiple topics in SVM classification (Application 4) Student-Friendly: All papers are \u226420 pages Accessible mathematical rigor without being overwhelming Build progressively on course material Mix of classical foundations and modern perspectives Practical Relevance: Each paper connects directly to ML practice Cover the three main course applications (PCA, GMM, SVM) Provide foundation for understanding modern deep learning","title":"Why These 5 Papers?"},{"location":"papers/#suggested-reading-schedule","text":"Week Paper Topic Review Due 3-4 Paper 1 (PCA Tutorial) Topics 1-3 Week 5 5-6 Paper 4 (Adam Optimizer) Topic 4 Week 7 7-8 Paper 2 (Gradient Descent Overview) Topic 6 Week 9 9-10 Paper 3 (VAE) Topic 5 Week 11 11-12 Paper 5 (SVM Primer) Application 4 Week 13","title":"Suggested Reading Schedule"},{"location":"papers/#optional-papers-15-papers-for-bonus","text":"Want to earn bonus points and gain deeper understanding? Read any of these papers and discuss them with me during office hours! All papers available on arXiv and are \u226420 pages","title":"OPTIONAL PAPERS (15 Papers - For Bonus)"},{"location":"papers/#linear-algebra-matrix-theory-4-papers","text":"","title":"Linear Algebra &amp; Matrix Theory (4 papers)"},{"location":"papers/#optional-1-linear-algebra-in-transformers","text":"Title: \"Attention Is All You Need\" Authors: Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin (2017) | Pages: 15 | arXiv:1706.03762 Focus: Matrix multiplications, linear projections, scaled dot-product attention Why Read: See how linear algebra (matrix operations, projections, inner products) powers the architecture behind ChatGPT and modern AI","title":"Optional 1: Linear Algebra in Transformers"},{"location":"papers/#optional-2-matrix-factorization-in-recommender-systems","text":"Title: \"Neural Collaborative Filtering\" Authors: He, Liao, Zhang, Nie, Hu, Chua (2017) | Pages: 10 | arXiv:1708.05031 Focus: Matrix factorization extended with neural networks for recommendation Why Read: Shows how matrix decomposition concepts (Topic 3) are applied in real recommendation systems","title":"Optional 2: Matrix Factorization in Recommender Systems"},{"location":"papers/#optional-3-vector-spaces-in-nlp","text":"Title: \"Efficient Estimation of Word Representations in Vector Space\" Authors: Mikolov, Chen, Corrado, Dean (2013) | Pages: 12 | arXiv:1301.3781 Focus: Word embeddings as vectors, linear relationships in semantic space Why Read: Demonstrates that words can be represented as vectors where linear algebra operations capture meaning (king - man + woman = queen)","title":"Optional 3: Vector Spaces in NLP"},{"location":"papers/#optional-4-normalization-and-matrix-statistics","text":"Title: \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\" Authors: Ioffe, Szegedy (2015) | Pages: 11 | arXiv:1502.03167 Focus: Statistics of activations, normalization transformations, gradient flow Why Read: Shows how matrix statistics (mean, variance) and normalization improve neural network training","title":"Optional 4: Normalization and Matrix Statistics"},{"location":"papers/#optimization-methods-4-papers","text":"","title":"Optimization Methods (4 papers)"},{"location":"papers/#optional-5-escaping-saddle-points","text":"Title: \"How to Escape Saddle Points Efficiently\" Authors: Jin, Ge, Netrapalli, Kakade, Jordan (2017) | Pages: 9 | arXiv:1703.00887 Focus: Non-convex optimization, perturbed gradient descent, convergence to second-order stationary points Why Read: Explains mathematically why gradient descent succeeds in non-convex deep learning landscapes","title":"Optional 5: Escaping Saddle Points"},{"location":"papers/#optional-6-loss-landscape-visualization","text":"Title: \"Visualizing the Loss Landscape of Neural Nets\" Authors: Li, Xu, Taylor, Studer, Goldstein (2018) | Pages: 14 | arXiv:1712.09913 Focus: Loss surface geometry, filter normalization, effect of skip connections Why Read: Visual and geometric understanding of why some networks are easier to optimize than others","title":"Optional 6: Loss Landscape Visualization"},{"location":"papers/#optional-7-bayesian-hyperparameter-optimization","text":"Title: \"Practical Bayesian Optimization of Machine Learning Algorithms\" Authors: Snoek, Larochelle, Adams (2012) | Pages: 9 | arXiv:1206.2944 Focus: Gaussian processes for hyperparameter tuning, acquisition functions Why Read: Elegant application of probability (Gaussian processes) to the practical problem of tuning ML models","title":"Optional 7: Bayesian Hyperparameter Optimization"},{"location":"papers/#optional-8-adversarial-optimization","text":"Title: \"Generative Adversarial Nets\" Authors: Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, Bengio (2014) | Pages: 9 | arXiv:1406.2661 Focus: Minimax optimization, game theory, probability distribution matching Why Read: Foundational paper for generative AI, shows optimization as a two-player game between neural networks","title":"Optional 8: Adversarial Optimization"},{"location":"papers/#probability-statistics-3-papers","text":"","title":"Probability &amp; Statistics (3 papers)"},{"location":"papers/#optional-9-em-algorithm","text":"Title: \"EM Algorithm and Variants: An Informal Tutorial\" Authors: Roche (2012) | Pages: 20 | arXiv:1105.1476 Focus: Expectation-Maximization algorithm, mixture models, convergence properties Why Read: Clear tutorial on the EM algorithm, directly relevant to GMM (Application 3)","title":"Optional 9: EM Algorithm"},{"location":"papers/#optional-10-information-theory-deep-learning","text":"Title: \"Deep Learning and the Information Bottleneck Principle\" Authors: Tishby, Zaslavsky (2015) | Pages: 5 | arXiv:1503.02406 Focus: Information bottleneck, mutual information, deep network layers as compression Why Read: Connects information theory to understanding why deep learning works \u2014 layers progressively compress information","title":"Optional 10: Information Theory &amp; Deep Learning"},{"location":"papers/#optional-11-uncertainty-in-deep-learning","text":"Title: \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\" Authors: Gal, Ghahramani (2016) | Pages: 12 | arXiv:1506.02142 Focus: Bayesian inference, model uncertainty, dropout as variational inference Why Read: Beautiful connection between a practical technique (dropout) and Bayesian probability theory","title":"Optional 11: Uncertainty in Deep Learning"},{"location":"papers/#deep-learning-theory-4-papers","text":"","title":"Deep Learning Theory (4 papers)"},{"location":"papers/#optional-12-neural-network-approximation-theory","text":"Title: \"Deep vs. Shallow Networks: An Approximation Theory Perspective\" Authors: Mhaskar, Poggio (2016) | Pages: 8 | arXiv:1608.03287 Focus: Function approximation, depth vs width, compositional functions Why Read: Rigorous analysis of why deep networks outperform shallow ones \u2014 depth enables exponential efficiency","title":"Optional 12: Neural Network Approximation Theory"},{"location":"papers/#optional-13-sparse-networks","text":"Title: \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\" Authors: Frankle, Carlin (2019) | Pages: ~15 | arXiv:1803.03635 Focus: Network pruning, sparse subnetworks, weight initialization Why Read: Reveals that small subnetworks within large networks can match full network performance \u2014 implications for efficiency","title":"Optional 13: Sparse Networks"},{"location":"papers/#optional-14-knowledge-distillation","text":"Title: \"Distilling the Knowledge in a Neural Network\" Authors: Hinton, Vinyals, Dean (2015) | Pages: 9 | arXiv:1503.02531 Focus: Model compression, soft targets, temperature scaling of probability distributions Why Read: Shows how probability distributions (softmax outputs) can transfer knowledge between networks","title":"Optional 14: Knowledge Distillation"},{"location":"papers/#optional-15-rethinking-generalization","text":"Title: \"Understanding Deep Learning Requires Rethinking Generalization\" Authors: Zhang, Bengio, Hardt, Recht, Vinyals (2017) | Pages: ~15 | arXiv:1611.03530 Focus: Generalization theory, memorization vs learning, role of regularization Why Read: Challenges classical learning theory by showing neural networks can memorize random labels \u2014 a foundational puzzle in ML theory","title":"Optional 15: Rethinking Generalization"},{"location":"papers/#how-to-earn-bonus-points","text":"Read any optional paper(s) from the list above Schedule office hours to discuss the paper with me Come prepared with: Summary of main mathematical concepts Connections to course material Questions or insights How it extends your understanding Bonus Structure: 1 optional paper discussed = +2% bonus on final grade 2 optional papers discussed = +4% bonus on final grade 3+ optional papers discussed = +5% bonus on final grade (maximum) Quality matters: Superficial reading won't earn bonus points. Show deep engagement with the mathematical content!","title":"How to Earn Bonus Points"},{"location":"papers/#paper-topic-coverage-matrix","text":"","title":"Paper-Topic Coverage Matrix"},{"location":"papers/#required-papers","text":"Paper Topics 1-3 (LA) Topic 4 (Calc) Topic 5 (Prob) Topic 6 (Opt) Applications 1. PCA Tutorial \u2713\u2713\u2713 App 2 2. Gradient Descent Overview \u2713\u2713 \u2713\u2713\u2713 Training 3. VAE \u2713 \u2713\u2713\u2713 \u2713\u2713 App 3 4. Adam Optimizer \u2713\u2713\u2713 \u2713 \u2713\u2713 Training 5. SVM Primer \u2713\u2713 \u2713\u2713 App 4","title":"Required Papers"},{"location":"papers/#optional-papers-coverage","text":"Linear Algebra Deep Dives: Optional 1-4 (Transformers, Recommenders, Word Vectors, Normalization) Optimization Deep Dives: Optional 5-8 (Saddle Points, Loss Landscapes, Bayesian Opt, GANs) Probability Deep Dives: Optional 9-11 (EM Algorithm, Information Theory, Bayesian Dropout) Deep Learning Theory: Optional 12-15 (Approximation, Lottery Tickets, Distillation, Generalization)","title":"Optional Papers Coverage"},{"location":"papers/#phd-level-papers-advanced-reading","text":"For graduate students and researchers seeking deeper mathematical foundations. These are comprehensive, longer papers (20-160 pages) that provide rigorous treatments with full proofs and advanced theory.","title":"PhD-Level Papers (Advanced Reading)"},{"location":"papers/#foundational-theory-5-papers","text":"","title":"Foundational Theory (5 papers)"},{"location":"papers/#phd-1-mathematical-foundations-of-deep-learning","text":"Title: \"The Modern Mathematics of Deep Learning\" Authors: Berner, Grohs, Kutyniok, Petersen (2021) | Pages: ~60 | arXiv:2105.04026 Focus: Complete mathematical treatment of deep learning theory \u2014 vector spaces, function approximation, optimization, and generalization Why Read: The most comprehensive mathematical survey connecting linear algebra, analysis, and approximation theory to deep learning","title":"PhD 1: Mathematical Foundations of Deep Learning"},{"location":"papers/#phd-2-large-scale-optimization-theory","text":"Title: \"Optimization Methods for Large-Scale Machine Learning\" Authors: Bottou, Curtis, Nocedal (2018) | Pages: ~90 | arXiv:1606.04838 Focus: Complete convergence analysis, noise reduction methods, second-order methods, stochastic gradient theory Why Read: The definitive reference on optimization for ML with full mathematical proofs of convergence rates","title":"PhD 2: Large-Scale Optimization Theory"},{"location":"papers/#phd-3-variational-inference","text":"Title: \"Variational Inference: A Review for Statisticians\" Authors: Blei, Kucukelbir, McAuliffe (2017) | Pages: ~33 | arXiv:1601.00670 Focus: Complete variational inference framework, ELBO derivation, mean-field methods, stochastic variational inference Why Read: Rigorous treatment of how optimization approximates Bayesian inference \u2014 connects probability theory to practical algorithms","title":"PhD 3: Variational Inference"},{"location":"papers/#phd-4-gradient-based-training","text":"Title: \"Practical Recommendations for Gradient-Based Training of Deep Architectures\" Authors: Bengio (2012) | Pages: ~35 | arXiv:1206.5533 Focus: Learning rate theory, momentum analysis, weight initialization, preprocessing, hyperparameter optimization Why Read: Written by Turing Award winner Yoshua Bengio \u2014 comprehensive guide to training neural networks with mathematical justification","title":"PhD 4: Gradient-Based Training"},{"location":"papers/#phd-5-kernel-methods-and-rkhs-theory","text":"Title: \"Kernel Methods in Machine Learning\" Authors: Hofmann, Sch\u00f6lkopf, Smola (2008) | Pages: ~50 | arXiv:math/0701907 Focus: Reproducing Kernel Hilbert Spaces, Mercer's theorem, kernel design, representer theorem, SVM duality Why Read: Published in Annals of Statistics \u2014 the definitive mathematical treatment of kernel methods by the field's pioneers","title":"PhD 5: Kernel Methods and RKHS Theory"},{"location":"papers/#advanced-topics-8-papers","text":"","title":"Advanced Topics (8 papers)"},{"location":"papers/#phd-6-randomized-numerical-linear-algebra","text":"Title: \"Randomized Numerical Linear Algebra: Foundations and Algorithms\" Authors: Martinsson, Tropp (2020) | Pages: ~90 | arXiv:2002.01387 Focus: Randomized SVD, matrix sketching, Johnson-Lindenstrauss, low-rank approximation algorithms Why Read: Comprehensive survey of probabilistic approaches to large-scale matrix computations","title":"PhD 6: Randomized Numerical Linear Algebra"},{"location":"papers/#phd-7-random-matrix-theory-for-neural-networks","text":"Title: \"A Random Matrix Approach to Neural Networks\" Authors: Louart, Liao, Couillet (2017) | Pages: ~30 | arXiv:1702.05419 Focus: Spectral analysis of Gram matrices, deterministic equivalents, concentration inequalities Why Read: Applies random matrix theory to rigorously analyze neural network behavior","title":"PhD 7: Random Matrix Theory for Neural Networks"},{"location":"papers/#phd-8-convex-optimization","text":"Title: \"Convex Optimization: Algorithms and Complexity\" Authors: Bubeck (2015) | Pages: ~130 | arXiv:1405.4980 Focus: Black-box optimization, accelerated methods, mirror descent, interior point methods, stochastic methods Why Read: Self-contained monograph covering all of convex optimization theory \u2014 the mathematical backbone of ML","title":"PhD 8: Convex Optimization"},{"location":"papers/#phd-9-non-convex-optimization","text":"Title: \"Non-convex Optimization for Machine Learning\" Authors: Jain, Kar (2017) | Pages: ~160 | arXiv:1712.07897 Focus: Alternating minimization, EM convergence, matrix completion, phase retrieval, non-convex landscape analysis Why Read: Comprehensive treatment of why and when non-convex optimization succeeds in machine learning","title":"PhD 9: Non-Convex Optimization"},{"location":"papers/#phd-10-bayesian-optimization","text":"Title: \"A Tutorial on Bayesian Optimization\" Authors: Frazier (2018) | Pages: ~35 | arXiv:1807.02811 Focus: Gaussian process regression, acquisition functions, theoretical guarantees, practical considerations Why Read: Complete mathematical framework for Bayesian optimization with full derivations","title":"PhD 10: Bayesian Optimization"},{"location":"papers/#phd-11-statistical-learning-theory","text":"Title: \"Statistical Learning Theory: Models, Concepts, and Results\" Authors: von Luxburg, Sch\u00f6lkopf (2008) | Pages: ~26 | arXiv:0810.4752 Focus: PAC learning, VC dimension, Rademacher complexity, generalization bounds, model selection Why Read: Rigorous overview of the theoretical foundations of learning \u2014 when and why algorithms generalize","title":"PhD 11: Statistical Learning Theory"},{"location":"papers/#phd-12-svm-theory","text":"Title: \"Support Vector Machines with Applications\" Authors: Moguerza, Munoz (2006) | Pages: ~28 | arXiv:math/0612817 Focus: SVM duality theory, margin theory, kernel trick proofs, multi-class extensions Why Read: Complete mathematical treatment of SVMs with real-world application examples","title":"PhD 12: SVM Theory"},{"location":"papers/#phd-13-dimensionality-reduction","text":"Title: \"A Survey of Dimensionality Reduction Techniques\" Authors: Sorzano, Vargas, Pascual Montano (2014) | Pages: ~35 | arXiv:1403.2877 Focus: PCA, MDS, Isomap, LLE, t-SNE, kernel PCA, manifold learning theory Why Read: Comprehensive mathematical survey covering both linear and nonlinear dimensionality reduction methods","title":"PhD 13: Dimensionality Reduction"},{"location":"papers/#research-frontiers-10-papers","text":"These papers represent active research frontiers that every PhD student in mathematical ML should understand.","title":"Research Frontiers (10 papers)"},{"location":"papers/#phd-14-neural-tangent-kernel","text":"Title: \"Neural Tangent Kernel: Convergence and Generalization in Neural Networks\" Authors: Jacot, Gabriel, Hongler (2018) | arXiv:1806.07572 Focus: Infinite-width neural networks, kernel regime, convergence of gradient descent, lazy training Why Read: Foundational breakthrough showing that infinitely wide neural networks behave as kernel methods \u2014 bridges classical kernel theory with deep learning and provides convergence guarantees for gradient descent","title":"PhD 14: Neural Tangent Kernel"},{"location":"papers/#phd-15-computational-optimal-transport","text":"Title: \"Computational Optimal Transport\" Authors: Peyr\u00e9, Cuturi (2019) | arXiv:1803.00567 Focus: Wasserstein distances, Sinkhorn algorithm, entropy regularization, optimal transport theory Why Read: Optimal transport provides the mathematical framework behind Wasserstein GANs, distribution matching, and domain adaptation \u2014 essential for modern generative AI and understanding geometry of probability distributions","title":"PhD 15: Computational Optimal Transport"},{"location":"papers/#phd-16-matrix-concentration-inequalities","text":"Title: \"An Introduction to Matrix Concentration Inequalities\" Authors: Tropp (2015) | arXiv:1501.01571 Focus: Matrix Bernstein, matrix Chernoff, spectral norm bounds, applications to randomized algorithms Why Read: Essential mathematical toolkit for proving guarantees about randomized matrix algorithms, random graphs, and high-dimensional statistics \u2014 the probability theory that makes modern ML theory rigorous","title":"PhD 16: Matrix Concentration Inequalities"},{"location":"papers/#phd-17-neural-ordinary-differential-equations","text":"Title: \"Neural Ordinary Differential Equations\" Authors: Chen, Rubanova, Bettencourt, Duvenaud (2018) | arXiv:1806.07366 Focus: Continuous-depth networks, adjoint method for backpropagation, ODE solvers as network layers Why Read: NeurIPS Best Paper \u2014 reframes neural networks as continuous dynamical systems, connecting differential equations (Topic 4) to deep learning architecture design. Shows how the adjoint method provides memory-efficient backpropagation through ODE solvers","title":"PhD 17: Neural Ordinary Differential Equations"},{"location":"papers/#phd-18-diffusion-models","text":"Title: \"Denoising Diffusion Probabilistic Models\" Authors: Ho, Jain, Abbeel (2020) | arXiv:2006.11239 Focus: Forward/reverse diffusion processes, variational lower bound, score matching, Markov chains Why Read: The mathematical foundation of modern image generation (DALL-E, Stable Diffusion). Combines probability theory (Markov chains, Gaussian distributions), calculus (score functions, denoising), and optimization in one elegant framework","title":"PhD 18: Diffusion Models"},{"location":"papers/#phd-19-normalizing-flows","text":"Title: \"Normalizing Flows for Probabilistic Modeling and Inference\" Authors: Papamakarios, Nalisnick, Rezende, Mohamed, Lakshminarayanan (2021) | arXiv:1912.02762 Focus: Change of variables formula, invertible transformations, Jacobian computation, density estimation Why Read: Comprehensive review connecting the change-of-variables theorem from calculus (Topic 4) and probability distributions (Topic 5) to powerful generative models \u2014 demonstrates how mathematical foundations enable practical density estimation","title":"PhD 19: Normalizing Flows"},{"location":"papers/#phd-20-graph-neural-networks","text":"Title: \"Semi-Supervised Classification with Graph Convolutional Networks\" Authors: Kipf, Welling (2017) | arXiv:1609.02907 Focus: Spectral graph theory, graph Laplacian, Chebyshev polynomials, message passing Why Read: Bridges spectral graph theory (eigenvalues of the Laplacian) with deep learning \u2014 shows how linear algebra on graphs enables learning from relational data (social networks, molecules, knowledge graphs)","title":"PhD 20: Graph Neural Networks"},{"location":"papers/#phd-21-representation-learning","text":"Title: \"Representation Learning: A Review and New Perspectives\" Authors: Bengio, Courville, Vincent (2014) | arXiv:1206.5538 Focus: Feature learning, autoencoders, manifold hypothesis, disentangled representations, deep architectures Why Read: Written by Turing Award winner Yoshua Bengio \u2014 comprehensive review of how neural networks learn useful representations from data. Connects information theory, manifold geometry, and probabilistic models to understanding what deep networks actually learn","title":"PhD 21: Representation Learning"},{"location":"papers/#phd-22-double-descent-and-modern-generalization","text":"Title: \"Reconciling Modern Machine Learning Practice and the Bias-Variance Trade-off\" Authors: Belkin, Hsu, Ma, Mandal (2019) | arXiv:1812.11118 Focus: Double descent curve, interpolation threshold, over-parameterization, classical vs modern learning theory Why Read: Challenges the classical bias-variance trade-off by showing that going beyond the interpolation threshold can improve generalization \u2014 a fundamental puzzle that reshaped our understanding of why over-parameterized neural networks generalize","title":"PhD 22: Double Descent and Modern Generalization"},{"location":"papers/#phd-23-matrix-calculus-for-deep-learning","text":"Title: \"The Matrix Calculus You Need For Deep Learning\" Authors: Parr, Howard (2018) | arXiv:1802.01528 Focus: Jacobian matrices, chain rule for vectors/matrices, gradient computation, backpropagation derivation Why Read: The most accessible and complete reference for matrix calculus in deep learning \u2014 derives all the gradient rules needed for understanding backpropagation, from scalar derivatives to full Jacobian computations. Essential reference for implementing and understanding any neural network","title":"PhD 23: Matrix Calculus for Deep Learning"},{"location":"papers/#reading-tips","text":"Before Reading: Review relevant course topics first Have the textbook nearby for reference Each paper is \u226420 pages \u2014 plan 2-3 hours for focused reading During Reading: Work through all mathematical derivations yourself Write down questions as they arise Make notes connecting to course material Don't skip the proofs \u2014 they build understanding After Reading: Summarize the main mathematical results Identify connections to other course topics Attempt practice problems if provided Prepare your written review For Bonus Papers: Choose papers that interest you or align with research goals Focus on areas where you want deeper expertise Prepare specific questions for office hour discussion Paper selection curated by Mohammed Alnemari Mathematics of AI \u2022 Spring 2026 Last Updated: February 8, 2026","title":"Reading Tips"},{"location":"tutorials/","text":"Math Tutorials \u00b6 Detailed mathematical notes, derivations, and tutorials to supplement lectures. Getting Started \u00b6 Tutorial 1: Mathematical Foundations and Terminology \u00b6 Essential notation, terminology, and basic concepts you need to understand before starting the course. Read Tutorial What you'll learn: Mathematical notation and symbols ( \\(\\in, \\forall, \\exists, \\sum, \\prod\\) ) Number systems ( \\(\\mathbb{N}, \\mathbb{Z}, \\mathbb{R}, \\mathbb{C}\\) ) Vectors and matrices basics Essential operations (dot product, matrix multiplication) Linear independence, span, and basis Norms and distance Practice problems with solutions Lecture Tutorials \u00b6 Tutorial 2: Linear Algebra \u00b6 The computational foundation of machine learning \u2014 systems of equations, vector spaces, and linear mappings. Read Tutorial What you'll learn: Systems of linear equations and their geometric interpretation Matrices: operations, inverse, transpose Gaussian elimination and row echelon form Vector spaces, subspaces, and the 3-step subspace test Linear independence, span, basis, and dimension Rank, kernel, and image of linear mappings Rank-Nullity Theorem Change of basis and affine spaces 8 practice problems with solutions Tutorial 3: Analytic Geometry \u00b6 The geometry behind machine learning \u2014 norms, inner products, projections, and the Gram-Schmidt process. Read Tutorial What you'll learn: Norms ( \\(\\ell_1\\) , \\(\\ell_2\\) , \\(\\ell_\\infty\\) ) and their properties Inner products and positive definite matrices Cauchy-Schwarz inequality and distances Angles, orthogonality, and orthonormal bases Orthogonal complement and its relation to the kernel Projections onto lines and general subspaces Gram-Schmidt orthogonalization process Rotation matrices and their properties Practice problems with solutions Tutorial 4: Matrix Decomposition \u00b6 Reveals the hidden structure in matrices \u2014 eigenvalues, SVD, and matrix approximation. Read Tutorial What you'll learn: Determinants and trace: computation and properties Eigenvalues and eigenvectors: characteristic polynomial, computation Cholesky decomposition for positive definite matrices Eigendecomposition and diagonalization Singular Value Decomposition (SVD): geometric interpretation Matrix approximation via truncated SVD Practice problems with solutions Tutorial 5: Vector Calculus \u00b6 The mathematics of learning \u2014 from derivatives to backpropagation. Read Tutorial What you'll learn: Differentiation rules and Taylor series Partial derivatives and gradients Jacobians for vector-valued functions Matrix calculus and useful gradient identities Chain rule for multivariate functions Backpropagation and computation graphs Hessian matrix and second-order methods Practice problems with solutions Tutorial 6: Probability and Distributions \u00b6 The language of uncertainty \u2014 from sample spaces to Gaussian distributions. Read Tutorial What you'll learn: Probability spaces and axioms (Kolmogorov) Conditional probability and Bayes' Theorem Discrete distributions (Bernoulli, Binomial, Geometric) Continuous distributions (Uniform, Exponential, Gaussian) Expected value, variance, and computation rules Joint and marginal distributions Covariance, correlation, and independence Common distributions reference table Practice problems with solutions Tutorial Format \u00b6 Each tutorial includes: Motivation - Why this topic matters for AI/ML Mathematical Theory - Rigorous treatment with definitions Worked Examples - Step-by-step solutions Practice Problems - With full solutions Key Takeaways - Summary of essential concepts How to Use These Tutorials \u00b6 Study Strategy: Read tutorial before corresponding lecture Work through examples by hand Attempt practice problems before checking solutions Use as reference during homework Review before exams Tips: Don't skip the proofs \u2014 they build understanding Work examples yourself before checking solutions Connect abstract concepts to concrete ML applications Build your own formula sheet as you go Tutorials curated by Mohammed Alnemari Mathematics of AI \u2022 Spring 2026 Last Updated: February 8, 2026","title":"Math Tutorials"},{"location":"tutorials/#math-tutorials","text":"Detailed mathematical notes, derivations, and tutorials to supplement lectures.","title":"Math Tutorials"},{"location":"tutorials/#getting-started","text":"","title":"Getting Started"},{"location":"tutorials/#tutorial-1-mathematical-foundations-and-terminology","text":"Essential notation, terminology, and basic concepts you need to understand before starting the course. Read Tutorial What you'll learn: Mathematical notation and symbols ( \\(\\in, \\forall, \\exists, \\sum, \\prod\\) ) Number systems ( \\(\\mathbb{N}, \\mathbb{Z}, \\mathbb{R}, \\mathbb{C}\\) ) Vectors and matrices basics Essential operations (dot product, matrix multiplication) Linear independence, span, and basis Norms and distance Practice problems with solutions","title":"Tutorial 1: Mathematical Foundations and Terminology"},{"location":"tutorials/#lecture-tutorials","text":"","title":"Lecture Tutorials"},{"location":"tutorials/#tutorial-2-linear-algebra","text":"The computational foundation of machine learning \u2014 systems of equations, vector spaces, and linear mappings. Read Tutorial What you'll learn: Systems of linear equations and their geometric interpretation Matrices: operations, inverse, transpose Gaussian elimination and row echelon form Vector spaces, subspaces, and the 3-step subspace test Linear independence, span, basis, and dimension Rank, kernel, and image of linear mappings Rank-Nullity Theorem Change of basis and affine spaces 8 practice problems with solutions","title":"Tutorial 2: Linear Algebra"},{"location":"tutorials/#tutorial-3-analytic-geometry","text":"The geometry behind machine learning \u2014 norms, inner products, projections, and the Gram-Schmidt process. Read Tutorial What you'll learn: Norms ( \\(\\ell_1\\) , \\(\\ell_2\\) , \\(\\ell_\\infty\\) ) and their properties Inner products and positive definite matrices Cauchy-Schwarz inequality and distances Angles, orthogonality, and orthonormal bases Orthogonal complement and its relation to the kernel Projections onto lines and general subspaces Gram-Schmidt orthogonalization process Rotation matrices and their properties Practice problems with solutions","title":"Tutorial 3: Analytic Geometry"},{"location":"tutorials/#tutorial-4-matrix-decomposition","text":"Reveals the hidden structure in matrices \u2014 eigenvalues, SVD, and matrix approximation. Read Tutorial What you'll learn: Determinants and trace: computation and properties Eigenvalues and eigenvectors: characteristic polynomial, computation Cholesky decomposition for positive definite matrices Eigendecomposition and diagonalization Singular Value Decomposition (SVD): geometric interpretation Matrix approximation via truncated SVD Practice problems with solutions","title":"Tutorial 4: Matrix Decomposition"},{"location":"tutorials/#tutorial-5-vector-calculus","text":"The mathematics of learning \u2014 from derivatives to backpropagation. Read Tutorial What you'll learn: Differentiation rules and Taylor series Partial derivatives and gradients Jacobians for vector-valued functions Matrix calculus and useful gradient identities Chain rule for multivariate functions Backpropagation and computation graphs Hessian matrix and second-order methods Practice problems with solutions","title":"Tutorial 5: Vector Calculus"},{"location":"tutorials/#tutorial-6-probability-and-distributions","text":"The language of uncertainty \u2014 from sample spaces to Gaussian distributions. Read Tutorial What you'll learn: Probability spaces and axioms (Kolmogorov) Conditional probability and Bayes' Theorem Discrete distributions (Bernoulli, Binomial, Geometric) Continuous distributions (Uniform, Exponential, Gaussian) Expected value, variance, and computation rules Joint and marginal distributions Covariance, correlation, and independence Common distributions reference table Practice problems with solutions","title":"Tutorial 6: Probability and Distributions"},{"location":"tutorials/#tutorial-format","text":"Each tutorial includes: Motivation - Why this topic matters for AI/ML Mathematical Theory - Rigorous treatment with definitions Worked Examples - Step-by-step solutions Practice Problems - With full solutions Key Takeaways - Summary of essential concepts","title":"Tutorial Format"},{"location":"tutorials/#how-to-use-these-tutorials","text":"Study Strategy: Read tutorial before corresponding lecture Work through examples by hand Attempt practice problems before checking solutions Use as reference during homework Review before exams Tips: Don't skip the proofs \u2014 they build understanding Work examples yourself before checking solutions Connect abstract concepts to concrete ML applications Build your own formula sheet as you go Tutorials curated by Mohammed Alnemari Mathematics of AI \u2022 Spring 2026 Last Updated: February 8, 2026","title":"How to Use These Tutorials"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/","text":"Tutorial 1: Mathematical Foundations and Terminology \u00b6 Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari \ud83d\udcda Learning Objectives \u00b6 By the end of this tutorial, you will understand: Essential mathematical notation and terminology Basic set theory concepts Fundamental number systems Vector and matrix terminology Key mathematical operations Part 1: Mathematical Notation \u00b6 1.1 Basic Symbols \u00b6 Symbol Meaning Example \\(\\in\\) \"is an element of\" / \"belongs to\" \\(x \\in \\mathbb{R}\\) means \"x is a real number\" \\(\\notin\\) \"is not an element of\" \\(i \\notin \\mathbb{R}\\) \\(\\subset\\) \"is a subset of\" \\(\\mathbb{N} \\subset \\mathbb{Z}\\) \\(\\cup\\) \"union\" \\(A \\cup B\\) (all elements in A or B) \\(\\cap\\) \"intersection\" \\(A \\cap B\\) (elements in both A and B) \\(\\emptyset\\) \"empty set\" A set with no elements \\(\\forall\\) \"for all\" \\(\\forall x \\in \\mathbb{R}\\) means \"for all x in real numbers\" \\(\\exists\\) \"there exists\" \\(\\exists x : x > 0\\) means \"there exists an x such that x > 0\" 1.2 Common Notation Conventions \u00b6 Scalars (single numbers): - Lower case letters: \\(a, b, c, x, y, z\\) - Greek letters: \\(\\alpha, \\beta, \\gamma, \\lambda, \\mu\\) Vectors (ordered lists of numbers): - Bold lower case: \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{v}, \\mathbf{w}\\) - Sometimes with arrows: \\(\\vec{x}, \\vec{y}\\) Matrices (rectangular arrays of numbers): - Bold upper case: \\(\\mathbf{A}, \\mathbf{B}, \\mathbf{X}, \\mathbf{Y}\\) - Sometimes just upper case: \\(A, B, X, Y\\) Sets : - Upper case letters: \\(A, B, S, V\\) - Number systems: \\(\\mathbb{N}, \\mathbb{Z}, \\mathbb{Q}, \\mathbb{R}, \\mathbb{C}\\) Part 2: Number Systems \u00b6 2.1 Common Number Systems \u00b6 Symbol Name Description Examples \\(\\mathbb{N}\\) Natural numbers Positive integers \\(1, 2, 3, 4, \\ldots\\) \\(\\mathbb{Z}\\) Integers Whole numbers \\(\\ldots, -2, -1, 0, 1, 2, \\ldots\\) \\(\\mathbb{Q}\\) Rational numbers Fractions \\(\\frac{1}{2}, \\frac{3}{4}, -\\frac{5}{3}\\) \\(\\mathbb{R}\\) Real numbers All numbers on number line \\(\\pi, \\sqrt{2}, -3.5\\) \\(\\mathbb{C}\\) Complex numbers Numbers with imaginary part \\(2 + 3i, -1 + i\\) 2.2 Vector Spaces \u00b6 \\(\\mathbb{R}^n\\) - The set of all n-dimensional real vectors \\(\\mathbb{R}^1\\) = Real numbers (1D) \\(\\mathbb{R}^2\\) = Pairs of real numbers (2D plane) \\(\\mathbb{R}^3\\) = Triples of real numbers (3D space) \\(\\mathbb{R}^n\\) = n-tuples of real numbers (n-dimensional space) Example: $ \\(\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\in \\mathbb{R}^2\\) $ This means \\(\\mathbf{x}\\) is a 2-dimensional vector with components 2 and 3. Part 3: Vectors \u00b6 3.1 What is a Vector? \u00b6 A vector is an ordered list of numbers representing: - A point in space - A direction and magnitude - Features in machine learning Example in \\(\\mathbb{R}^3\\) : $ \\(\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\) $ 3.2 Vector Terminology \u00b6 Dimension : The number of components in a vector Notation: - Column vector (default): \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\) - Row vector: \\(\\mathbf{x}^T = \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix}\\) Components/Entries : The individual numbers in a vector - \\(x_1\\) is the first component - \\(x_2\\) is the second component - \\(x_i\\) is the i-th component 3.3 Basic Vector Operations \u00b6 Addition \u00b6 \\[\\mathbf{a} + \\mathbf{b} = \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} = \\begin{bmatrix} a_1 + b_1 \\\\ a_2 + b_2 \\end{bmatrix}\\] Example: $ \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix}\\) $ Scalar Multiplication \u00b6 \\[c \\cdot \\mathbf{a} = c \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix} = \\begin{bmatrix} c \\cdot a_1 \\\\ c \\cdot a_2 \\end{bmatrix}\\] Example: $ \\(3 \\cdot \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\\) $ Dot Product (Inner Product) \u00b6 \\[\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n = \\sum_{i=1}^{n} a_i b_i\\] Example: $ \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\cdot \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} = 1(4) + 2(5) + 3(6) = 4 + 10 + 18 = 32\\) $ Part 4: Matrices \u00b6 4.1 What is a Matrix? \u00b6 A matrix is a rectangular array of numbers arranged in rows and columns. General form: $ \\(\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{bmatrix}\\) $ 4.2 Matrix Terminology \u00b6 Dimension/Size : \\(m \\times n\\) where - \\(m\\) = number of rows - \\(n\\) = number of columns Example: $ \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}_{2 \\times 3}\\) $ This is a \\(2 \\times 3\\) matrix (2 rows, 3 columns). Entry/Element : \\(a_{ij}\\) is the element in row \\(i\\) , column \\(j\\) Square Matrix : A matrix where \\(m = n\\) (same number of rows and columns) Example: $ \\(\\mathbf{B} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}_{2 \\times 2}\\) $ 4.3 Special Matrices \u00b6 Identity Matrix ( \\(\\mathbf{I}\\) ) \u00b6 A square matrix with 1s on the diagonal and 0s elsewhere. \\[\\mathbf{I}_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\] Property: \\(\\mathbf{A} \\mathbf{I} = \\mathbf{I} \\mathbf{A} = \\mathbf{A}\\) Zero Matrix \u00b6 A matrix where all elements are zero. \\[\\mathbf{0} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}\\] Diagonal Matrix \u00b6 A square matrix where all off-diagonal elements are zero. \\[\\mathbf{D} = \\begin{bmatrix} d_1 & 0 & 0 \\\\ 0 & d_2 & 0 \\\\ 0 & 0 & d_3 \\end{bmatrix}\\] Transpose ( \\(\\mathbf{A}^T\\) ) \u00b6 Flip rows and columns. Example: $ \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}, \\quad \\mathbf{A}^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\) $ Rule: \\((a_{ij})^T = a_{ji}\\) Part 5: Matrix Operations \u00b6 5.1 Matrix Addition \u00b6 Add corresponding elements (matrices must have same dimensions). \\[\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} a_{11} + b_{11} & a_{12} + b_{12} \\\\ a_{21} + b_{21} & a_{22} + b_{22} \\end{bmatrix}\\] Example: $ \\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}\\) $ 5.2 Scalar Multiplication \u00b6 Multiply every element by a scalar. \\[c \\mathbf{A} = \\begin{bmatrix} c \\cdot a_{11} & c \\cdot a_{12} \\\\ c \\cdot a_{21} & c \\cdot a_{22} \\end{bmatrix}\\] Example: $ \\(2 \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 2 & 4 \\\\ 6 & 8 \\end{bmatrix}\\) $ 5.3 Matrix Multiplication \u00b6 Rule: To multiply \\(\\mathbf{A}_{m \\times n}\\) and \\(\\mathbf{B}_{n \\times p}\\) : - Number of columns in \\(\\mathbf{A}\\) must equal number of rows in \\(\\mathbf{B}\\) - Result is \\(\\mathbf{C}_{m \\times p}\\) Formula: $ \\(c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\\) $ Example: $ \\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1(5) + 2(7) & 1(6) + 2(8) \\\\ 3(5) + 4(7) & 3(6) + 4(8) \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\\) $ Important: Matrix multiplication is NOT commutative - \\(\\mathbf{AB} \\neq \\mathbf{BA}\\) (in general) Part 6: Important Concepts \u00b6 6.1 Linear Combination \u00b6 A sum of scalar multiples of vectors. \\[\\mathbf{v} = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n\\] Example: $ \\(\\mathbf{v} = 2\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 3\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\) $ 6.2 Linear Independence \u00b6 Vectors are linearly independent if no vector can be written as a linear combination of the others. Example (Independent): $ \\(\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) $ Example (Dependent): $ \\(\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\\) $ (Here \\(\\mathbf{v}_2 = 2\\mathbf{v}_1\\) ) 6.3 Span \u00b6 The span of a set of vectors is all possible linear combinations of those vectors. \\[\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\} = \\{c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n : c_i \\in \\mathbb{R}\\}\\] 6.4 Basis \u00b6 A basis for a vector space is a set of linearly independent vectors that span the space. Standard basis for \\(\\mathbb{R}^3\\) : $ \\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\) $ 6.5 Dimension \u00b6 The dimension of a vector space is the number of vectors in any basis. \\(\\mathbb{R}^1\\) has dimension 1 \\(\\mathbb{R}^2\\) has dimension 2 \\(\\mathbb{R}^3\\) has dimension 3 \\(\\mathbb{R}^n\\) has dimension n Part 7: Functions and Mappings \u00b6 7.1 Function Notation \u00b6 \\[f: A \\to B\\] Means: \"function \\(f\\) maps from set \\(A\\) to set \\(B\\) \" - \\(A\\) is the domain (input set) - \\(B\\) is the codomain (output set) - \\(f(x)\\) is the image of \\(x\\) Example: $ \\(f: \\mathbb{R} \\to \\mathbb{R}, \\quad f(x) = x^2\\) $ 7.2 Linear Transformation \u00b6 A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is linear if: \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) (additivity) \\(T(c\\mathbf{u}) = cT(\\mathbf{u})\\) (homogeneity) Key fact: Every linear transformation can be represented by a matrix! Part 8: Norms and Distance \u00b6 8.1 Vector Norm \u00b6 A norm measures the \"length\" or \"magnitude\" of a vector. Euclidean Norm (L2 norm): $ \\(\\|\\mathbf{x}\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2} = \\sqrt{\\sum_{i=1}^{n} x_i^2}\\) $ Example: $ \\(\\left\\|\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\\right\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\\) $ Other norms: - L1 norm: \\(\\|\\mathbf{x}\\|_1 = |x_1| + |x_2| + \\cdots + |x_n|\\) - L\u221e norm: \\(\\|\\mathbf{x}\\|_\\infty = \\max\\{|x_1|, |x_2|, \\ldots, |x_n|\\}\\) 8.2 Distance \u00b6 The distance between two vectors: $ \\(d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|\\) $ Example: $ \\(d\\left(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix}\\right) = \\left\\|\\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix}\\right\\| = \\sqrt{9 + 16} = 5\\) $ Part 9: Summation and Product Notation \u00b6 9.1 Summation ( \\(\\Sigma\\) ) \u00b6 \\[\\sum_{i=1}^{n} a_i = a_1 + a_2 + a_3 + \\cdots + a_n\\] Examples: $ \\(\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\\) $ \\[\\sum_{i=1}^{3} i^2 = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14\\] 9.2 Product ( \\(\\Pi\\) ) \u00b6 \\[\\prod_{i=1}^{n} a_i = a_1 \\times a_2 \\times a_3 \\times \\cdots \\times a_n\\] Example: $ \\(\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\\) $ Part 10: Common Functions \u00b6 10.1 Exponential Function \u00b6 \\[f(x) = e^x \\text{ where } e \\approx 2.71828\\] Properties: - \\(e^0 = 1\\) - \\(e^{a+b} = e^a \\cdot e^b\\) - \\((e^x)' = e^x\\) 10.2 Logarithm \u00b6 \\[y = \\log_b(x) \\text{ means } b^y = x\\] Natural logarithm: \\(\\ln(x) = \\log_e(x)\\) Properties: - \\(\\ln(1) = 0\\) - \\(\\ln(ab) = \\ln(a) + \\ln(b)\\) - \\(\\ln(a^b) = b\\ln(a)\\) 10.3 Sigmoid Function \u00b6 \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\] Properties: - Range: \\((0, 1)\\) - \\(\\sigma(0) = 0.5\\) - Used in logistic regression and neural networks Summary: Key Takeaways \u00b6 Essential Notation \u00b6 Vectors: \\(\\mathbf{x} \\in \\mathbb{R}^n\\) Matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) Sets: \\(A \\subset B\\) , \\(x \\in S\\) Fundamental Operations \u00b6 Vector addition, scalar multiplication, dot product Matrix addition, multiplication, transpose Norms and distances Core Concepts \u00b6 Linear independence and span Basis and dimension Linear transformations Functions and mappings Practice Problems \u00b6 Problem 1 \u00b6 Calculate the following: $ \\(\\begin{bmatrix} 2 \\\\ 3 \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 4 \\\\ 2 \\end{bmatrix}\\) $ Problem 2 \u00b6 Find the norm: $ \\(\\left\\|\\begin{bmatrix} 5 \\\\ 12 \\end{bmatrix}\\right\\|_2\\) $ Problem 3 \u00b6 Multiply these matrices: $ \\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 2 & 0 \\\\ 1 & 3 \\end{bmatrix}\\) $ Problem 4 \u00b6 Are these vectors linearly independent? $ \\(\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\\) $ Solutions \u00b6 Solution 1: $ \\(2(1) + 3(4) + 1(2) = 2 + 12 + 2 = 16\\) $ Solution 2: $ \\(\\sqrt{5^2 + 12^2} = \\sqrt{25 + 144} = \\sqrt{169} = 13\\) $ Solution 3: $ \\(\\begin{bmatrix} 1(2) + 2(1) & 1(0) + 2(3) \\\\ 3(2) + 4(1) & 3(0) + 4(3) \\end{bmatrix} = \\begin{bmatrix} 4 & 6 \\\\ 10 & 12 \\end{bmatrix}\\) $ Solution 4: No, they are dependent because \\(\\mathbf{v}_2 = 3\\mathbf{v}_1\\) Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Next: Tutorial 2 - Vector Spaces and Linear Transformations","title":"Tutorial 1: Mathematical Foundations and Terminology"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#tutorial-1-mathematical-foundations-and-terminology","text":"Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari","title":"Tutorial 1: Mathematical Foundations and Terminology"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#learning-objectives","text":"By the end of this tutorial, you will understand: Essential mathematical notation and terminology Basic set theory concepts Fundamental number systems Vector and matrix terminology Key mathematical operations","title":"\ud83d\udcda Learning Objectives"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#part-1-mathematical-notation","text":"","title":"Part 1: Mathematical Notation"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#11-basic-symbols","text":"Symbol Meaning Example \\(\\in\\) \"is an element of\" / \"belongs to\" \\(x \\in \\mathbb{R}\\) means \"x is a real number\" \\(\\notin\\) \"is not an element of\" \\(i \\notin \\mathbb{R}\\) \\(\\subset\\) \"is a subset of\" \\(\\mathbb{N} \\subset \\mathbb{Z}\\) \\(\\cup\\) \"union\" \\(A \\cup B\\) (all elements in A or B) \\(\\cap\\) \"intersection\" \\(A \\cap B\\) (elements in both A and B) \\(\\emptyset\\) \"empty set\" A set with no elements \\(\\forall\\) \"for all\" \\(\\forall x \\in \\mathbb{R}\\) means \"for all x in real numbers\" \\(\\exists\\) \"there exists\" \\(\\exists x : x > 0\\) means \"there exists an x such that x > 0\"","title":"1.1 Basic Symbols"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#12-common-notation-conventions","text":"Scalars (single numbers): - Lower case letters: \\(a, b, c, x, y, z\\) - Greek letters: \\(\\alpha, \\beta, \\gamma, \\lambda, \\mu\\) Vectors (ordered lists of numbers): - Bold lower case: \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{v}, \\mathbf{w}\\) - Sometimes with arrows: \\(\\vec{x}, \\vec{y}\\) Matrices (rectangular arrays of numbers): - Bold upper case: \\(\\mathbf{A}, \\mathbf{B}, \\mathbf{X}, \\mathbf{Y}\\) - Sometimes just upper case: \\(A, B, X, Y\\) Sets : - Upper case letters: \\(A, B, S, V\\) - Number systems: \\(\\mathbb{N}, \\mathbb{Z}, \\mathbb{Q}, \\mathbb{R}, \\mathbb{C}\\)","title":"1.2 Common Notation Conventions"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#part-2-number-systems","text":"","title":"Part 2: Number Systems"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#21-common-number-systems","text":"Symbol Name Description Examples \\(\\mathbb{N}\\) Natural numbers Positive integers \\(1, 2, 3, 4, \\ldots\\) \\(\\mathbb{Z}\\) Integers Whole numbers \\(\\ldots, -2, -1, 0, 1, 2, \\ldots\\) \\(\\mathbb{Q}\\) Rational numbers Fractions \\(\\frac{1}{2}, \\frac{3}{4}, -\\frac{5}{3}\\) \\(\\mathbb{R}\\) Real numbers All numbers on number line \\(\\pi, \\sqrt{2}, -3.5\\) \\(\\mathbb{C}\\) Complex numbers Numbers with imaginary part \\(2 + 3i, -1 + i\\)","title":"2.1 Common Number Systems"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#22-vector-spaces","text":"\\(\\mathbb{R}^n\\) - The set of all n-dimensional real vectors \\(\\mathbb{R}^1\\) = Real numbers (1D) \\(\\mathbb{R}^2\\) = Pairs of real numbers (2D plane) \\(\\mathbb{R}^3\\) = Triples of real numbers (3D space) \\(\\mathbb{R}^n\\) = n-tuples of real numbers (n-dimensional space) Example: $ \\(\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\in \\mathbb{R}^2\\) $ This means \\(\\mathbf{x}\\) is a 2-dimensional vector with components 2 and 3.","title":"2.2 Vector Spaces"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#part-3-vectors","text":"","title":"Part 3: Vectors"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#31-what-is-a-vector","text":"A vector is an ordered list of numbers representing: - A point in space - A direction and magnitude - Features in machine learning Example in \\(\\mathbb{R}^3\\) : $ \\(\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\) $","title":"3.1 What is a Vector?"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#32-vector-terminology","text":"Dimension : The number of components in a vector Notation: - Column vector (default): \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\) - Row vector: \\(\\mathbf{x}^T = \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix}\\) Components/Entries : The individual numbers in a vector - \\(x_1\\) is the first component - \\(x_2\\) is the second component - \\(x_i\\) is the i-th component","title":"3.2 Vector Terminology"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#33-basic-vector-operations","text":"","title":"3.3 Basic Vector Operations"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#addition","text":"\\[\\mathbf{a} + \\mathbf{b} = \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} = \\begin{bmatrix} a_1 + b_1 \\\\ a_2 + b_2 \\end{bmatrix}\\] Example: $ \\(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix}\\) $","title":"Addition"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#scalar-multiplication","text":"\\[c \\cdot \\mathbf{a} = c \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix} = \\begin{bmatrix} c \\cdot a_1 \\\\ c \\cdot a_2 \\end{bmatrix}\\] Example: $ \\(3 \\cdot \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\\) $","title":"Scalar Multiplication"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#dot-product-inner-product","text":"\\[\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n = \\sum_{i=1}^{n} a_i b_i\\] Example: $ \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\cdot \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} = 1(4) + 2(5) + 3(6) = 4 + 10 + 18 = 32\\) $","title":"Dot Product (Inner Product)"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#part-4-matrices","text":"","title":"Part 4: Matrices"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#41-what-is-a-matrix","text":"A matrix is a rectangular array of numbers arranged in rows and columns. General form: $ \\(\\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{bmatrix}\\) $","title":"4.1 What is a Matrix?"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#42-matrix-terminology","text":"Dimension/Size : \\(m \\times n\\) where - \\(m\\) = number of rows - \\(n\\) = number of columns Example: $ \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}_{2 \\times 3}\\) $ This is a \\(2 \\times 3\\) matrix (2 rows, 3 columns). Entry/Element : \\(a_{ij}\\) is the element in row \\(i\\) , column \\(j\\) Square Matrix : A matrix where \\(m = n\\) (same number of rows and columns) Example: $ \\(\\mathbf{B} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}_{2 \\times 2}\\) $","title":"4.2 Matrix Terminology"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#43-special-matrices","text":"","title":"4.3 Special Matrices"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#identity-matrix-mathbfi","text":"A square matrix with 1s on the diagonal and 0s elsewhere. \\[\\mathbf{I}_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\] Property: \\(\\mathbf{A} \\mathbf{I} = \\mathbf{I} \\mathbf{A} = \\mathbf{A}\\)","title":"Identity Matrix (\\(\\mathbf{I}\\))"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#zero-matrix","text":"A matrix where all elements are zero. \\[\\mathbf{0} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}\\]","title":"Zero Matrix"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#diagonal-matrix","text":"A square matrix where all off-diagonal elements are zero. \\[\\mathbf{D} = \\begin{bmatrix} d_1 & 0 & 0 \\\\ 0 & d_2 & 0 \\\\ 0 & 0 & d_3 \\end{bmatrix}\\]","title":"Diagonal Matrix"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#transpose-mathbfat","text":"Flip rows and columns. Example: $ \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}, \\quad \\mathbf{A}^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\) $ Rule: \\((a_{ij})^T = a_{ji}\\)","title":"Transpose (\\(\\mathbf{A}^T\\))"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#part-5-matrix-operations","text":"","title":"Part 5: Matrix Operations"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#51-matrix-addition","text":"Add corresponding elements (matrices must have same dimensions). \\[\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} a_{11} + b_{11} & a_{12} + b_{12} \\\\ a_{21} + b_{21} & a_{22} + b_{22} \\end{bmatrix}\\] Example: $ \\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}\\) $","title":"5.1 Matrix Addition"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#52-scalar-multiplication","text":"Multiply every element by a scalar. \\[c \\mathbf{A} = \\begin{bmatrix} c \\cdot a_{11} & c \\cdot a_{12} \\\\ c \\cdot a_{21} & c \\cdot a_{22} \\end{bmatrix}\\] Example: $ \\(2 \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 2 & 4 \\\\ 6 & 8 \\end{bmatrix}\\) $","title":"5.2 Scalar Multiplication"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#53-matrix-multiplication","text":"Rule: To multiply \\(\\mathbf{A}_{m \\times n}\\) and \\(\\mathbf{B}_{n \\times p}\\) : - Number of columns in \\(\\mathbf{A}\\) must equal number of rows in \\(\\mathbf{B}\\) - Result is \\(\\mathbf{C}_{m \\times p}\\) Formula: $ \\(c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\\) $ Example: $ \\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1(5) + 2(7) & 1(6) + 2(8) \\\\ 3(5) + 4(7) & 3(6) + 4(8) \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\\) $ Important: Matrix multiplication is NOT commutative - \\(\\mathbf{AB} \\neq \\mathbf{BA}\\) (in general)","title":"5.3 Matrix Multiplication"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#part-6-important-concepts","text":"","title":"Part 6: Important Concepts"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#61-linear-combination","text":"A sum of scalar multiples of vectors. \\[\\mathbf{v} = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n\\] Example: $ \\(\\mathbf{v} = 2\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 3\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\) $","title":"6.1 Linear Combination"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#62-linear-independence","text":"Vectors are linearly independent if no vector can be written as a linear combination of the others. Example (Independent): $ \\(\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) $ Example (Dependent): $ \\(\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\\) $ (Here \\(\\mathbf{v}_2 = 2\\mathbf{v}_1\\) )","title":"6.2 Linear Independence"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#63-span","text":"The span of a set of vectors is all possible linear combinations of those vectors. \\[\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\} = \\{c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n : c_i \\in \\mathbb{R}\\}\\]","title":"6.3 Span"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#64-basis","text":"A basis for a vector space is a set of linearly independent vectors that span the space. Standard basis for \\(\\mathbb{R}^3\\) : $ \\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\) $","title":"6.4 Basis"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#65-dimension","text":"The dimension of a vector space is the number of vectors in any basis. \\(\\mathbb{R}^1\\) has dimension 1 \\(\\mathbb{R}^2\\) has dimension 2 \\(\\mathbb{R}^3\\) has dimension 3 \\(\\mathbb{R}^n\\) has dimension n","title":"6.5 Dimension"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#part-7-functions-and-mappings","text":"","title":"Part 7: Functions and Mappings"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#71-function-notation","text":"\\[f: A \\to B\\] Means: \"function \\(f\\) maps from set \\(A\\) to set \\(B\\) \" - \\(A\\) is the domain (input set) - \\(B\\) is the codomain (output set) - \\(f(x)\\) is the image of \\(x\\) Example: $ \\(f: \\mathbb{R} \\to \\mathbb{R}, \\quad f(x) = x^2\\) $","title":"7.1 Function Notation"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#72-linear-transformation","text":"A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is linear if: \\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) (additivity) \\(T(c\\mathbf{u}) = cT(\\mathbf{u})\\) (homogeneity) Key fact: Every linear transformation can be represented by a matrix!","title":"7.2 Linear Transformation"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#part-8-norms-and-distance","text":"","title":"Part 8: Norms and Distance"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#81-vector-norm","text":"A norm measures the \"length\" or \"magnitude\" of a vector. Euclidean Norm (L2 norm): $ \\(\\|\\mathbf{x}\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2} = \\sqrt{\\sum_{i=1}^{n} x_i^2}\\) $ Example: $ \\(\\left\\|\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}\\right\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\\) $ Other norms: - L1 norm: \\(\\|\\mathbf{x}\\|_1 = |x_1| + |x_2| + \\cdots + |x_n|\\) - L\u221e norm: \\(\\|\\mathbf{x}\\|_\\infty = \\max\\{|x_1|, |x_2|, \\ldots, |x_n|\\}\\)","title":"8.1 Vector Norm"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#82-distance","text":"The distance between two vectors: $ \\(d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|\\) $ Example: $ \\(d\\left(\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 4 \\\\ 6 \\end{bmatrix}\\right) = \\left\\|\\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix}\\right\\| = \\sqrt{9 + 16} = 5\\) $","title":"8.2 Distance"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#part-9-summation-and-product-notation","text":"","title":"Part 9: Summation and Product Notation"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#91-summation-sigma","text":"\\[\\sum_{i=1}^{n} a_i = a_1 + a_2 + a_3 + \\cdots + a_n\\] Examples: $ \\(\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\\) $ \\[\\sum_{i=1}^{3} i^2 = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14\\]","title":"9.1 Summation (\\(\\Sigma\\))"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#92-product-pi","text":"\\[\\prod_{i=1}^{n} a_i = a_1 \\times a_2 \\times a_3 \\times \\cdots \\times a_n\\] Example: $ \\(\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\\) $","title":"9.2 Product (\\(\\Pi\\))"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#part-10-common-functions","text":"","title":"Part 10: Common Functions"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#101-exponential-function","text":"\\[f(x) = e^x \\text{ where } e \\approx 2.71828\\] Properties: - \\(e^0 = 1\\) - \\(e^{a+b} = e^a \\cdot e^b\\) - \\((e^x)' = e^x\\)","title":"10.1 Exponential Function"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#102-logarithm","text":"\\[y = \\log_b(x) \\text{ means } b^y = x\\] Natural logarithm: \\(\\ln(x) = \\log_e(x)\\) Properties: - \\(\\ln(1) = 0\\) - \\(\\ln(ab) = \\ln(a) + \\ln(b)\\) - \\(\\ln(a^b) = b\\ln(a)\\)","title":"10.2 Logarithm"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#103-sigmoid-function","text":"\\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\] Properties: - Range: \\((0, 1)\\) - \\(\\sigma(0) = 0.5\\) - Used in logistic regression and neural networks","title":"10.3 Sigmoid Function"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#summary-key-takeaways","text":"","title":"Summary: Key Takeaways"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#essential-notation","text":"Vectors: \\(\\mathbf{x} \\in \\mathbb{R}^n\\) Matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) Sets: \\(A \\subset B\\) , \\(x \\in S\\)","title":"Essential Notation"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#fundamental-operations","text":"Vector addition, scalar multiplication, dot product Matrix addition, multiplication, transpose Norms and distances","title":"Fundamental Operations"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#core-concepts","text":"Linear independence and span Basis and dimension Linear transformations Functions and mappings","title":"Core Concepts"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#practice-problems","text":"","title":"Practice Problems"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#problem-1","text":"Calculate the following: $ \\(\\begin{bmatrix} 2 \\\\ 3 \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 4 \\\\ 2 \\end{bmatrix}\\) $","title":"Problem 1"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#problem-2","text":"Find the norm: $ \\(\\left\\|\\begin{bmatrix} 5 \\\\ 12 \\end{bmatrix}\\right\\|_2\\) $","title":"Problem 2"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#problem-3","text":"Multiply these matrices: $ \\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 2 & 0 \\\\ 1 & 3 \\end{bmatrix}\\) $","title":"Problem 3"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#problem-4","text":"Are these vectors linearly independent? $ \\(\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\\) $","title":"Problem 4"},{"location":"tutorials/Tutorial_01_Mathematical_Foundations/#solutions","text":"Solution 1: $ \\(2(1) + 3(4) + 1(2) = 2 + 12 + 2 = 16\\) $ Solution 2: $ \\(\\sqrt{5^2 + 12^2} = \\sqrt{25 + 144} = \\sqrt{169} = 13\\) $ Solution 3: $ \\(\\begin{bmatrix} 1(2) + 2(1) & 1(0) + 2(3) \\\\ 3(2) + 4(1) & 3(0) + 4(3) \\end{bmatrix} = \\begin{bmatrix} 4 & 6 \\\\ 10 & 12 \\end{bmatrix}\\) $ Solution 4: No, they are dependent because \\(\\mathbf{v}_2 = 3\\mathbf{v}_1\\) Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Next: Tutorial 2 - Vector Spaces and Linear Transformations","title":"Solutions"},{"location":"tutorials/Tutorial_02_Linear_Algebra/","text":"Tutorial 2: Linear Algebra \u00b6 Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari \ud83d\udcda Learning Objectives \u00b6 By the end of this tutorial, you will understand: Systems of linear equations and the three types of solutions Matrices, their types, and core operations Matrix inverses and how to compute them Gaussian elimination and row echelon form Vector spaces, subspaces, and the subspace test Linear independence and how to check it Span, generating sets, basis, and dimension Rank of a matrix and its role in solvability Linear mappings, kernel, and image The Rank-Nullity Theorem Change of basis Affine spaces and their connection to solution sets Part 1: Systems of Linear Equations \u00b6 1.1 What is a System of Linear Equations? \u00b6 A system of linear equations is a collection of equations where each equation is linear (no powers, no products of variables). We want to find the values of the unknowns that satisfy all the equations at the same time. General form (two equations, two unknowns): \\[a_{11}x_1 + a_{12}x_2 = b_1\\] \\[a_{21}x_1 + a_{22}x_2 = b_2\\] In plain English: we have some unknowns ( \\(x_1, x_2\\) ), each multiplied by known constants ( \\(a_{ij}\\) ), and we want the results to equal known values ( \\(b_1, b_2\\) ). Think of it as... each equation describes a line (in 2D) or a plane (in 3D). Solving the system means finding where all the lines or planes intersect. 1.2 The Three Types of Solutions \u00b6 Every system of linear equations has exactly one of three outcomes: Type Description Geometric Picture (2D) Example Unique solution Exactly one solution Two lines cross at a single point \\(x + y = 3\\) , \\(x - y = 1\\) gives \\(x=2, y=1\\) Infinitely many solutions A family of solutions Two lines lie on top of each other \\(x + y = 2\\) , \\(2x + 2y = 4\\) (same line) No solution The system is inconsistent Two lines are parallel and never meet \\(x + y = 1\\) , \\(x + y = 3\\) (parallel lines) Worked Example: Determine the type of solution. System 1: \\(x + y = 5\\) and \\(x - y = 1\\) Add the equations: \\(2x = 6\\) , so \\(x = 3\\) , \\(y = 2\\) . Unique solution. System 2: \\(x + y = 2\\) and \\(2x + 2y = 4\\) The second equation is just \\(2 \\times\\) the first. Every point on the line \\(x + y = 2\\) is a solution. Infinitely many solutions. System 3: \\(x + y = 1\\) and \\(x + y = 3\\) These say the same expression \\(x+y\\) equals two different things. Impossible. No solution. 1.3 Matrix Notation: \\(A\\mathbf{x} = \\mathbf{b}\\) \u00b6 We can write any system of linear equations compactly using matrices: \\[\\underbrace{\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{bmatrix}}_{A} \\underbrace{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}}_{\\mathbf{x}} = \\underbrace{\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}}_{\\mathbf{b}}\\] \\(A\\) is the coefficient matrix ( \\(m \\times n\\) ) \\(\\mathbf{x}\\) is the unknown vector ( \\(n \\times 1\\) ) \\(\\mathbf{b}\\) is the right-hand side vector ( \\(m \\times 1\\) ) 1.4 The Augmented Matrix \u00b6 The augmented matrix \\([A \\mid \\mathbf{b}]\\) combines the coefficient matrix and the right-hand side into one object, which is convenient for Gaussian elimination: \\[[A \\mid \\mathbf{b}] = \\left[\\begin{array}{ccc|c} a_{11} & a_{12} & \\cdots & b_1 \\\\ a_{21} & a_{22} & \\cdots & b_2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & b_m \\end{array}\\right]\\] Example: The system \\(2x + 3y = 7\\) and \\(x - y = 1\\) becomes: \\[\\left[\\begin{array}{cc|c} 2 & 3 & 7 \\\\ 1 & -1 & 1 \\end{array}\\right]\\] Think of it as... packing all the important numbers from your system into one neat table, with a vertical line separating the left side from the right side of the equals sign. Part 2: Matrices \u00b6 2.1 Definition \u00b6 A matrix is a rectangular array of numbers with \\(m\\) rows and \\(n\\) columns. We say it has dimensions \\(m \\times n\\) (read \"m by n\"). \\[A \\in \\mathbb{R}^{m \\times n} \\quad \\text{means } A \\text{ has } m \\text{ rows and } n \\text{ columns of real numbers.}\\] The entry in row \\(i\\) and column \\(j\\) is written \\(a_{ij}\\) . 2.2 Special Matrices \u00b6 Matrix Definition Example Identity \\(I_n\\) Square matrix with 1s on the diagonal, 0s everywhere else \\(I_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\) Zero \\(\\mathbf{0}\\) All entries are zero \\(\\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}\\) Diagonal Only the diagonal entries can be nonzero \\(\\begin{bmatrix} 3 & 0 \\\\ 0 & 7 \\end{bmatrix}\\) Symmetric \\(A = A^T\\) (equal to its own transpose) \\(\\begin{bmatrix} 1 & 4 \\\\ 4 & 5 \\end{bmatrix}\\) Square Number of rows equals number of columns ( \\(m = n\\) ) Any \\(n \\times n\\) matrix Key property of the identity matrix: For any matrix \\(A\\) of compatible size: \\[AI = IA = A\\] Think of it as... the identity matrix is like multiplying by 1. It does nothing to whatever it touches. 2.3 Matrix Operations \u00b6 Matrix Addition \u00b6 Add corresponding entries. Both matrices must have the same dimensions. \\[\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}\\] Scalar Multiplication \u00b6 Multiply every entry by a number. \\[3 \\begin{bmatrix} 1 & 2 \\\\ 4 & 5 \\end{bmatrix} = \\begin{bmatrix} 3 & 6 \\\\ 12 & 15 \\end{bmatrix}\\] Matrix Multiplication \u00b6 To compute \\(C = AB\\) where \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\) , each entry of \\(C\\) is: \\[c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj} = (\\text{row } i \\text{ of } A) \\cdot (\\text{column } j \\text{ of } B)\\] The result \\(C\\) has dimensions \\(m \\times p\\) . Requirement: The number of columns of \\(A\\) must equal the number of rows of \\(B\\) . Worked Example: \\[\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1(5)+2(7) & 1(6)+2(8) \\\\ 3(5)+4(7) & 3(6)+4(8) \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\\] Critical fact: Matrix multiplication is NOT commutative in general: \\[AB \\neq BA\\] Think of it as... putting on socks then shoes is not the same as putting on shoes then socks. The order matters. 2.4 Transpose \u00b6 The transpose \\(A^T\\) flips a matrix over its diagonal: rows become columns and columns become rows. If \\(A\\) is \\(m \\times n\\) , then \\(A^T\\) is \\(n \\times m\\) , and \\((A^T)_{ij} = A_{ji}\\) . Example: \\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}, \\quad A^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\] Transpose properties: Property Formula Double transpose \\((A^T)^T = A\\) Sum \\((A + B)^T = A^T + B^T\\) Scalar \\((cA)^T = cA^T\\) Product \\((AB)^T = B^T A^T\\) (note the reversed order!) Part 3: Matrix Inverse \u00b6 3.1 Definition \u00b6 The inverse of a square matrix \\(A\\) is a matrix \\(A^{-1}\\) such that: \\[AA^{-1} = A^{-1}A = I\\] In plain English: \\(A^{-1}\\) \"undoes\" whatever \\(A\\) does. If \\(A\\) transforms a vector, \\(A^{-1}\\) transforms it back. A matrix that has an inverse is called invertible (or non-singular or regular ). A matrix that does not have an inverse is called singular . Think of it as... the inverse is like an \"undo button.\" If \\(A\\) scrambles your data, \\(A^{-1}\\) unscrambles it perfectly. 3.2 When Does the Inverse Exist? \u00b6 A square matrix \\(A\\) is invertible if and only if: \\(\\det(A) \\neq 0\\) The columns of \\(A\\) are linearly independent The only solution to \\(A\\mathbf{x} = \\mathbf{0}\\) is \\(\\mathbf{x} = \\mathbf{0}\\) \\(\\text{rank}(A) = n\\) (full rank) These are all equivalent conditions -- if one is true, they are all true. 3.3 Computing the Inverse of a 2x2 Matrix \u00b6 For \\(A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) , the inverse is: \\[A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\] The number \\(ad - bc\\) is the determinant \\(\\det(A)\\) . If \\(\\det(A) = 0\\) , the inverse does not exist. Worked Example: \\[A = \\begin{bmatrix} 4 & 7 \\\\ 2 & 6 \\end{bmatrix}\\] \\[\\det(A) = 4(6) - 7(2) = 24 - 14 = 10\\] \\[A^{-1} = \\frac{1}{10}\\begin{bmatrix} 6 & -7 \\\\ -2 & 4 \\end{bmatrix} = \\begin{bmatrix} 0.6 & -0.7 \\\\ -0.2 & 0.4 \\end{bmatrix}\\] Verify: \\(AA^{-1} = \\begin{bmatrix} 4 & 7 \\\\ 2 & 6 \\end{bmatrix}\\begin{bmatrix} 0.6 & -0.7 \\\\ -0.2 & 0.4 \\end{bmatrix} = \\begin{bmatrix} 2.4-1.4 & -2.8+2.8 \\\\ 1.2-1.2 & -1.4+2.4 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I \\quad \\checkmark\\) 3.4 Properties of the Inverse \u00b6 Property Formula Inverse of inverse \\((A^{-1})^{-1} = A\\) Inverse of product \\((AB)^{-1} = B^{-1}A^{-1}\\) (note the reversed order!) Inverse of transpose \\((A^T)^{-1} = (A^{-1})^T\\) Inverse of scalar multiple \\((cA)^{-1} = \\frac{1}{c}A^{-1}\\) for \\(c \\neq 0\\) The property \\((AB)^{-1} = B^{-1}A^{-1}\\) is very important. Notice the order reverses -- just like with the transpose of a product. Think of it as... if you put on socks then shoes, to undo it you take off shoes first, then socks. The same \"reverse order\" logic applies to matrix inverses. Part 4: Solving Systems -- Gaussian Elimination \u00b6 4.1 The Idea \u00b6 Gaussian elimination is a systematic method for solving \\(A\\mathbf{x} = \\mathbf{b}\\) . The idea is simple: use allowed row operations to transform the augmented matrix \\([A \\mid \\mathbf{b}]\\) into a simpler form from which the solution is easy to read off. 4.2 Elementary Row Operations \u00b6 There are three operations you are allowed to perform on rows. None of them change the solution set of the system. Operation Description Example Swap Exchange two rows: \\(R_i \\leftrightarrow R_j\\) Swap row 1 and row 2 Scale Multiply a row by a nonzero constant: \\(R_i \\to cR_i\\) Multiply row 2 by \\(\\frac{1}{3}\\) Add Add a multiple of one row to another: \\(R_i \\to R_i + cR_j\\) Add \\((-2) \\times\\) row 1 to row 2 Think of it as... you are doing the same algebra you would do by hand (adding equations, multiplying both sides by a constant), but in a more organized table format. 4.3 Row Echelon Form (REF) \u00b6 A matrix is in row echelon form if: All rows that are entirely zero are at the bottom The first nonzero entry in each row (called the pivot ) is to the right of the pivot in the row above All entries below each pivot are zero Example of REF: \\[\\begin{bmatrix} 2 & 1 & -1 \\\\ 0 & 3 & 5 \\\\ 0 & 0 & 4 \\end{bmatrix}\\] The pivots are 2, 3, and 4. Each is to the right and below the previous one, forming a \"staircase\" pattern. 4.4 Reduced Row Echelon Form (RREF) \u00b6 A matrix is in reduced row echelon form if it is in REF and additionally: Every pivot is 1 Every pivot is the only nonzero entry in its column Example of RREF: \\[\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\] When the augmented matrix \\([A \\mid \\mathbf{b}]\\) is in RREF, you can read the solution directly. 4.5 Full Worked Example: Solving a System via Gaussian Elimination \u00b6 Solve the system: \\[x_1 + 2x_2 - x_3 = 3\\] \\[2x_1 + 5x_2 - 2x_3 = 7\\] \\[-x_1 - x_2 + 3x_3 = 2\\] Step 1: Write the augmented matrix. \\[\\left[\\begin{array}{ccc|c} 1 & 2 & -1 & 3 \\\\ 2 & 5 & -2 & 7 \\\\ -1 & -1 & 3 & 2 \\end{array}\\right]\\] Step 2: Eliminate below the first pivot (the 1 in position \\((1,1)\\) ). \\(R_2 \\to R_2 - 2R_1\\) : Replace row 2 with (row 2 minus 2 times row 1). \\(R_3 \\to R_3 + R_1\\) : Replace row 3 with (row 3 plus row 1). \\[\\left[\\begin{array}{ccc|c} 1 & 2 & -1 & 3 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 1 & 2 & 5 \\end{array}\\right]\\] Step 3: Eliminate below the second pivot (the 1 in position \\((2,2)\\) ). \\(R_3 \\to R_3 - R_2\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 2 & -1 & 3 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 2 & 4 \\end{array}\\right]\\] This is now in row echelon form . We have three pivots for three unknowns, so there is a unique solution. Step 4: Scale row 3 to make the pivot equal to 1. \\(R_3 \\to \\frac{1}{2}R_3\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 2 & -1 & 3 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 2 \\end{array}\\right]\\] Step 5: Back substitution (or continue to RREF). Let us continue to RREF. \\(R_1 \\to R_1 + R_3\\) (eliminate the \\(-1\\) in position \\((1,3)\\) ): \\[\\left[\\begin{array}{ccc|c} 1 & 2 & 0 & 5 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 2 \\end{array}\\right]\\] \\(R_1 \\to R_1 - 2R_2\\) (eliminate the \\(2\\) in position \\((1,2)\\) ): \\[\\left[\\begin{array}{ccc|c} 1 & 0 & 0 & 3 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 2 \\end{array}\\right]\\] Step 6: Read the solution from RREF. \\[x_1 = 3, \\quad x_2 = 1, \\quad x_3 = 2\\] Verify: Plug back into the original equations: \\(1(3) + 2(1) - 1(2) = 3 + 2 - 2 = 3\\) \\(\\checkmark\\) \\(2(3) + 5(1) - 2(2) = 6 + 5 - 4 = 7\\) \\(\\checkmark\\) \\(-1(3) - 1(1) + 3(2) = -3 - 1 + 6 = 2\\) \\(\\checkmark\\) Part 5: Vector Spaces \u00b6 5.1 What is a Vector Space? \u00b6 A vector space is a set \\(V\\) of objects (called \"vectors\") together with two operations -- vector addition and scalar multiplication -- that satisfy certain rules (axioms). In plain English: a vector space is a collection of things that you can add together and scale, and the result always stays inside the same collection. Think of it as... a playground with fences. You can run around (add vectors, scale them) however you like, but you can never leave the playground. Everything you create stays inside. 5.2 The Vector Space Axioms \u00b6 For all \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V\\) and all scalars \\(\\alpha, \\beta \\in \\mathbb{R}\\) : Axiom Statement In Plain English Closure under addition \\(\\mathbf{u} + \\mathbf{v} \\in V\\) Adding two vectors keeps you in \\(V\\) Closure under scalar multiplication \\(\\alpha \\mathbf{u} \\in V\\) Scaling a vector keeps you in \\(V\\) Associativity of addition \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\) Grouping does not matter for addition Commutativity of addition \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\) Order does not matter for addition Zero vector exists \\(\\exists \\mathbf{0} \\in V : \\mathbf{u} + \\mathbf{0} = \\mathbf{u}\\) There is a \"do-nothing\" element Additive inverse \\(\\forall \\mathbf{u} \\in V, \\exists (-\\mathbf{u}) : \\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\) Every vector has a \"negative\" Distributivity (vector) \\(\\alpha(\\mathbf{u} + \\mathbf{v}) = \\alpha\\mathbf{u} + \\alpha\\mathbf{v}\\) Scaling distributes over vector addition Distributivity (scalar) \\((\\alpha + \\beta)\\mathbf{u} = \\alpha\\mathbf{u} + \\beta\\mathbf{u}\\) Scaling distributes over scalar addition Associativity of scaling \\(\\alpha(\\beta \\mathbf{u}) = (\\alpha\\beta)\\mathbf{u}\\) Order of scaling does not matter Identity of scaling \\(1 \\cdot \\mathbf{u} = \\mathbf{u}\\) Scaling by 1 does nothing 5.3 Common Examples of Vector Spaces \u00b6 \\(\\mathbb{R}^n\\) : the set of all \\(n\\) -tuples of real numbers (the most common example) \\(\\mathbb{R}^{m \\times n}\\) : the set of all \\(m \\times n\\) real matrices The set of all polynomials of degree at most \\(n\\) The set of all continuous functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\) 5.4 Subspaces \u00b6 A subspace \\(U\\) of a vector space \\(V\\) is a subset \\(U \\subseteq V\\) that is itself a vector space under the same operations. The 3-Step Subspace Test \u00b6 To check whether a subset \\(U \\subseteq V\\) is a subspace, verify three things: Step Check Why 1 \\(\\mathbf{0} \\in U\\) (the zero vector is in \\(U\\) ) Every vector space must contain the zero vector 2 \\(\\mathbf{u} + \\mathbf{v} \\in U\\) for all \\(\\mathbf{u}, \\mathbf{v} \\in U\\) (closed under addition) Adding members should not kick you out 3 \\(\\alpha \\mathbf{u} \\in U\\) for all \\(\\alpha \\in \\mathbb{R}\\) , \\(\\mathbf{u} \\in U\\) (closed under scalar multiplication) Scaling should not kick you out If all three pass, \\(U\\) is a subspace. If any one fails, it is not. Think of it as... a \"mini playground\" inside the bigger playground. As long as the mini playground contains the origin and any combination of its members stays inside, it qualifies as a subspace. Example (is a subspace): Let \\(U = \\left\\{\\begin{bmatrix} x \\\\ 0 \\end{bmatrix} : x \\in \\mathbb{R}\\right\\} \\subseteq \\mathbb{R}^2\\) (the \\(x\\) -axis). Zero vector: \\(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\in U\\) \\(\\checkmark\\) Closed under addition: \\(\\begin{bmatrix} a \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} b \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} a+b \\\\ 0 \\end{bmatrix} \\in U\\) \\(\\checkmark\\) Closed under scalar multiplication: \\(c\\begin{bmatrix} a \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} ca \\\\ 0 \\end{bmatrix} \\in U\\) \\(\\checkmark\\) Example (not a subspace): Let \\(W = \\left\\{\\begin{bmatrix} x \\\\ 1 \\end{bmatrix} : x \\in \\mathbb{R}\\right\\} \\subseteq \\mathbb{R}^2\\) (a horizontal line at height 1). Zero vector: \\(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\notin W\\) because the second component is always 1, not 0. \\(\\times\\) \\(W\\) fails the very first check, so it is not a subspace. Part 6: Linear Independence \u00b6 6.1 Definition \u00b6 A set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}\\) is linearly independent if the only way to combine them to get the zero vector is with all coefficients equal to zero: \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0} \\implies c_1 = c_2 = \\cdots = c_k = 0\\] If there is a nontrivial combination (some \\(c_i \\neq 0\\) ) that gives \\(\\mathbf{0}\\) , the vectors are linearly dependent . In plain English: vectors are independent when none of them is \"redundant\" -- you cannot build any one of them from the others. Think of it as... independent vectors each bring something genuinely new to the table. Dependent vectors are freeloaders -- at least one of them could be replaced by a combination of the rest. 6.2 How to Check Linear Independence \u00b6 Method: Form a matrix with the vectors as columns and row-reduce to REF. Count the pivots. If every column has a pivot, the vectors are linearly independent . If any column lacks a pivot, the vectors are linearly dependent . Worked Example: Are the following vectors linearly independent? \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}\\] Form the matrix \\([\\mathbf{v}_1 \\mid \\mathbf{v}_2 \\mid \\mathbf{v}_3]\\) and row reduce: \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 2 & 1 & 3 \\\\ 3 & 1 & 4 \\end{bmatrix} \\xrightarrow{R_2 - 2R_1} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 3 & 1 & 4 \\end{bmatrix} \\xrightarrow{R_3 - 3R_1} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\xrightarrow{R_3 - R_2} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}\\] Only 2 pivots for 3 vectors. Column 3 has no pivot, so \\(\\mathbf{v}_3\\) is a linear combination of the others. The vectors are linearly dependent . Indeed, \\(\\mathbf{v}_3 = \\mathbf{v}_1 + \\mathbf{v}_2 = \\begin{bmatrix} 1+0 \\\\ 2+1 \\\\ 3+1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}\\) . 6.3 Geometric Meaning \u00b6 Number of vectors Independent means... Dependent means... 2 vectors in \\(\\mathbb{R}^2\\) They point in different directions They lie on the same line 3 vectors in \\(\\mathbb{R}^3\\) They span all of 3D space They all lie in the same plane (or line) \\(k\\) vectors in \\(\\mathbb{R}^n\\) They span a \\(k\\) -dimensional subspace They span something less than \\(k\\) -dimensional 6.4 Maximum Number of Independent Vectors \u00b6 In \\(\\mathbb{R}^n\\) , you can have at most \\(n\\) linearly independent vectors. If you have more than \\(n\\) vectors in \\(\\mathbb{R}^n\\) , they are guaranteed to be linearly dependent. Part 7: Generating Sets, Span, and Basis \u00b6 7.1 Span \u00b6 The span of a set of vectors is the set of all possible linear combinations of those vectors: \\[\\text{span}\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\} = \\left\\{c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k : c_1, \\ldots, c_k \\in \\mathbb{R}\\right\\}\\] In plain English: the span is everything you can \"reach\" by adding and scaling the given vectors. 7.2 Generating Set \u00b6 A set of vectors \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\) is a generating set (or spanning set) for a vector space \\(V\\) if: \\[V = \\text{span}\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\] This means every vector in \\(V\\) can be written as a linear combination of \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) . 7.3 Basis \u00b6 A basis for a vector space \\(V\\) is a set of vectors that is: Linearly independent (no redundancy) Spanning (reaches everything in \\(V\\) ) A basis is the most \"efficient\" generating set -- it spans the whole space with no wasted vectors. Standard basis for \\(\\mathbb{R}^3\\) : \\[\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\] These three vectors are linearly independent and every vector in \\(\\mathbb{R}^3\\) can be written as \\(\\mathbf{v} = v_1\\mathbf{e}_1 + v_2\\mathbf{e}_2 + v_3\\mathbf{e}_3\\) . Think of it as... a basis is like a set of building blocks. You need enough blocks to build anything in the space, but you do not want any block that is just a copy of others glued together. The standard basis in \\(\\mathbb{R}^3\\) uses the three coordinate axes as blocks. 7.4 Dimension \u00b6 The dimension of a vector space \\(V\\) is the number of vectors in any basis of \\(V\\) . This number is always the same no matter which basis you choose. \\[\\dim(\\mathbb{R}^n) = n\\] Examples: A line through the origin in \\(\\mathbb{R}^3\\) has dimension 1 A plane through the origin in \\(\\mathbb{R}^3\\) has dimension 2 \\(\\mathbb{R}^3\\) itself has dimension 3 7.5 Finding a Basis for a Set of Vectors \u00b6 Given vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) , to find a basis for \\(\\text{span}\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\) : Form the matrix \\(A = [\\mathbf{v}_1 \\mid \\cdots \\mid \\mathbf{v}_k]\\) Row reduce to REF The original vectors corresponding to pivot columns form a basis Part 8: Rank \u00b6 8.1 Definition \u00b6 The rank of a matrix \\(A\\) is the number of linearly independent columns (equivalently, the number of linearly independent rows). A fundamental fact: column rank = row rank . This means you get the same number whether you count independent columns or independent rows. 8.2 Computing Rank via RREF \u00b6 To find the rank of \\(A\\) : Row reduce \\(A\\) to REF or RREF Count the number of pivots (the nonzero leading entries) \\[\\text{rank}(A) = \\text{number of pivots}\\] Worked Example: \\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 0 & 1 & 1 \\end{bmatrix}\\] Row reduce: \\[\\xrightarrow{R_2 - 2R_1} \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\\\ 0 & 1 & 1 \\end{bmatrix} \\xrightarrow{R_2 \\leftrightarrow R_3} \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}\\] There are 2 pivots (in columns 1 and 2), so \\(\\text{rank}(A) = 2\\) . 8.3 Rank and Solvability of \\(A\\mathbf{x} = \\mathbf{b}\\) \u00b6 The rank tells you about the solvability of \\(A\\mathbf{x} = \\mathbf{b}\\) : Condition What it means \\(\\text{rank}(A) = \\text{rank}([A \\mid \\mathbf{b}])\\) The system \\(A\\mathbf{x} = \\mathbf{b}\\) has at least one solution \\(\\text{rank}(A) < \\text{rank}([A \\mid \\mathbf{b}])\\) The system \\(A\\mathbf{x} = \\mathbf{b}\\) has no solution \\(\\text{rank}(A) = n\\) (number of unknowns) If a solution exists, it is unique \\(\\text{rank}(A) < n\\) If a solution exists, there are infinitely many Think of it as... rank tells you \"how many genuinely useful equations\" you have. If the rank equals the number of unknowns, you have enough information to pin down a unique answer. If the rank is less, you have leftover freedom, meaning infinitely many solutions. Part 9: Linear Mappings \u00b6 9.1 Definition \u00b6 A function \\(\\Phi: V \\to W\\) between vector spaces is a linear mapping (or linear transformation) if it preserves both addition and scalar multiplication: \\(\\Phi(\\mathbf{u} + \\mathbf{v}) = \\Phi(\\mathbf{u}) + \\Phi(\\mathbf{v})\\) for all \\(\\mathbf{u}, \\mathbf{v} \\in V\\) (preserves addition) \\(\\Phi(\\alpha \\mathbf{u}) = \\alpha \\Phi(\\mathbf{u})\\) for all \\(\\alpha \\in \\mathbb{R}\\) , \\(\\mathbf{u} \\in V\\) (preserves scalar multiplication) Equivalently, in one combined condition: \\[\\Phi(\\alpha \\mathbf{u} + \\beta \\mathbf{v}) = \\alpha \\Phi(\\mathbf{u}) + \\beta \\Phi(\\mathbf{v})\\] In plain English: a linear mapping respects the structure of the vector space. It does not \"break\" addition or scaling. Think of it as... a linear mapping is a \"well-behaved\" transformation. It keeps straight lines straight, keeps the origin fixed, and does not do anything \"nonlinear\" like bending or shifting. 9.2 Transformation Matrix \u00b6 Every linear mapping \\(\\Phi: \\mathbb{R}^n \\to \\mathbb{R}^m\\) can be represented by a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) such that: \\[\\Phi(\\mathbf{x}) = A\\mathbf{x}\\] To find \\(A\\) , compute what \\(\\Phi\\) does to each standard basis vector \\(\\mathbf{e}_j\\) and place the result as the \\(j\\) -th column of \\(A\\) : \\[A = [\\Phi(\\mathbf{e}_1) \\mid \\Phi(\\mathbf{e}_2) \\mid \\cdots \\mid \\Phi(\\mathbf{e}_n)]\\] 9.3 Kernel (Null Space) \u00b6 The kernel (or null space) of a linear mapping \\(\\Phi: V \\to W\\) (or equivalently of its matrix \\(A\\) ) is the set of all vectors that map to the zero vector: \\[\\ker(\\Phi) = \\{\\mathbf{x} \\in V : \\Phi(\\mathbf{x}) = \\mathbf{0}\\} = \\{\\mathbf{x} \\in \\mathbb{R}^n : A\\mathbf{x} = \\mathbf{0}\\}\\] In plain English: the kernel is everything that \\(\\Phi\\) \"kills\" (sends to zero). How to compute: Solve the homogeneous system \\(A\\mathbf{x} = \\mathbf{0}\\) by row reducing \\(A\\) and expressing the free variables. The kernel is always a subspace of \\(V\\) . 9.4 Image (Column Space) \u00b6 The image (or range, or column space) of a linear mapping \\(\\Phi\\) is the set of all possible outputs: \\[\\text{Im}(\\Phi) = \\{\\Phi(\\mathbf{x}) : \\mathbf{x} \\in V\\} = \\{A\\mathbf{x} : \\mathbf{x} \\in \\mathbb{R}^n\\}\\] This is the same as the column space of \\(A\\) -- the span of the columns of \\(A\\) : \\[\\text{Im}(A) = \\text{span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\}\\] where \\(\\mathbf{a}_j\\) are the columns of \\(A\\) . The image is always a subspace of \\(W\\) . 9.5 Injective, Surjective, Bijective \u00b6 Property Meaning Condition Injective (one-to-one) Different inputs give different outputs \\(\\ker(\\Phi) = \\{\\mathbf{0}\\}\\) Surjective (onto) Every element of \\(W\\) is an output \\(\\text{Im}(\\Phi) = W\\) Bijective Both injective and surjective The mapping has an inverse Think of it as... Injective means no two inputs \"collide\" at the same output. Surjective means every target is \"hit\" by some input. Bijective means there is a perfect one-to-one pairing between inputs and outputs. 9.6 Worked Example: Finding the Kernel and Image \u00b6 Let \\(A = \\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\) . Kernel: Solve \\(A\\mathbf{x} = \\mathbf{0}\\) : \\[\\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\] From the matrix (already in RREF): \\(x_1 + 2x_2 = 0\\) and \\(x_3 = 0\\) . Variable \\(x_2\\) is free. Set \\(x_2 = t\\) : \\[\\mathbf{x} = \\begin{bmatrix} -2t \\\\ t \\\\ 0 \\end{bmatrix} = t\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}\\] \\[\\ker(A) = \\text{span}\\left\\{\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}\\right\\}\\] Dimension of kernel: \\(\\dim(\\ker(A)) = 1\\) . Image: The image is the column space of \\(A\\) . Since \\(A\\) has 2 pivots (in columns 1 and 3), the pivot columns form a basis for the image: \\[\\text{Im}(A) = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right\\} = \\mathbb{R}^2\\] Dimension of image: \\(\\dim(\\text{Im}(A)) = 2\\) . Part 10: The Rank-Nullity Theorem \u00b6 10.1 Statement \u00b6 For a linear mapping \\(\\Phi: V \\to W\\) with transformation matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) : \\[\\dim(\\ker(\\Phi)) + \\dim(\\text{Im}(\\Phi)) = \\dim(V)\\] Or equivalently, using the matrix \\(A\\) : \\[\\text{nullity}(A) + \\text{rank}(A) = n\\] where \\(n\\) is the number of columns of \\(A\\) (the dimension of the domain). In plain English: the number of dimensions that get \"killed\" (mapped to zero) plus the number of dimensions that \"survive\" (form the output) always add up to the total number of input dimensions. Think of it as... you start with \\(n\\) dimensions of freedom. Some get collapsed to zero (the nullity), and the rest get mapped to actual outputs (the rank). Nothing is lost or created -- everything is accounted for. 10.2 Example \u00b6 From the worked example in Part 9: \\(A \\in \\mathbb{R}^{2 \\times 3}\\) , so \\(n = 3\\) \\(\\dim(\\ker(A)) = 1\\) (the nullity) \\(\\dim(\\text{Im}(A)) = 2\\) (the rank) Check: \\(1 + 2 = 3 = n\\) \\(\\checkmark\\) 10.3 Using the Theorem \u00b6 The Rank-Nullity Theorem is useful for: If you know... You can deduce... Rank and \\(n\\) Nullity = \\(n - \\text{rank}\\) Nullity and \\(n\\) Rank = \\(n - \\text{nullity}\\) Rank \\(= n\\) Nullity = 0, so the mapping is injective Rank \\(= m\\) (rows) The mapping is surjective Part 11: Change of Basis \u00b6 11.1 Coordinate Vectors \u00b6 Every vector \\(\\mathbf{x}\\) in a vector space \\(V\\) can be represented as a linear combination of basis vectors. The coordinate vector of \\(\\mathbf{x}\\) with respect to a basis \\(B = \\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\) is the vector of coefficients: If \\(\\mathbf{x} = \\alpha_1 \\mathbf{b}_1 + \\alpha_2 \\mathbf{b}_2 + \\cdots + \\alpha_n \\mathbf{b}_n\\) , then the coordinate vector is: \\[[\\mathbf{x}]_B = \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\vdots \\\\ \\alpha_n \\end{bmatrix}\\] In plain English: the same vector can look different depending on which coordinate system (basis) you use. Think of it as... describing a location. You can say \"3 blocks east and 2 blocks north\" or \"3.6 blocks northeast.\" Same point, different descriptions depending on your coordinate system. 11.2 The Change of Basis Matrix \u00b6 Suppose you have two bases \\(B = \\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\) and \\(\\tilde{B} = \\{\\tilde{\\mathbf{b}}_1, \\ldots, \\tilde{\\mathbf{b}}_n\\}\\) for the same vector space. The change of basis matrix \\(S\\) from \\(\\tilde{B}\\) to \\(B\\) satisfies: \\[[\\mathbf{x}]_B = S \\, [\\mathbf{x}]_{\\tilde{B}}\\] To construct \\(S\\) : write each new basis vector \\(\\tilde{\\mathbf{b}}_j\\) in terms of the old basis \\(B\\) . The coefficients form the \\(j\\) -th column of \\(S\\) . 11.3 Transformation Under a Change of Basis \u00b6 If a linear mapping \\(\\Phi\\) has transformation matrix \\(A\\) with respect to basis \\(B\\) , then with respect to basis \\(\\tilde{B}\\) , it has transformation matrix: \\[\\tilde{A} = S^{-1} A S\\] where \\(S\\) is the change of basis matrix from \\(\\tilde{B}\\) to \\(B\\) . Worked Example: Let \\(B = \\left\\{\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right\\}\\) (the standard basis) and \\(\\tilde{B} = \\left\\{\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\right\\}\\) . The change of basis matrix \\(S\\) from \\(\\tilde{B}\\) to \\(B\\) is formed by writing the new basis vectors in terms of the standard basis (which is trivial since \\(B\\) is the standard basis): \\[S = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}\\] Now if \\(\\mathbf{x}\\) has coordinates \\([\\mathbf{x}]_{\\tilde{B}} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}\\) in the new basis, the standard coordinates are: \\[[\\mathbf{x}]_B = S \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}\\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix}\\] This means \\(\\mathbf{x} = 3\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 2\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix}\\) . Part 12: Affine Spaces \u00b6 12.1 Affine Subspaces vs. Vector Subspaces \u00b6 A vector subspace must pass through the origin. An affine subspace is a \"shifted\" version of a vector subspace -- it may or may not pass through the origin. Formal definition: An affine subspace \\(L\\) of a vector space \\(V\\) is a set of the form: \\[L = \\mathbf{x}_0 + U = \\{\\mathbf{x}_0 + \\mathbf{u} : \\mathbf{u} \\in U\\}\\] where \\(\\mathbf{x}_0 \\in V\\) is a fixed point and \\(U \\subseteq V\\) is a vector subspace. In plain English: take a subspace \\(U\\) (which passes through the origin) and slide it so it passes through the point \\(\\mathbf{x}_0\\) instead. The result is an affine subspace. Think of it as... a vector subspace is a flat surface passing through the origin (a line through zero, a plane through zero, etc.). An affine subspace is that same flat surface slid to a different location. A line that does not pass through the origin is affine but not a vector subspace. 12.2 Examples \u00b6 Geometric object Vector subspace? Affine subspace? A line through the origin Yes Yes (with \\(\\mathbf{x}_0 = \\mathbf{0}\\) ) A line NOT through the origin No Yes A plane through the origin Yes Yes A plane NOT through the origin No Yes A single point No (unless it is the origin) Yes (a 0-dimensional affine subspace) 12.3 Connection to Solutions of \\(A\\mathbf{x} = \\mathbf{b}\\) \u00b6 This is the key insight: the solution set of \\(A\\mathbf{x} = \\mathbf{b}\\) (when \\(\\mathbf{b} \\neq \\mathbf{0}\\) ) is an affine subspace, not a vector subspace. Here is why. Suppose \\(\\mathbf{x}_p\\) is one particular solution to \\(A\\mathbf{x} = \\mathbf{b}\\) . Then every solution can be written as: \\[\\mathbf{x} = \\mathbf{x}_p + \\mathbf{v}, \\quad \\text{where } \\mathbf{v} \\in \\ker(A)\\] In other words, the full solution set is: \\[\\{\\mathbf{x}_p + \\mathbf{v} : \\mathbf{v} \\in \\ker(A)\\} = \\mathbf{x}_p + \\ker(A)\\] This is exactly an affine subspace: a particular solution \\(\\mathbf{x}_p\\) plus the kernel (null space) of \\(A\\) . Why this works: If \\(A\\mathbf{x}_p = \\mathbf{b}\\) and \\(A\\mathbf{v} = \\mathbf{0}\\) , then \\(A(\\mathbf{x}_p + \\mathbf{v}) = A\\mathbf{x}_p + A\\mathbf{v} = \\mathbf{b} + \\mathbf{0} = \\mathbf{b}\\) \\(\\checkmark\\) Worked Example: Solve \\(A\\mathbf{x} = \\mathbf{b}\\) where \\(A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}\\) and \\(\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\\) . Step 1: Find a particular solution \\(\\mathbf{x}_p\\) . The augmented matrix is \\(\\left[\\begin{array}{cc|c} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{array}\\right]\\) . \\(R_2 \\to R_2 - 2R_1\\) : \\(\\left[\\begin{array}{cc|c} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{array}\\right]\\) So \\(x_1 + 2x_2 = 3\\) . Setting \\(x_2 = 0\\) gives \\(x_1 = 3\\) . A particular solution is \\(\\mathbf{x}_p = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\\) . Step 2: Find the kernel \\(\\ker(A)\\) . Solve \\(A\\mathbf{x} = \\mathbf{0}\\) : \\(x_1 + 2x_2 = 0\\) , so \\(x_1 = -2x_2\\) . Setting \\(x_2 = t\\) : \\[\\ker(A) = \\text{span}\\left\\{\\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\right\\}\\] Step 3: Write the full solution as an affine subspace. \\[\\mathbf{x} = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} + t\\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}, \\quad t \\in \\mathbb{R}\\] This is a line in \\(\\mathbb{R}^2\\) that passes through \\(\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\\) with direction \\(\\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\) . It does not pass through the origin, so it is affine but not a vector subspace. Summary: Key Takeaways \u00b6 Systems and Matrices \u00b6 A system \\(A\\mathbf{x} = \\mathbf{b}\\) has 0, 1, or infinitely many solutions The augmented matrix \\([A \\mid \\mathbf{b}]\\) is the starting point for Gaussian elimination The inverse \\(A^{-1}\\) exists when \\(\\det(A) \\neq 0\\) ; for \\(2 \\times 2\\) : \\(A^{-1} = \\frac{1}{ad-bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\) Vector Spaces and Subspaces \u00b6 A vector space satisfies closure under addition and scalar multiplication Subspace test: contains \\(\\mathbf{0}\\) , closed under \\(+\\) , closed under scalar multiplication Basis = linearly independent + spanning; dimension = number of basis vectors Rank, Kernel, and Image \u00b6 \\(\\text{rank}(A) =\\) number of pivots in REF \\(\\ker(A) = \\{\\mathbf{x} : A\\mathbf{x} = \\mathbf{0}\\}\\) ; $\\text{Im}(A) = $ column space of \\(A\\) Rank-Nullity Theorem: \\(\\text{nullity}(A) + \\text{rank}(A) = n\\) Linear Mappings and Affine Spaces \u00b6 Linear mappings preserve addition and scalar multiplication Change of basis: \\(\\tilde{A} = S^{-1}AS\\) Solution set of \\(A\\mathbf{x} = \\mathbf{b}\\) is an affine subspace: \\(\\mathbf{x}_p + \\ker(A)\\) Practice Problems \u00b6 Problem 1: Solving a System via Gaussian Elimination \u00b6 Solve the following system using Gaussian elimination: \\[x_1 + x_2 + 2x_3 = 9\\] \\[2x_1 + 4x_2 - 3x_3 = 1\\] \\[3x_1 + 6x_2 - 5x_3 = 0\\] Problem 2: Finding the Inverse of a Matrix \u00b6 Find the inverse of: \\[A = \\begin{bmatrix} 3 & 5 \\\\ 1 & 2 \\end{bmatrix}\\] Verify your answer by checking that \\(AA^{-1} = I\\) . Problem 3: Checking if a Set is a Subspace \u00b6 Determine whether the following subset of \\(\\mathbb{R}^3\\) is a subspace: \\[U = \\left\\{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\in \\mathbb{R}^3 : x_1 + 2x_2 - x_3 = 0\\right\\}\\] Problem 4: Testing for Linear Independence \u00b6 Determine whether the following vectors are linearly independent: \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}\\] Problem 5: Finding a Basis and Dimension \u00b6 Find a basis for the column space of the following matrix and state its dimension: \\[A = \\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 6 & 10 \\\\ 1 & 1 & 3 \\end{bmatrix}\\] Problem 6: Computing the Rank \u00b6 Compute the rank of: \\[M = \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 2 & 4 & 3 & 1 \\\\ 3 & 6 & 4 & 1 \\end{bmatrix}\\] Problem 7: Kernel and Image \u00b6 For the matrix \\(A = \\begin{bmatrix} 1 & 0 & -1 \\\\ 2 & 1 & 0 \\\\ -1 & 1 & 2 \\end{bmatrix}\\) : (a) Find the kernel of \\(A\\) . (b) Find a basis for the image of \\(A\\) . (c) Verify the Rank-Nullity Theorem. Problem 8: Affine Space -- Parametric Form of Solution Set \u00b6 Find the general solution (in affine/parametric form) of: \\[\\begin{bmatrix} 1 & 1 & 2 \\\\ 2 & 2 & 4 \\end{bmatrix}\\mathbf{x} = \\begin{bmatrix} 4 \\\\ 8 \\end{bmatrix}\\] Identify the particular solution, the kernel, and explain why the solution set is an affine subspace. Solutions \u00b6 Solution 1: Write the augmented matrix: \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 9 \\\\ 2 & 4 & -3 & 1 \\\\ 3 & 6 & -5 & 0 \\end{array}\\right]\\] \\(R_2 \\to R_2 - 2R_1\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 9 \\\\ 0 & 2 & -7 & -17 \\\\ 3 & 6 & -5 & 0 \\end{array}\\right]\\] \\(R_3 \\to R_3 - 3R_1\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 9 \\\\ 0 & 2 & -7 & -17 \\\\ 0 & 3 & -11 & -27 \\end{array}\\right]\\] \\(R_3 \\to R_3 - \\frac{3}{2}R_2\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 9 \\\\ 0 & 2 & -7 & -17 \\\\ 0 & 0 & -\\frac{1}{2} & -\\frac{3}{2} \\end{array}\\right]\\] \\(R_3 \\to -2R_3\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 9 \\\\ 0 & 2 & -7 & -17 \\\\ 0 & 0 & 1 & 3 \\end{array}\\right]\\] Back substitution: From row 3: \\(x_3 = 3\\) . From row 2: \\(2x_2 - 7(3) = -17 \\implies 2x_2 = 4 \\implies x_2 = 2\\) . From row 1: \\(x_1 + 2 + 2(3) = 9 \\implies x_1 = 9 - 2 - 6 = 1\\) . \\[x_1 = 1, \\quad x_2 = 2, \\quad x_3 = 3\\] Verify: \\(1+2+6=9\\) \\(\\checkmark\\) , \\(2+8-9=1\\) \\(\\checkmark\\) , \\(3+12-15=0\\) \\(\\checkmark\\) . Solution 2: \\[\\det(A) = 3(2) - 5(1) = 6 - 5 = 1\\] \\[A^{-1} = \\frac{1}{1}\\begin{bmatrix} 2 & -5 \\\\ -1 & 3 \\end{bmatrix} = \\begin{bmatrix} 2 & -5 \\\\ -1 & 3 \\end{bmatrix}\\] Verify: \\[AA^{-1} = \\begin{bmatrix} 3 & 5 \\\\ 1 & 2 \\end{bmatrix}\\begin{bmatrix} 2 & -5 \\\\ -1 & 3 \\end{bmatrix} = \\begin{bmatrix} 3(2)+5(-1) & 3(-5)+5(3) \\\\ 1(2)+2(-1) & 1(-5)+2(3) \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I \\quad \\checkmark\\] Solution 3: We apply the 3-step subspace test. Step 1 (Zero vector): Let \\(x_1 = x_2 = x_3 = 0\\) . Then \\(0 + 2(0) - 0 = 0\\) . So \\(\\mathbf{0} \\in U\\) . \\(\\checkmark\\) Step 2 (Closed under addition): Let \\(\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}\\) both be in \\(U\\) , so \\(u_1 + 2u_2 - u_3 = 0\\) and \\(v_1 + 2v_2 - v_3 = 0\\) . Then \\(\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1+v_1 \\\\ u_2+v_2 \\\\ u_3+v_3 \\end{bmatrix}\\) and: \\[(u_1+v_1) + 2(u_2+v_2) - (u_3+v_3) = (u_1+2u_2-u_3) + (v_1+2v_2-v_3) = 0 + 0 = 0\\] So \\(\\mathbf{u} + \\mathbf{v} \\in U\\) . \\(\\checkmark\\) Step 3 (Closed under scalar multiplication): Let \\(\\alpha \\in \\mathbb{R}\\) and \\(\\mathbf{u} \\in U\\) . \\[\\alpha u_1 + 2(\\alpha u_2) - \\alpha u_3 = \\alpha(u_1 + 2u_2 - u_3) = \\alpha \\cdot 0 = 0\\] So \\(\\alpha \\mathbf{u} \\in U\\) . \\(\\checkmark\\) Conclusion: \\(U\\) is a subspace of \\(\\mathbb{R}^3\\) . (Geometrically, it is a plane through the origin.) Solution 4: Form the matrix and row reduce: \\[\\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 1 \\\\ 2 & -1 & 3 \\end{bmatrix} \\xrightarrow{R_3 - 2R_1} \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 1 \\\\ 0 & -1 & -1 \\end{bmatrix} \\xrightarrow{R_3 + R_2} \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}\\] There are 2 pivots for 3 vectors. Column 3 has no pivot, meaning \\(\\mathbf{v}_3\\) depends on the others. The vectors are linearly dependent . From the RREF: \\(\\mathbf{v}_3 = 2\\mathbf{v}_1 + 1\\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}\\) . \\(\\checkmark\\) Solution 5: Row reduce \\(A\\) : \\[\\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 6 & 10 \\\\ 1 & 1 & 3 \\end{bmatrix} \\xrightarrow{R_2 - 2R_1} \\begin{bmatrix} 1 & 3 & 5 \\\\ 0 & 0 & 0 \\\\ 1 & 1 & 3 \\end{bmatrix} \\xrightarrow{R_3 - R_1} \\begin{bmatrix} 1 & 3 & 5 \\\\ 0 & 0 & 0 \\\\ 0 & -2 & -2 \\end{bmatrix} \\xrightarrow{R_2 \\leftrightarrow R_3} \\begin{bmatrix} 1 & 3 & 5 \\\\ 0 & -2 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix}\\] Pivots are in columns 1 and 2. A basis for the column space consists of the corresponding original columns of \\(A\\) : \\[\\text{Basis} = \\left\\{\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 3 \\\\ 6 \\\\ 1 \\end{bmatrix}\\right\\}\\] Dimension: \\(\\dim(\\text{Col}(A)) = \\text{rank}(A) = 2\\) . Solution 6: Row reduce \\(M\\) : \\[\\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 2 & 4 & 3 & 1 \\\\ 3 & 6 & 4 & 1 \\end{bmatrix} \\xrightarrow{R_2 - 2R_1} \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 3 & 6 & 4 & 1 \\end{bmatrix} \\xrightarrow{R_3 - 3R_1} \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix} \\xrightarrow{R_3 - R_2} \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\] There are 2 pivots (in columns 1 and 3). \\[\\text{rank}(M) = 2\\] Solution 7: (a) Kernel: Solve \\(A\\mathbf{x} = \\mathbf{0}\\) . \\[\\begin{bmatrix} 1 & 0 & -1 \\\\ 2 & 1 & 0 \\\\ -1 & 1 & 2 \\end{bmatrix} \\xrightarrow{R_2 - 2R_1} \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ -1 & 1 & 2 \\end{bmatrix} \\xrightarrow{R_3 + R_1} \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 1 & 1 \\end{bmatrix} \\xrightarrow{R_3 - R_2} \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & -1 \\end{bmatrix}\\] Three pivots for three unknowns, so the only solution to \\(A\\mathbf{x} = \\mathbf{0}\\) is \\(\\mathbf{x} = \\mathbf{0}\\) . \\[\\ker(A) = \\{\\mathbf{0}\\}\\] (b) Image: Since \\(\\text{rank}(A) = 3\\) and \\(A \\in \\mathbb{R}^{3 \\times 3}\\) , all three columns are linearly independent. A basis for the image is: \\[\\text{Basis for Im}(A) = \\left\\{\\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 0 \\\\ 2 \\end{bmatrix}\\right\\}\\] So \\(\\text{Im}(A) = \\mathbb{R}^3\\) (the mapping is surjective). (c) Verify Rank-Nullity: \\[\\dim(\\ker(A)) + \\dim(\\text{Im}(A)) = 0 + 3 = 3 = n \\quad \\checkmark\\] Since \\(\\ker(A) = \\{\\mathbf{0}\\}\\) (injective) and \\(\\text{Im}(A) = \\mathbb{R}^3\\) (surjective), the mapping is bijective and \\(A\\) is invertible. Solution 8: Write the augmented matrix and row reduce: \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 4 \\\\ 2 & 2 & 4 & 8 \\end{array}\\right] \\xrightarrow{R_2 - 2R_1} \\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 4 \\\\ 0 & 0 & 0 & 0 \\end{array}\\right]\\] There is 1 pivot (in column 1) and 2 free variables ( \\(x_2\\) and \\(x_3\\) ). From the first row: \\(x_1 + x_2 + 2x_3 = 4\\) , so \\(x_1 = 4 - x_2 - 2x_3\\) . Particular solution: Set the free variables to zero ( \\(x_2 = 0, x_3 = 0\\) ): \\[\\mathbf{x}_p = \\begin{bmatrix} 4 \\\\ 0 \\\\ 0 \\end{bmatrix}\\] Kernel: Solve \\(A\\mathbf{x} = \\mathbf{0}\\) : \\(x_1 = -x_2 - 2x_3\\) , with \\(x_2 = s, x_3 = t\\) free: \\[\\ker(A) = \\text{span}\\left\\{\\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\end{bmatrix}\\right\\}\\] General solution (affine/parametric form): \\[\\mathbf{x} = \\underbrace{\\begin{bmatrix} 4 \\\\ 0 \\\\ 0 \\end{bmatrix}}_{\\mathbf{x}_p} + s\\underbrace{\\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix}}_{\\text{kernel direction 1}} + t\\underbrace{\\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\end{bmatrix}}_{\\text{kernel direction 2}}, \\quad s, t \\in \\mathbb{R}\\] Why is this an affine subspace? The solution set does not pass through the origin (when \\(s = t = 0\\) , the solution is \\(\\begin{bmatrix} 4 \\\\ 0 \\\\ 0 \\end{bmatrix} \\neq \\mathbf{0}\\) ). It is formed by taking one particular solution \\(\\mathbf{x}_p\\) and adding all elements of the kernel, which is a 2-dimensional vector subspace. The result is a 2-dimensional plane in \\(\\mathbb{R}^3\\) , shifted away from the origin -- a classic affine subspace. Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Next: Tutorial - Analytic Geometry","title":"Tutorial 2: Linear Algebra"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#tutorial-2-linear-algebra","text":"Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari","title":"Tutorial 2: Linear Algebra"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#learning-objectives","text":"By the end of this tutorial, you will understand: Systems of linear equations and the three types of solutions Matrices, their types, and core operations Matrix inverses and how to compute them Gaussian elimination and row echelon form Vector spaces, subspaces, and the subspace test Linear independence and how to check it Span, generating sets, basis, and dimension Rank of a matrix and its role in solvability Linear mappings, kernel, and image The Rank-Nullity Theorem Change of basis Affine spaces and their connection to solution sets","title":"\ud83d\udcda Learning Objectives"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-1-systems-of-linear-equations","text":"","title":"Part 1: Systems of Linear Equations"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#11-what-is-a-system-of-linear-equations","text":"A system of linear equations is a collection of equations where each equation is linear (no powers, no products of variables). We want to find the values of the unknowns that satisfy all the equations at the same time. General form (two equations, two unknowns): \\[a_{11}x_1 + a_{12}x_2 = b_1\\] \\[a_{21}x_1 + a_{22}x_2 = b_2\\] In plain English: we have some unknowns ( \\(x_1, x_2\\) ), each multiplied by known constants ( \\(a_{ij}\\) ), and we want the results to equal known values ( \\(b_1, b_2\\) ). Think of it as... each equation describes a line (in 2D) or a plane (in 3D). Solving the system means finding where all the lines or planes intersect.","title":"1.1 What is a System of Linear Equations?"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#12-the-three-types-of-solutions","text":"Every system of linear equations has exactly one of three outcomes: Type Description Geometric Picture (2D) Example Unique solution Exactly one solution Two lines cross at a single point \\(x + y = 3\\) , \\(x - y = 1\\) gives \\(x=2, y=1\\) Infinitely many solutions A family of solutions Two lines lie on top of each other \\(x + y = 2\\) , \\(2x + 2y = 4\\) (same line) No solution The system is inconsistent Two lines are parallel and never meet \\(x + y = 1\\) , \\(x + y = 3\\) (parallel lines) Worked Example: Determine the type of solution. System 1: \\(x + y = 5\\) and \\(x - y = 1\\) Add the equations: \\(2x = 6\\) , so \\(x = 3\\) , \\(y = 2\\) . Unique solution. System 2: \\(x + y = 2\\) and \\(2x + 2y = 4\\) The second equation is just \\(2 \\times\\) the first. Every point on the line \\(x + y = 2\\) is a solution. Infinitely many solutions. System 3: \\(x + y = 1\\) and \\(x + y = 3\\) These say the same expression \\(x+y\\) equals two different things. Impossible. No solution.","title":"1.2 The Three Types of Solutions"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#13-matrix-notation-amathbfx-mathbfb","text":"We can write any system of linear equations compactly using matrices: \\[\\underbrace{\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{bmatrix}}_{A} \\underbrace{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}}_{\\mathbf{x}} = \\underbrace{\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix}}_{\\mathbf{b}}\\] \\(A\\) is the coefficient matrix ( \\(m \\times n\\) ) \\(\\mathbf{x}\\) is the unknown vector ( \\(n \\times 1\\) ) \\(\\mathbf{b}\\) is the right-hand side vector ( \\(m \\times 1\\) )","title":"1.3 Matrix Notation: \\(A\\mathbf{x} = \\mathbf{b}\\)"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#14-the-augmented-matrix","text":"The augmented matrix \\([A \\mid \\mathbf{b}]\\) combines the coefficient matrix and the right-hand side into one object, which is convenient for Gaussian elimination: \\[[A \\mid \\mathbf{b}] = \\left[\\begin{array}{ccc|c} a_{11} & a_{12} & \\cdots & b_1 \\\\ a_{21} & a_{22} & \\cdots & b_2 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & b_m \\end{array}\\right]\\] Example: The system \\(2x + 3y = 7\\) and \\(x - y = 1\\) becomes: \\[\\left[\\begin{array}{cc|c} 2 & 3 & 7 \\\\ 1 & -1 & 1 \\end{array}\\right]\\] Think of it as... packing all the important numbers from your system into one neat table, with a vertical line separating the left side from the right side of the equals sign.","title":"1.4 The Augmented Matrix"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-2-matrices","text":"","title":"Part 2: Matrices"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#21-definition","text":"A matrix is a rectangular array of numbers with \\(m\\) rows and \\(n\\) columns. We say it has dimensions \\(m \\times n\\) (read \"m by n\"). \\[A \\in \\mathbb{R}^{m \\times n} \\quad \\text{means } A \\text{ has } m \\text{ rows and } n \\text{ columns of real numbers.}\\] The entry in row \\(i\\) and column \\(j\\) is written \\(a_{ij}\\) .","title":"2.1 Definition"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#22-special-matrices","text":"Matrix Definition Example Identity \\(I_n\\) Square matrix with 1s on the diagonal, 0s everywhere else \\(I_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\) Zero \\(\\mathbf{0}\\) All entries are zero \\(\\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}\\) Diagonal Only the diagonal entries can be nonzero \\(\\begin{bmatrix} 3 & 0 \\\\ 0 & 7 \\end{bmatrix}\\) Symmetric \\(A = A^T\\) (equal to its own transpose) \\(\\begin{bmatrix} 1 & 4 \\\\ 4 & 5 \\end{bmatrix}\\) Square Number of rows equals number of columns ( \\(m = n\\) ) Any \\(n \\times n\\) matrix Key property of the identity matrix: For any matrix \\(A\\) of compatible size: \\[AI = IA = A\\] Think of it as... the identity matrix is like multiplying by 1. It does nothing to whatever it touches.","title":"2.2 Special Matrices"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#23-matrix-operations","text":"","title":"2.3 Matrix Operations"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#matrix-addition","text":"Add corresponding entries. Both matrices must have the same dimensions. \\[\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}\\]","title":"Matrix Addition"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#scalar-multiplication","text":"Multiply every entry by a number. \\[3 \\begin{bmatrix} 1 & 2 \\\\ 4 & 5 \\end{bmatrix} = \\begin{bmatrix} 3 & 6 \\\\ 12 & 15 \\end{bmatrix}\\]","title":"Scalar Multiplication"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#matrix-multiplication","text":"To compute \\(C = AB\\) where \\(A\\) is \\(m \\times n\\) and \\(B\\) is \\(n \\times p\\) , each entry of \\(C\\) is: \\[c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj} = (\\text{row } i \\text{ of } A) \\cdot (\\text{column } j \\text{ of } B)\\] The result \\(C\\) has dimensions \\(m \\times p\\) . Requirement: The number of columns of \\(A\\) must equal the number of rows of \\(B\\) . Worked Example: \\[\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 1(5)+2(7) & 1(6)+2(8) \\\\ 3(5)+4(7) & 3(6)+4(8) \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\\] Critical fact: Matrix multiplication is NOT commutative in general: \\[AB \\neq BA\\] Think of it as... putting on socks then shoes is not the same as putting on shoes then socks. The order matters.","title":"Matrix Multiplication"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#24-transpose","text":"The transpose \\(A^T\\) flips a matrix over its diagonal: rows become columns and columns become rows. If \\(A\\) is \\(m \\times n\\) , then \\(A^T\\) is \\(n \\times m\\) , and \\((A^T)_{ij} = A_{ji}\\) . Example: \\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}, \\quad A^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\] Transpose properties: Property Formula Double transpose \\((A^T)^T = A\\) Sum \\((A + B)^T = A^T + B^T\\) Scalar \\((cA)^T = cA^T\\) Product \\((AB)^T = B^T A^T\\) (note the reversed order!)","title":"2.4 Transpose"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-3-matrix-inverse","text":"","title":"Part 3: Matrix Inverse"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#31-definition","text":"The inverse of a square matrix \\(A\\) is a matrix \\(A^{-1}\\) such that: \\[AA^{-1} = A^{-1}A = I\\] In plain English: \\(A^{-1}\\) \"undoes\" whatever \\(A\\) does. If \\(A\\) transforms a vector, \\(A^{-1}\\) transforms it back. A matrix that has an inverse is called invertible (or non-singular or regular ). A matrix that does not have an inverse is called singular . Think of it as... the inverse is like an \"undo button.\" If \\(A\\) scrambles your data, \\(A^{-1}\\) unscrambles it perfectly.","title":"3.1 Definition"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#32-when-does-the-inverse-exist","text":"A square matrix \\(A\\) is invertible if and only if: \\(\\det(A) \\neq 0\\) The columns of \\(A\\) are linearly independent The only solution to \\(A\\mathbf{x} = \\mathbf{0}\\) is \\(\\mathbf{x} = \\mathbf{0}\\) \\(\\text{rank}(A) = n\\) (full rank) These are all equivalent conditions -- if one is true, they are all true.","title":"3.2 When Does the Inverse Exist?"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#33-computing-the-inverse-of-a-2x2-matrix","text":"For \\(A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) , the inverse is: \\[A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\] The number \\(ad - bc\\) is the determinant \\(\\det(A)\\) . If \\(\\det(A) = 0\\) , the inverse does not exist. Worked Example: \\[A = \\begin{bmatrix} 4 & 7 \\\\ 2 & 6 \\end{bmatrix}\\] \\[\\det(A) = 4(6) - 7(2) = 24 - 14 = 10\\] \\[A^{-1} = \\frac{1}{10}\\begin{bmatrix} 6 & -7 \\\\ -2 & 4 \\end{bmatrix} = \\begin{bmatrix} 0.6 & -0.7 \\\\ -0.2 & 0.4 \\end{bmatrix}\\] Verify: \\(AA^{-1} = \\begin{bmatrix} 4 & 7 \\\\ 2 & 6 \\end{bmatrix}\\begin{bmatrix} 0.6 & -0.7 \\\\ -0.2 & 0.4 \\end{bmatrix} = \\begin{bmatrix} 2.4-1.4 & -2.8+2.8 \\\\ 1.2-1.2 & -1.4+2.4 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I \\quad \\checkmark\\)","title":"3.3 Computing the Inverse of a 2x2 Matrix"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#34-properties-of-the-inverse","text":"Property Formula Inverse of inverse \\((A^{-1})^{-1} = A\\) Inverse of product \\((AB)^{-1} = B^{-1}A^{-1}\\) (note the reversed order!) Inverse of transpose \\((A^T)^{-1} = (A^{-1})^T\\) Inverse of scalar multiple \\((cA)^{-1} = \\frac{1}{c}A^{-1}\\) for \\(c \\neq 0\\) The property \\((AB)^{-1} = B^{-1}A^{-1}\\) is very important. Notice the order reverses -- just like with the transpose of a product. Think of it as... if you put on socks then shoes, to undo it you take off shoes first, then socks. The same \"reverse order\" logic applies to matrix inverses.","title":"3.4 Properties of the Inverse"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-4-solving-systems-gaussian-elimination","text":"","title":"Part 4: Solving Systems -- Gaussian Elimination"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#41-the-idea","text":"Gaussian elimination is a systematic method for solving \\(A\\mathbf{x} = \\mathbf{b}\\) . The idea is simple: use allowed row operations to transform the augmented matrix \\([A \\mid \\mathbf{b}]\\) into a simpler form from which the solution is easy to read off.","title":"4.1 The Idea"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#42-elementary-row-operations","text":"There are three operations you are allowed to perform on rows. None of them change the solution set of the system. Operation Description Example Swap Exchange two rows: \\(R_i \\leftrightarrow R_j\\) Swap row 1 and row 2 Scale Multiply a row by a nonzero constant: \\(R_i \\to cR_i\\) Multiply row 2 by \\(\\frac{1}{3}\\) Add Add a multiple of one row to another: \\(R_i \\to R_i + cR_j\\) Add \\((-2) \\times\\) row 1 to row 2 Think of it as... you are doing the same algebra you would do by hand (adding equations, multiplying both sides by a constant), but in a more organized table format.","title":"4.2 Elementary Row Operations"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#43-row-echelon-form-ref","text":"A matrix is in row echelon form if: All rows that are entirely zero are at the bottom The first nonzero entry in each row (called the pivot ) is to the right of the pivot in the row above All entries below each pivot are zero Example of REF: \\[\\begin{bmatrix} 2 & 1 & -1 \\\\ 0 & 3 & 5 \\\\ 0 & 0 & 4 \\end{bmatrix}\\] The pivots are 2, 3, and 4. Each is to the right and below the previous one, forming a \"staircase\" pattern.","title":"4.3 Row Echelon Form (REF)"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#44-reduced-row-echelon-form-rref","text":"A matrix is in reduced row echelon form if it is in REF and additionally: Every pivot is 1 Every pivot is the only nonzero entry in its column Example of RREF: \\[\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\] When the augmented matrix \\([A \\mid \\mathbf{b}]\\) is in RREF, you can read the solution directly.","title":"4.4 Reduced Row Echelon Form (RREF)"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#45-full-worked-example-solving-a-system-via-gaussian-elimination","text":"Solve the system: \\[x_1 + 2x_2 - x_3 = 3\\] \\[2x_1 + 5x_2 - 2x_3 = 7\\] \\[-x_1 - x_2 + 3x_3 = 2\\] Step 1: Write the augmented matrix. \\[\\left[\\begin{array}{ccc|c} 1 & 2 & -1 & 3 \\\\ 2 & 5 & -2 & 7 \\\\ -1 & -1 & 3 & 2 \\end{array}\\right]\\] Step 2: Eliminate below the first pivot (the 1 in position \\((1,1)\\) ). \\(R_2 \\to R_2 - 2R_1\\) : Replace row 2 with (row 2 minus 2 times row 1). \\(R_3 \\to R_3 + R_1\\) : Replace row 3 with (row 3 plus row 1). \\[\\left[\\begin{array}{ccc|c} 1 & 2 & -1 & 3 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 1 & 2 & 5 \\end{array}\\right]\\] Step 3: Eliminate below the second pivot (the 1 in position \\((2,2)\\) ). \\(R_3 \\to R_3 - R_2\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 2 & -1 & 3 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 2 & 4 \\end{array}\\right]\\] This is now in row echelon form . We have three pivots for three unknowns, so there is a unique solution. Step 4: Scale row 3 to make the pivot equal to 1. \\(R_3 \\to \\frac{1}{2}R_3\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 2 & -1 & 3 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 2 \\end{array}\\right]\\] Step 5: Back substitution (or continue to RREF). Let us continue to RREF. \\(R_1 \\to R_1 + R_3\\) (eliminate the \\(-1\\) in position \\((1,3)\\) ): \\[\\left[\\begin{array}{ccc|c} 1 & 2 & 0 & 5 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 2 \\end{array}\\right]\\] \\(R_1 \\to R_1 - 2R_2\\) (eliminate the \\(2\\) in position \\((1,2)\\) ): \\[\\left[\\begin{array}{ccc|c} 1 & 0 & 0 & 3 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 2 \\end{array}\\right]\\] Step 6: Read the solution from RREF. \\[x_1 = 3, \\quad x_2 = 1, \\quad x_3 = 2\\] Verify: Plug back into the original equations: \\(1(3) + 2(1) - 1(2) = 3 + 2 - 2 = 3\\) \\(\\checkmark\\) \\(2(3) + 5(1) - 2(2) = 6 + 5 - 4 = 7\\) \\(\\checkmark\\) \\(-1(3) - 1(1) + 3(2) = -3 - 1 + 6 = 2\\) \\(\\checkmark\\)","title":"4.5 Full Worked Example: Solving a System via Gaussian Elimination"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-5-vector-spaces","text":"","title":"Part 5: Vector Spaces"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#51-what-is-a-vector-space","text":"A vector space is a set \\(V\\) of objects (called \"vectors\") together with two operations -- vector addition and scalar multiplication -- that satisfy certain rules (axioms). In plain English: a vector space is a collection of things that you can add together and scale, and the result always stays inside the same collection. Think of it as... a playground with fences. You can run around (add vectors, scale them) however you like, but you can never leave the playground. Everything you create stays inside.","title":"5.1 What is a Vector Space?"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#52-the-vector-space-axioms","text":"For all \\(\\mathbf{u}, \\mathbf{v}, \\mathbf{w} \\in V\\) and all scalars \\(\\alpha, \\beta \\in \\mathbb{R}\\) : Axiom Statement In Plain English Closure under addition \\(\\mathbf{u} + \\mathbf{v} \\in V\\) Adding two vectors keeps you in \\(V\\) Closure under scalar multiplication \\(\\alpha \\mathbf{u} \\in V\\) Scaling a vector keeps you in \\(V\\) Associativity of addition \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\) Grouping does not matter for addition Commutativity of addition \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\) Order does not matter for addition Zero vector exists \\(\\exists \\mathbf{0} \\in V : \\mathbf{u} + \\mathbf{0} = \\mathbf{u}\\) There is a \"do-nothing\" element Additive inverse \\(\\forall \\mathbf{u} \\in V, \\exists (-\\mathbf{u}) : \\mathbf{u} + (-\\mathbf{u}) = \\mathbf{0}\\) Every vector has a \"negative\" Distributivity (vector) \\(\\alpha(\\mathbf{u} + \\mathbf{v}) = \\alpha\\mathbf{u} + \\alpha\\mathbf{v}\\) Scaling distributes over vector addition Distributivity (scalar) \\((\\alpha + \\beta)\\mathbf{u} = \\alpha\\mathbf{u} + \\beta\\mathbf{u}\\) Scaling distributes over scalar addition Associativity of scaling \\(\\alpha(\\beta \\mathbf{u}) = (\\alpha\\beta)\\mathbf{u}\\) Order of scaling does not matter Identity of scaling \\(1 \\cdot \\mathbf{u} = \\mathbf{u}\\) Scaling by 1 does nothing","title":"5.2 The Vector Space Axioms"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#53-common-examples-of-vector-spaces","text":"\\(\\mathbb{R}^n\\) : the set of all \\(n\\) -tuples of real numbers (the most common example) \\(\\mathbb{R}^{m \\times n}\\) : the set of all \\(m \\times n\\) real matrices The set of all polynomials of degree at most \\(n\\) The set of all continuous functions \\(f: \\mathbb{R} \\to \\mathbb{R}\\)","title":"5.3 Common Examples of Vector Spaces"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#54-subspaces","text":"A subspace \\(U\\) of a vector space \\(V\\) is a subset \\(U \\subseteq V\\) that is itself a vector space under the same operations.","title":"5.4 Subspaces"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#the-3-step-subspace-test","text":"To check whether a subset \\(U \\subseteq V\\) is a subspace, verify three things: Step Check Why 1 \\(\\mathbf{0} \\in U\\) (the zero vector is in \\(U\\) ) Every vector space must contain the zero vector 2 \\(\\mathbf{u} + \\mathbf{v} \\in U\\) for all \\(\\mathbf{u}, \\mathbf{v} \\in U\\) (closed under addition) Adding members should not kick you out 3 \\(\\alpha \\mathbf{u} \\in U\\) for all \\(\\alpha \\in \\mathbb{R}\\) , \\(\\mathbf{u} \\in U\\) (closed under scalar multiplication) Scaling should not kick you out If all three pass, \\(U\\) is a subspace. If any one fails, it is not. Think of it as... a \"mini playground\" inside the bigger playground. As long as the mini playground contains the origin and any combination of its members stays inside, it qualifies as a subspace. Example (is a subspace): Let \\(U = \\left\\{\\begin{bmatrix} x \\\\ 0 \\end{bmatrix} : x \\in \\mathbb{R}\\right\\} \\subseteq \\mathbb{R}^2\\) (the \\(x\\) -axis). Zero vector: \\(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\in U\\) \\(\\checkmark\\) Closed under addition: \\(\\begin{bmatrix} a \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} b \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} a+b \\\\ 0 \\end{bmatrix} \\in U\\) \\(\\checkmark\\) Closed under scalar multiplication: \\(c\\begin{bmatrix} a \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} ca \\\\ 0 \\end{bmatrix} \\in U\\) \\(\\checkmark\\) Example (not a subspace): Let \\(W = \\left\\{\\begin{bmatrix} x \\\\ 1 \\end{bmatrix} : x \\in \\mathbb{R}\\right\\} \\subseteq \\mathbb{R}^2\\) (a horizontal line at height 1). Zero vector: \\(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\notin W\\) because the second component is always 1, not 0. \\(\\times\\) \\(W\\) fails the very first check, so it is not a subspace.","title":"The 3-Step Subspace Test"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-6-linear-independence","text":"","title":"Part 6: Linear Independence"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#61-definition","text":"A set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}\\) is linearly independent if the only way to combine them to get the zero vector is with all coefficients equal to zero: \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0} \\implies c_1 = c_2 = \\cdots = c_k = 0\\] If there is a nontrivial combination (some \\(c_i \\neq 0\\) ) that gives \\(\\mathbf{0}\\) , the vectors are linearly dependent . In plain English: vectors are independent when none of them is \"redundant\" -- you cannot build any one of them from the others. Think of it as... independent vectors each bring something genuinely new to the table. Dependent vectors are freeloaders -- at least one of them could be replaced by a combination of the rest.","title":"6.1 Definition"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#62-how-to-check-linear-independence","text":"Method: Form a matrix with the vectors as columns and row-reduce to REF. Count the pivots. If every column has a pivot, the vectors are linearly independent . If any column lacks a pivot, the vectors are linearly dependent . Worked Example: Are the following vectors linearly independent? \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}\\] Form the matrix \\([\\mathbf{v}_1 \\mid \\mathbf{v}_2 \\mid \\mathbf{v}_3]\\) and row reduce: \\[\\begin{bmatrix} 1 & 0 & 1 \\\\ 2 & 1 & 3 \\\\ 3 & 1 & 4 \\end{bmatrix} \\xrightarrow{R_2 - 2R_1} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 3 & 1 & 4 \\end{bmatrix} \\xrightarrow{R_3 - 3R_1} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\xrightarrow{R_3 - R_2} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}\\] Only 2 pivots for 3 vectors. Column 3 has no pivot, so \\(\\mathbf{v}_3\\) is a linear combination of the others. The vectors are linearly dependent . Indeed, \\(\\mathbf{v}_3 = \\mathbf{v}_1 + \\mathbf{v}_2 = \\begin{bmatrix} 1+0 \\\\ 2+1 \\\\ 3+1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}\\) .","title":"6.2 How to Check Linear Independence"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#63-geometric-meaning","text":"Number of vectors Independent means... Dependent means... 2 vectors in \\(\\mathbb{R}^2\\) They point in different directions They lie on the same line 3 vectors in \\(\\mathbb{R}^3\\) They span all of 3D space They all lie in the same plane (or line) \\(k\\) vectors in \\(\\mathbb{R}^n\\) They span a \\(k\\) -dimensional subspace They span something less than \\(k\\) -dimensional","title":"6.3 Geometric Meaning"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#64-maximum-number-of-independent-vectors","text":"In \\(\\mathbb{R}^n\\) , you can have at most \\(n\\) linearly independent vectors. If you have more than \\(n\\) vectors in \\(\\mathbb{R}^n\\) , they are guaranteed to be linearly dependent.","title":"6.4 Maximum Number of Independent Vectors"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-7-generating-sets-span-and-basis","text":"","title":"Part 7: Generating Sets, Span, and Basis"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#71-span","text":"The span of a set of vectors is the set of all possible linear combinations of those vectors: \\[\\text{span}\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\} = \\left\\{c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k : c_1, \\ldots, c_k \\in \\mathbb{R}\\right\\}\\] In plain English: the span is everything you can \"reach\" by adding and scaling the given vectors.","title":"7.1 Span"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#72-generating-set","text":"A set of vectors \\(\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\) is a generating set (or spanning set) for a vector space \\(V\\) if: \\[V = \\text{span}\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\] This means every vector in \\(V\\) can be written as a linear combination of \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) .","title":"7.2 Generating Set"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#73-basis","text":"A basis for a vector space \\(V\\) is a set of vectors that is: Linearly independent (no redundancy) Spanning (reaches everything in \\(V\\) ) A basis is the most \"efficient\" generating set -- it spans the whole space with no wasted vectors. Standard basis for \\(\\mathbb{R}^3\\) : \\[\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\] These three vectors are linearly independent and every vector in \\(\\mathbb{R}^3\\) can be written as \\(\\mathbf{v} = v_1\\mathbf{e}_1 + v_2\\mathbf{e}_2 + v_3\\mathbf{e}_3\\) . Think of it as... a basis is like a set of building blocks. You need enough blocks to build anything in the space, but you do not want any block that is just a copy of others glued together. The standard basis in \\(\\mathbb{R}^3\\) uses the three coordinate axes as blocks.","title":"7.3 Basis"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#74-dimension","text":"The dimension of a vector space \\(V\\) is the number of vectors in any basis of \\(V\\) . This number is always the same no matter which basis you choose. \\[\\dim(\\mathbb{R}^n) = n\\] Examples: A line through the origin in \\(\\mathbb{R}^3\\) has dimension 1 A plane through the origin in \\(\\mathbb{R}^3\\) has dimension 2 \\(\\mathbb{R}^3\\) itself has dimension 3","title":"7.4 Dimension"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#75-finding-a-basis-for-a-set-of-vectors","text":"Given vectors \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\) , to find a basis for \\(\\text{span}\\{\\mathbf{v}_1, \\ldots, \\mathbf{v}_k\\}\\) : Form the matrix \\(A = [\\mathbf{v}_1 \\mid \\cdots \\mid \\mathbf{v}_k]\\) Row reduce to REF The original vectors corresponding to pivot columns form a basis","title":"7.5 Finding a Basis for a Set of Vectors"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-8-rank","text":"","title":"Part 8: Rank"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#81-definition","text":"The rank of a matrix \\(A\\) is the number of linearly independent columns (equivalently, the number of linearly independent rows). A fundamental fact: column rank = row rank . This means you get the same number whether you count independent columns or independent rows.","title":"8.1 Definition"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#82-computing-rank-via-rref","text":"To find the rank of \\(A\\) : Row reduce \\(A\\) to REF or RREF Count the number of pivots (the nonzero leading entries) \\[\\text{rank}(A) = \\text{number of pivots}\\] Worked Example: \\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\\\ 0 & 1 & 1 \\end{bmatrix}\\] Row reduce: \\[\\xrightarrow{R_2 - 2R_1} \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\\\ 0 & 1 & 1 \\end{bmatrix} \\xrightarrow{R_2 \\leftrightarrow R_3} \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}\\] There are 2 pivots (in columns 1 and 2), so \\(\\text{rank}(A) = 2\\) .","title":"8.2 Computing Rank via RREF"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#83-rank-and-solvability-of-amathbfx-mathbfb","text":"The rank tells you about the solvability of \\(A\\mathbf{x} = \\mathbf{b}\\) : Condition What it means \\(\\text{rank}(A) = \\text{rank}([A \\mid \\mathbf{b}])\\) The system \\(A\\mathbf{x} = \\mathbf{b}\\) has at least one solution \\(\\text{rank}(A) < \\text{rank}([A \\mid \\mathbf{b}])\\) The system \\(A\\mathbf{x} = \\mathbf{b}\\) has no solution \\(\\text{rank}(A) = n\\) (number of unknowns) If a solution exists, it is unique \\(\\text{rank}(A) < n\\) If a solution exists, there are infinitely many Think of it as... rank tells you \"how many genuinely useful equations\" you have. If the rank equals the number of unknowns, you have enough information to pin down a unique answer. If the rank is less, you have leftover freedom, meaning infinitely many solutions.","title":"8.3 Rank and Solvability of \\(A\\mathbf{x} = \\mathbf{b}\\)"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-9-linear-mappings","text":"","title":"Part 9: Linear Mappings"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#91-definition","text":"A function \\(\\Phi: V \\to W\\) between vector spaces is a linear mapping (or linear transformation) if it preserves both addition and scalar multiplication: \\(\\Phi(\\mathbf{u} + \\mathbf{v}) = \\Phi(\\mathbf{u}) + \\Phi(\\mathbf{v})\\) for all \\(\\mathbf{u}, \\mathbf{v} \\in V\\) (preserves addition) \\(\\Phi(\\alpha \\mathbf{u}) = \\alpha \\Phi(\\mathbf{u})\\) for all \\(\\alpha \\in \\mathbb{R}\\) , \\(\\mathbf{u} \\in V\\) (preserves scalar multiplication) Equivalently, in one combined condition: \\[\\Phi(\\alpha \\mathbf{u} + \\beta \\mathbf{v}) = \\alpha \\Phi(\\mathbf{u}) + \\beta \\Phi(\\mathbf{v})\\] In plain English: a linear mapping respects the structure of the vector space. It does not \"break\" addition or scaling. Think of it as... a linear mapping is a \"well-behaved\" transformation. It keeps straight lines straight, keeps the origin fixed, and does not do anything \"nonlinear\" like bending or shifting.","title":"9.1 Definition"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#92-transformation-matrix","text":"Every linear mapping \\(\\Phi: \\mathbb{R}^n \\to \\mathbb{R}^m\\) can be represented by a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) such that: \\[\\Phi(\\mathbf{x}) = A\\mathbf{x}\\] To find \\(A\\) , compute what \\(\\Phi\\) does to each standard basis vector \\(\\mathbf{e}_j\\) and place the result as the \\(j\\) -th column of \\(A\\) : \\[A = [\\Phi(\\mathbf{e}_1) \\mid \\Phi(\\mathbf{e}_2) \\mid \\cdots \\mid \\Phi(\\mathbf{e}_n)]\\]","title":"9.2 Transformation Matrix"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#93-kernel-null-space","text":"The kernel (or null space) of a linear mapping \\(\\Phi: V \\to W\\) (or equivalently of its matrix \\(A\\) ) is the set of all vectors that map to the zero vector: \\[\\ker(\\Phi) = \\{\\mathbf{x} \\in V : \\Phi(\\mathbf{x}) = \\mathbf{0}\\} = \\{\\mathbf{x} \\in \\mathbb{R}^n : A\\mathbf{x} = \\mathbf{0}\\}\\] In plain English: the kernel is everything that \\(\\Phi\\) \"kills\" (sends to zero). How to compute: Solve the homogeneous system \\(A\\mathbf{x} = \\mathbf{0}\\) by row reducing \\(A\\) and expressing the free variables. The kernel is always a subspace of \\(V\\) .","title":"9.3 Kernel (Null Space)"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#94-image-column-space","text":"The image (or range, or column space) of a linear mapping \\(\\Phi\\) is the set of all possible outputs: \\[\\text{Im}(\\Phi) = \\{\\Phi(\\mathbf{x}) : \\mathbf{x} \\in V\\} = \\{A\\mathbf{x} : \\mathbf{x} \\in \\mathbb{R}^n\\}\\] This is the same as the column space of \\(A\\) -- the span of the columns of \\(A\\) : \\[\\text{Im}(A) = \\text{span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\}\\] where \\(\\mathbf{a}_j\\) are the columns of \\(A\\) . The image is always a subspace of \\(W\\) .","title":"9.4 Image (Column Space)"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#95-injective-surjective-bijective","text":"Property Meaning Condition Injective (one-to-one) Different inputs give different outputs \\(\\ker(\\Phi) = \\{\\mathbf{0}\\}\\) Surjective (onto) Every element of \\(W\\) is an output \\(\\text{Im}(\\Phi) = W\\) Bijective Both injective and surjective The mapping has an inverse Think of it as... Injective means no two inputs \"collide\" at the same output. Surjective means every target is \"hit\" by some input. Bijective means there is a perfect one-to-one pairing between inputs and outputs.","title":"9.5 Injective, Surjective, Bijective"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#96-worked-example-finding-the-kernel-and-image","text":"Let \\(A = \\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\) . Kernel: Solve \\(A\\mathbf{x} = \\mathbf{0}\\) : \\[\\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\] From the matrix (already in RREF): \\(x_1 + 2x_2 = 0\\) and \\(x_3 = 0\\) . Variable \\(x_2\\) is free. Set \\(x_2 = t\\) : \\[\\mathbf{x} = \\begin{bmatrix} -2t \\\\ t \\\\ 0 \\end{bmatrix} = t\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}\\] \\[\\ker(A) = \\text{span}\\left\\{\\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}\\right\\}\\] Dimension of kernel: \\(\\dim(\\ker(A)) = 1\\) . Image: The image is the column space of \\(A\\) . Since \\(A\\) has 2 pivots (in columns 1 and 3), the pivot columns form a basis for the image: \\[\\text{Im}(A) = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right\\} = \\mathbb{R}^2\\] Dimension of image: \\(\\dim(\\text{Im}(A)) = 2\\) .","title":"9.6 Worked Example: Finding the Kernel and Image"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-10-the-rank-nullity-theorem","text":"","title":"Part 10: The Rank-Nullity Theorem"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#101-statement","text":"For a linear mapping \\(\\Phi: V \\to W\\) with transformation matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) : \\[\\dim(\\ker(\\Phi)) + \\dim(\\text{Im}(\\Phi)) = \\dim(V)\\] Or equivalently, using the matrix \\(A\\) : \\[\\text{nullity}(A) + \\text{rank}(A) = n\\] where \\(n\\) is the number of columns of \\(A\\) (the dimension of the domain). In plain English: the number of dimensions that get \"killed\" (mapped to zero) plus the number of dimensions that \"survive\" (form the output) always add up to the total number of input dimensions. Think of it as... you start with \\(n\\) dimensions of freedom. Some get collapsed to zero (the nullity), and the rest get mapped to actual outputs (the rank). Nothing is lost or created -- everything is accounted for.","title":"10.1 Statement"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#102-example","text":"From the worked example in Part 9: \\(A \\in \\mathbb{R}^{2 \\times 3}\\) , so \\(n = 3\\) \\(\\dim(\\ker(A)) = 1\\) (the nullity) \\(\\dim(\\text{Im}(A)) = 2\\) (the rank) Check: \\(1 + 2 = 3 = n\\) \\(\\checkmark\\)","title":"10.2 Example"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#103-using-the-theorem","text":"The Rank-Nullity Theorem is useful for: If you know... You can deduce... Rank and \\(n\\) Nullity = \\(n - \\text{rank}\\) Nullity and \\(n\\) Rank = \\(n - \\text{nullity}\\) Rank \\(= n\\) Nullity = 0, so the mapping is injective Rank \\(= m\\) (rows) The mapping is surjective","title":"10.3 Using the Theorem"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-11-change-of-basis","text":"","title":"Part 11: Change of Basis"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#111-coordinate-vectors","text":"Every vector \\(\\mathbf{x}\\) in a vector space \\(V\\) can be represented as a linear combination of basis vectors. The coordinate vector of \\(\\mathbf{x}\\) with respect to a basis \\(B = \\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\) is the vector of coefficients: If \\(\\mathbf{x} = \\alpha_1 \\mathbf{b}_1 + \\alpha_2 \\mathbf{b}_2 + \\cdots + \\alpha_n \\mathbf{b}_n\\) , then the coordinate vector is: \\[[\\mathbf{x}]_B = \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\vdots \\\\ \\alpha_n \\end{bmatrix}\\] In plain English: the same vector can look different depending on which coordinate system (basis) you use. Think of it as... describing a location. You can say \"3 blocks east and 2 blocks north\" or \"3.6 blocks northeast.\" Same point, different descriptions depending on your coordinate system.","title":"11.1 Coordinate Vectors"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#112-the-change-of-basis-matrix","text":"Suppose you have two bases \\(B = \\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\) and \\(\\tilde{B} = \\{\\tilde{\\mathbf{b}}_1, \\ldots, \\tilde{\\mathbf{b}}_n\\}\\) for the same vector space. The change of basis matrix \\(S\\) from \\(\\tilde{B}\\) to \\(B\\) satisfies: \\[[\\mathbf{x}]_B = S \\, [\\mathbf{x}]_{\\tilde{B}}\\] To construct \\(S\\) : write each new basis vector \\(\\tilde{\\mathbf{b}}_j\\) in terms of the old basis \\(B\\) . The coefficients form the \\(j\\) -th column of \\(S\\) .","title":"11.2 The Change of Basis Matrix"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#113-transformation-under-a-change-of-basis","text":"If a linear mapping \\(\\Phi\\) has transformation matrix \\(A\\) with respect to basis \\(B\\) , then with respect to basis \\(\\tilde{B}\\) , it has transformation matrix: \\[\\tilde{A} = S^{-1} A S\\] where \\(S\\) is the change of basis matrix from \\(\\tilde{B}\\) to \\(B\\) . Worked Example: Let \\(B = \\left\\{\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\right\\}\\) (the standard basis) and \\(\\tilde{B} = \\left\\{\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\right\\}\\) . The change of basis matrix \\(S\\) from \\(\\tilde{B}\\) to \\(B\\) is formed by writing the new basis vectors in terms of the standard basis (which is trivial since \\(B\\) is the standard basis): \\[S = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}\\] Now if \\(\\mathbf{x}\\) has coordinates \\([\\mathbf{x}]_{\\tilde{B}} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}\\) in the new basis, the standard coordinates are: \\[[\\mathbf{x}]_B = S \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}\\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix}\\] This means \\(\\mathbf{x} = 3\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + 2\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix}\\) .","title":"11.3 Transformation Under a Change of Basis"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#part-12-affine-spaces","text":"","title":"Part 12: Affine Spaces"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#121-affine-subspaces-vs-vector-subspaces","text":"A vector subspace must pass through the origin. An affine subspace is a \"shifted\" version of a vector subspace -- it may or may not pass through the origin. Formal definition: An affine subspace \\(L\\) of a vector space \\(V\\) is a set of the form: \\[L = \\mathbf{x}_0 + U = \\{\\mathbf{x}_0 + \\mathbf{u} : \\mathbf{u} \\in U\\}\\] where \\(\\mathbf{x}_0 \\in V\\) is a fixed point and \\(U \\subseteq V\\) is a vector subspace. In plain English: take a subspace \\(U\\) (which passes through the origin) and slide it so it passes through the point \\(\\mathbf{x}_0\\) instead. The result is an affine subspace. Think of it as... a vector subspace is a flat surface passing through the origin (a line through zero, a plane through zero, etc.). An affine subspace is that same flat surface slid to a different location. A line that does not pass through the origin is affine but not a vector subspace.","title":"12.1 Affine Subspaces vs. Vector Subspaces"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#122-examples","text":"Geometric object Vector subspace? Affine subspace? A line through the origin Yes Yes (with \\(\\mathbf{x}_0 = \\mathbf{0}\\) ) A line NOT through the origin No Yes A plane through the origin Yes Yes A plane NOT through the origin No Yes A single point No (unless it is the origin) Yes (a 0-dimensional affine subspace)","title":"12.2 Examples"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#123-connection-to-solutions-of-amathbfx-mathbfb","text":"This is the key insight: the solution set of \\(A\\mathbf{x} = \\mathbf{b}\\) (when \\(\\mathbf{b} \\neq \\mathbf{0}\\) ) is an affine subspace, not a vector subspace. Here is why. Suppose \\(\\mathbf{x}_p\\) is one particular solution to \\(A\\mathbf{x} = \\mathbf{b}\\) . Then every solution can be written as: \\[\\mathbf{x} = \\mathbf{x}_p + \\mathbf{v}, \\quad \\text{where } \\mathbf{v} \\in \\ker(A)\\] In other words, the full solution set is: \\[\\{\\mathbf{x}_p + \\mathbf{v} : \\mathbf{v} \\in \\ker(A)\\} = \\mathbf{x}_p + \\ker(A)\\] This is exactly an affine subspace: a particular solution \\(\\mathbf{x}_p\\) plus the kernel (null space) of \\(A\\) . Why this works: If \\(A\\mathbf{x}_p = \\mathbf{b}\\) and \\(A\\mathbf{v} = \\mathbf{0}\\) , then \\(A(\\mathbf{x}_p + \\mathbf{v}) = A\\mathbf{x}_p + A\\mathbf{v} = \\mathbf{b} + \\mathbf{0} = \\mathbf{b}\\) \\(\\checkmark\\) Worked Example: Solve \\(A\\mathbf{x} = \\mathbf{b}\\) where \\(A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}\\) and \\(\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\\) . Step 1: Find a particular solution \\(\\mathbf{x}_p\\) . The augmented matrix is \\(\\left[\\begin{array}{cc|c} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{array}\\right]\\) . \\(R_2 \\to R_2 - 2R_1\\) : \\(\\left[\\begin{array}{cc|c} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{array}\\right]\\) So \\(x_1 + 2x_2 = 3\\) . Setting \\(x_2 = 0\\) gives \\(x_1 = 3\\) . A particular solution is \\(\\mathbf{x}_p = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\\) . Step 2: Find the kernel \\(\\ker(A)\\) . Solve \\(A\\mathbf{x} = \\mathbf{0}\\) : \\(x_1 + 2x_2 = 0\\) , so \\(x_1 = -2x_2\\) . Setting \\(x_2 = t\\) : \\[\\ker(A) = \\text{span}\\left\\{\\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\right\\}\\] Step 3: Write the full solution as an affine subspace. \\[\\mathbf{x} = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} + t\\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}, \\quad t \\in \\mathbb{R}\\] This is a line in \\(\\mathbb{R}^2\\) that passes through \\(\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\\) with direction \\(\\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\) . It does not pass through the origin, so it is affine but not a vector subspace.","title":"12.3 Connection to Solutions of \\(A\\mathbf{x} = \\mathbf{b}\\)"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#summary-key-takeaways","text":"","title":"Summary: Key Takeaways"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#systems-and-matrices","text":"A system \\(A\\mathbf{x} = \\mathbf{b}\\) has 0, 1, or infinitely many solutions The augmented matrix \\([A \\mid \\mathbf{b}]\\) is the starting point for Gaussian elimination The inverse \\(A^{-1}\\) exists when \\(\\det(A) \\neq 0\\) ; for \\(2 \\times 2\\) : \\(A^{-1} = \\frac{1}{ad-bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\)","title":"Systems and Matrices"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#vector-spaces-and-subspaces","text":"A vector space satisfies closure under addition and scalar multiplication Subspace test: contains \\(\\mathbf{0}\\) , closed under \\(+\\) , closed under scalar multiplication Basis = linearly independent + spanning; dimension = number of basis vectors","title":"Vector Spaces and Subspaces"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#rank-kernel-and-image","text":"\\(\\text{rank}(A) =\\) number of pivots in REF \\(\\ker(A) = \\{\\mathbf{x} : A\\mathbf{x} = \\mathbf{0}\\}\\) ; $\\text{Im}(A) = $ column space of \\(A\\) Rank-Nullity Theorem: \\(\\text{nullity}(A) + \\text{rank}(A) = n\\)","title":"Rank, Kernel, and Image"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#linear-mappings-and-affine-spaces","text":"Linear mappings preserve addition and scalar multiplication Change of basis: \\(\\tilde{A} = S^{-1}AS\\) Solution set of \\(A\\mathbf{x} = \\mathbf{b}\\) is an affine subspace: \\(\\mathbf{x}_p + \\ker(A)\\)","title":"Linear Mappings and Affine Spaces"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#practice-problems","text":"","title":"Practice Problems"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#problem-1-solving-a-system-via-gaussian-elimination","text":"Solve the following system using Gaussian elimination: \\[x_1 + x_2 + 2x_3 = 9\\] \\[2x_1 + 4x_2 - 3x_3 = 1\\] \\[3x_1 + 6x_2 - 5x_3 = 0\\]","title":"Problem 1: Solving a System via Gaussian Elimination"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#problem-2-finding-the-inverse-of-a-matrix","text":"Find the inverse of: \\[A = \\begin{bmatrix} 3 & 5 \\\\ 1 & 2 \\end{bmatrix}\\] Verify your answer by checking that \\(AA^{-1} = I\\) .","title":"Problem 2: Finding the Inverse of a Matrix"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#problem-3-checking-if-a-set-is-a-subspace","text":"Determine whether the following subset of \\(\\mathbb{R}^3\\) is a subspace: \\[U = \\left\\{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\in \\mathbb{R}^3 : x_1 + 2x_2 - x_3 = 0\\right\\}\\]","title":"Problem 3: Checking if a Set is a Subspace"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#problem-4-testing-for-linear-independence","text":"Determine whether the following vectors are linearly independent: \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}, \\quad \\mathbf{v}_3 = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}\\]","title":"Problem 4: Testing for Linear Independence"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#problem-5-finding-a-basis-and-dimension","text":"Find a basis for the column space of the following matrix and state its dimension: \\[A = \\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 6 & 10 \\\\ 1 & 1 & 3 \\end{bmatrix}\\]","title":"Problem 5: Finding a Basis and Dimension"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#problem-6-computing-the-rank","text":"Compute the rank of: \\[M = \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 2 & 4 & 3 & 1 \\\\ 3 & 6 & 4 & 1 \\end{bmatrix}\\]","title":"Problem 6: Computing the Rank"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#problem-7-kernel-and-image","text":"For the matrix \\(A = \\begin{bmatrix} 1 & 0 & -1 \\\\ 2 & 1 & 0 \\\\ -1 & 1 & 2 \\end{bmatrix}\\) : (a) Find the kernel of \\(A\\) . (b) Find a basis for the image of \\(A\\) . (c) Verify the Rank-Nullity Theorem.","title":"Problem 7: Kernel and Image"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#problem-8-affine-space-parametric-form-of-solution-set","text":"Find the general solution (in affine/parametric form) of: \\[\\begin{bmatrix} 1 & 1 & 2 \\\\ 2 & 2 & 4 \\end{bmatrix}\\mathbf{x} = \\begin{bmatrix} 4 \\\\ 8 \\end{bmatrix}\\] Identify the particular solution, the kernel, and explain why the solution set is an affine subspace.","title":"Problem 8: Affine Space -- Parametric Form of Solution Set"},{"location":"tutorials/Tutorial_02_Linear_Algebra/#solutions","text":"Solution 1: Write the augmented matrix: \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 9 \\\\ 2 & 4 & -3 & 1 \\\\ 3 & 6 & -5 & 0 \\end{array}\\right]\\] \\(R_2 \\to R_2 - 2R_1\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 9 \\\\ 0 & 2 & -7 & -17 \\\\ 3 & 6 & -5 & 0 \\end{array}\\right]\\] \\(R_3 \\to R_3 - 3R_1\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 9 \\\\ 0 & 2 & -7 & -17 \\\\ 0 & 3 & -11 & -27 \\end{array}\\right]\\] \\(R_3 \\to R_3 - \\frac{3}{2}R_2\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 9 \\\\ 0 & 2 & -7 & -17 \\\\ 0 & 0 & -\\frac{1}{2} & -\\frac{3}{2} \\end{array}\\right]\\] \\(R_3 \\to -2R_3\\) : \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 9 \\\\ 0 & 2 & -7 & -17 \\\\ 0 & 0 & 1 & 3 \\end{array}\\right]\\] Back substitution: From row 3: \\(x_3 = 3\\) . From row 2: \\(2x_2 - 7(3) = -17 \\implies 2x_2 = 4 \\implies x_2 = 2\\) . From row 1: \\(x_1 + 2 + 2(3) = 9 \\implies x_1 = 9 - 2 - 6 = 1\\) . \\[x_1 = 1, \\quad x_2 = 2, \\quad x_3 = 3\\] Verify: \\(1+2+6=9\\) \\(\\checkmark\\) , \\(2+8-9=1\\) \\(\\checkmark\\) , \\(3+12-15=0\\) \\(\\checkmark\\) . Solution 2: \\[\\det(A) = 3(2) - 5(1) = 6 - 5 = 1\\] \\[A^{-1} = \\frac{1}{1}\\begin{bmatrix} 2 & -5 \\\\ -1 & 3 \\end{bmatrix} = \\begin{bmatrix} 2 & -5 \\\\ -1 & 3 \\end{bmatrix}\\] Verify: \\[AA^{-1} = \\begin{bmatrix} 3 & 5 \\\\ 1 & 2 \\end{bmatrix}\\begin{bmatrix} 2 & -5 \\\\ -1 & 3 \\end{bmatrix} = \\begin{bmatrix} 3(2)+5(-1) & 3(-5)+5(3) \\\\ 1(2)+2(-1) & 1(-5)+2(3) \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I \\quad \\checkmark\\] Solution 3: We apply the 3-step subspace test. Step 1 (Zero vector): Let \\(x_1 = x_2 = x_3 = 0\\) . Then \\(0 + 2(0) - 0 = 0\\) . So \\(\\mathbf{0} \\in U\\) . \\(\\checkmark\\) Step 2 (Closed under addition): Let \\(\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}\\) both be in \\(U\\) , so \\(u_1 + 2u_2 - u_3 = 0\\) and \\(v_1 + 2v_2 - v_3 = 0\\) . Then \\(\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1+v_1 \\\\ u_2+v_2 \\\\ u_3+v_3 \\end{bmatrix}\\) and: \\[(u_1+v_1) + 2(u_2+v_2) - (u_3+v_3) = (u_1+2u_2-u_3) + (v_1+2v_2-v_3) = 0 + 0 = 0\\] So \\(\\mathbf{u} + \\mathbf{v} \\in U\\) . \\(\\checkmark\\) Step 3 (Closed under scalar multiplication): Let \\(\\alpha \\in \\mathbb{R}\\) and \\(\\mathbf{u} \\in U\\) . \\[\\alpha u_1 + 2(\\alpha u_2) - \\alpha u_3 = \\alpha(u_1 + 2u_2 - u_3) = \\alpha \\cdot 0 = 0\\] So \\(\\alpha \\mathbf{u} \\in U\\) . \\(\\checkmark\\) Conclusion: \\(U\\) is a subspace of \\(\\mathbb{R}^3\\) . (Geometrically, it is a plane through the origin.) Solution 4: Form the matrix and row reduce: \\[\\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 1 \\\\ 2 & -1 & 3 \\end{bmatrix} \\xrightarrow{R_3 - 2R_1} \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 1 \\\\ 0 & -1 & -1 \\end{bmatrix} \\xrightarrow{R_3 + R_2} \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}\\] There are 2 pivots for 3 vectors. Column 3 has no pivot, meaning \\(\\mathbf{v}_3\\) depends on the others. The vectors are linearly dependent . From the RREF: \\(\\mathbf{v}_3 = 2\\mathbf{v}_1 + 1\\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}\\) . \\(\\checkmark\\) Solution 5: Row reduce \\(A\\) : \\[\\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 6 & 10 \\\\ 1 & 1 & 3 \\end{bmatrix} \\xrightarrow{R_2 - 2R_1} \\begin{bmatrix} 1 & 3 & 5 \\\\ 0 & 0 & 0 \\\\ 1 & 1 & 3 \\end{bmatrix} \\xrightarrow{R_3 - R_1} \\begin{bmatrix} 1 & 3 & 5 \\\\ 0 & 0 & 0 \\\\ 0 & -2 & -2 \\end{bmatrix} \\xrightarrow{R_2 \\leftrightarrow R_3} \\begin{bmatrix} 1 & 3 & 5 \\\\ 0 & -2 & -2 \\\\ 0 & 0 & 0 \\end{bmatrix}\\] Pivots are in columns 1 and 2. A basis for the column space consists of the corresponding original columns of \\(A\\) : \\[\\text{Basis} = \\left\\{\\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 3 \\\\ 6 \\\\ 1 \\end{bmatrix}\\right\\}\\] Dimension: \\(\\dim(\\text{Col}(A)) = \\text{rank}(A) = 2\\) . Solution 6: Row reduce \\(M\\) : \\[\\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 2 & 4 & 3 & 1 \\\\ 3 & 6 & 4 & 1 \\end{bmatrix} \\xrightarrow{R_2 - 2R_1} \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 3 & 6 & 4 & 1 \\end{bmatrix} \\xrightarrow{R_3 - 3R_1} \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix} \\xrightarrow{R_3 - R_2} \\begin{bmatrix} 1 & 2 & 1 & 0 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\] There are 2 pivots (in columns 1 and 3). \\[\\text{rank}(M) = 2\\] Solution 7: (a) Kernel: Solve \\(A\\mathbf{x} = \\mathbf{0}\\) . \\[\\begin{bmatrix} 1 & 0 & -1 \\\\ 2 & 1 & 0 \\\\ -1 & 1 & 2 \\end{bmatrix} \\xrightarrow{R_2 - 2R_1} \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ -1 & 1 & 2 \\end{bmatrix} \\xrightarrow{R_3 + R_1} \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 1 & 1 \\end{bmatrix} \\xrightarrow{R_3 - R_2} \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & -1 \\end{bmatrix}\\] Three pivots for three unknowns, so the only solution to \\(A\\mathbf{x} = \\mathbf{0}\\) is \\(\\mathbf{x} = \\mathbf{0}\\) . \\[\\ker(A) = \\{\\mathbf{0}\\}\\] (b) Image: Since \\(\\text{rank}(A) = 3\\) and \\(A \\in \\mathbb{R}^{3 \\times 3}\\) , all three columns are linearly independent. A basis for the image is: \\[\\text{Basis for Im}(A) = \\left\\{\\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} -1 \\\\ 0 \\\\ 2 \\end{bmatrix}\\right\\}\\] So \\(\\text{Im}(A) = \\mathbb{R}^3\\) (the mapping is surjective). (c) Verify Rank-Nullity: \\[\\dim(\\ker(A)) + \\dim(\\text{Im}(A)) = 0 + 3 = 3 = n \\quad \\checkmark\\] Since \\(\\ker(A) = \\{\\mathbf{0}\\}\\) (injective) and \\(\\text{Im}(A) = \\mathbb{R}^3\\) (surjective), the mapping is bijective and \\(A\\) is invertible. Solution 8: Write the augmented matrix and row reduce: \\[\\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 4 \\\\ 2 & 2 & 4 & 8 \\end{array}\\right] \\xrightarrow{R_2 - 2R_1} \\left[\\begin{array}{ccc|c} 1 & 1 & 2 & 4 \\\\ 0 & 0 & 0 & 0 \\end{array}\\right]\\] There is 1 pivot (in column 1) and 2 free variables ( \\(x_2\\) and \\(x_3\\) ). From the first row: \\(x_1 + x_2 + 2x_3 = 4\\) , so \\(x_1 = 4 - x_2 - 2x_3\\) . Particular solution: Set the free variables to zero ( \\(x_2 = 0, x_3 = 0\\) ): \\[\\mathbf{x}_p = \\begin{bmatrix} 4 \\\\ 0 \\\\ 0 \\end{bmatrix}\\] Kernel: Solve \\(A\\mathbf{x} = \\mathbf{0}\\) : \\(x_1 = -x_2 - 2x_3\\) , with \\(x_2 = s, x_3 = t\\) free: \\[\\ker(A) = \\text{span}\\left\\{\\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\end{bmatrix}\\right\\}\\] General solution (affine/parametric form): \\[\\mathbf{x} = \\underbrace{\\begin{bmatrix} 4 \\\\ 0 \\\\ 0 \\end{bmatrix}}_{\\mathbf{x}_p} + s\\underbrace{\\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix}}_{\\text{kernel direction 1}} + t\\underbrace{\\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\end{bmatrix}}_{\\text{kernel direction 2}}, \\quad s, t \\in \\mathbb{R}\\] Why is this an affine subspace? The solution set does not pass through the origin (when \\(s = t = 0\\) , the solution is \\(\\begin{bmatrix} 4 \\\\ 0 \\\\ 0 \\end{bmatrix} \\neq \\mathbf{0}\\) ). It is formed by taking one particular solution \\(\\mathbf{x}_p\\) and adding all elements of the kernel, which is a 2-dimensional vector subspace. The result is a 2-dimensional plane in \\(\\mathbb{R}^3\\) , shifted away from the origin -- a classic affine subspace. Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Next: Tutorial - Analytic Geometry","title":"Solutions"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/","text":"Tutorial 2: Analytic Geometry \u00b6 Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari \ud83d\udcda Learning Objectives \u00b6 By the end of this tutorial, you will understand: Norms and their role in measuring vector magnitude Inner products and their defining axioms How lengths, distances, angles, and orthogonality arise from inner products Orthogonal matrices, orthonormal bases, and orthogonal complements Orthogonal projections and the Gram-Schmidt process Rotation matrices and their geometric meaning Part 1: Norms \u00b6 1.1 What is a Norm? \u00b6 A norm is a function \\(\\|\\cdot\\| : \\mathbb{R}^n \\to \\mathbb{R}\\) that assigns a non-negative \"length\" to every vector. Think of it as... a ruler for vectors. Different norms are like different ways of measuring distance \u2014 walking along city blocks versus flying in a straight line. A norm must satisfy these properties for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) and all \\(\\lambda \\in \\mathbb{R}\\) : Property Statement Intuition Non-negativity \\(\\|\\mathbf{x}\\| \\geq 0\\) Lengths are never negative Definiteness \\(\\|\\mathbf{x}\\| = 0 \\iff \\mathbf{x} = \\mathbf{0}\\) Only the zero vector has zero length Absolute homogeneity $|\\lambda \\mathbf{x}| = \\lambda Triangle inequality \\(\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|\\) The shortcut is never longer than going around 1.2 Common Norms \u00b6 Norm Name Formula Also Called \\(\\ell_1\\) Manhattan norm \\(\\|\\mathbf{x}\\|_1 = \\displaystyle\\sum_{i=1}^{n} \\|x_i\\|\\) Taxicab norm \\(\\ell_2\\) Euclidean norm \\(\\|\\mathbf{x}\\|_2 = \\sqrt{\\displaystyle\\sum_{i=1}^{n} x_i^2}\\) Standard norm \\(\\ell_\\infty\\) Max norm $|\\mathbf{x}| \\infty = \\max x_i 1.3 Worked Example: Computing Norms \u00b6 Let \\(\\mathbf{x} = \\begin{bmatrix} 3 \\\\ -4 \\\\ 2 \\end{bmatrix}\\) . \\(\\ell_1\\) norm: $ \\(\\|\\mathbf{x}\\|_1 = |3| + |-4| + |2| = 3 + 4 + 2 = 9\\) $ \\(\\ell_2\\) norm: $ \\(\\|\\mathbf{x}\\|_2 = \\sqrt{3^2 + (-4)^2 + 2^2} = \\sqrt{9 + 16 + 4} = \\sqrt{29} \\approx 5.39\\) $ \\(\\ell_\\infty\\) norm: $ \\(\\|\\mathbf{x}\\|_\\infty = \\max\\{|3|, |-4|, |2|\\} = 4\\) $ Think of it as... The \\(\\ell_1\\) norm counts total blocks walked in a grid city. The \\(\\ell_2\\) norm is the straight-line (as the crow flies) distance. The \\(\\ell_\\infty\\) norm is the longest single step you take along any one axis. Part 2: Inner Products \u00b6 2.1 Definition \u00b6 An inner product on a vector space \\(V\\) is a function \\(\\langle \\cdot, \\cdot \\rangle : V \\times V \\to \\mathbb{R}\\) that satisfies four axioms: Axiom Statement For all Symmetry \\(\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\langle \\mathbf{y}, \\mathbf{x} \\rangle\\) \\(\\mathbf{x}, \\mathbf{y} \\in V\\) Linearity in 1st argument \\(\\langle \\lambda\\mathbf{x} + \\mathbf{z}, \\mathbf{y} \\rangle = \\lambda\\langle \\mathbf{x}, \\mathbf{y} \\rangle + \\langle \\mathbf{z}, \\mathbf{y} \\rangle\\) \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in V,\\ \\lambda \\in \\mathbb{R}\\) Positive semi-definiteness \\(\\langle \\mathbf{x}, \\mathbf{x} \\rangle \\geq 0\\) \\(\\mathbf{x} \\in V\\) Positive definiteness \\(\\langle \\mathbf{x}, \\mathbf{x} \\rangle = 0 \\iff \\mathbf{x} = \\mathbf{0}\\) \\(\\mathbf{x} \\in V\\) Think of it as... an inner product is a generalized way of multiplying two vectors together to get a single number that tells you \"how much\" the vectors agree in direction. 2.2 The Dot Product \u00b6 The most common inner product in \\(\\mathbb{R}^n\\) is the dot product : \\[\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\mathbf{x}^T \\mathbf{y} = \\sum_{i=1}^{n} x_i y_i\\] Example: $ \\(\\left\\langle \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\begin{bmatrix} 4 \\\\ 0 \\\\ -1 \\end{bmatrix} \\right\\rangle = 1(4) + 2(0) + 3(-1) = 4 + 0 - 3 = 1\\) $ 2.3 General Inner Products and Positive Definite Matrices \u00b6 Not every inner product is the dot product. We can define a more general inner product using a symmetric positive definite matrix \\(A\\) : \\[\\langle \\mathbf{x}, \\mathbf{y} \\rangle_A = \\mathbf{x}^T A \\mathbf{y}\\] A symmetric matrix \\(A\\) is positive definite if: $ \\(\\mathbf{x}^T A \\mathbf{x} > 0 \\quad \\text{for all } \\mathbf{x} \\neq \\mathbf{0}\\) $ Example: Let \\(A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) . \\[\\mathbf{x}^T A \\mathbf{x} = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3 & 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 6 > 0\\] Think of it as... the standard dot product uses the identity matrix \\(I\\) as \\(A\\) . Choosing a different positive definite \\(A\\) stretches or skews the geometry, like measuring distance on a tilted surface instead of a flat table. Part 3: Lengths and Distances \u00b6 3.1 Induced Norm \u00b6 Every inner product induces a norm: \\[\\|\\mathbf{x}\\| = \\sqrt{\\langle \\mathbf{x}, \\mathbf{x} \\rangle}\\] For the standard dot product this gives the Euclidean norm: \\[\\|\\mathbf{x}\\| = \\sqrt{\\mathbf{x}^T \\mathbf{x}} = \\sqrt{\\sum_{i=1}^{n} x_i^2}\\] 3.2 Distance \u00b6 The distance between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is: \\[d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\| = \\sqrt{\\langle \\mathbf{x} - \\mathbf{y}, \\mathbf{x} - \\mathbf{y} \\rangle}\\] A distance function (metric) satisfies: Property Statement Non-negativity \\(d(\\mathbf{x}, \\mathbf{y}) \\geq 0\\) Identity \\(d(\\mathbf{x}, \\mathbf{y}) = 0 \\iff \\mathbf{x} = \\mathbf{y}\\) Symmetry \\(d(\\mathbf{x}, \\mathbf{y}) = d(\\mathbf{y}, \\mathbf{x})\\) Triangle inequality \\(d(\\mathbf{x}, \\mathbf{z}) \\leq d(\\mathbf{x}, \\mathbf{y}) + d(\\mathbf{y}, \\mathbf{z})\\) 3.3 Cauchy-Schwarz Inequality \u00b6 One of the most important inequalities in all of mathematics: \\[|\\langle \\mathbf{x}, \\mathbf{y} \\rangle| \\leq \\|\\mathbf{x}\\| \\cdot \\|\\mathbf{y}\\|\\] Equality holds if and only if \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are linearly dependent (i.e., one is a scalar multiple of the other). Think of it as... the dot product can never exceed the product of the lengths. This is what guarantees that the cosine of the angle between two vectors always stays between \\(-1\\) and \\(1\\) . Example: Let \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \\(\\mathbf{y} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}\\) . \\(|\\langle \\mathbf{x}, \\mathbf{y} \\rangle| = |1(3) + 2(1)| = |5| = 5\\) \\(\\|\\mathbf{x}\\| \\cdot \\|\\mathbf{y}\\| = \\sqrt{1+4}\\,\\sqrt{9+1} = \\sqrt{5}\\,\\sqrt{10} = \\sqrt{50} \\approx 7.07\\) Check: \\(5 \\leq 7.07\\) \u2713 Part 4: Angles and Orthogonality \u00b6 4.1 Angle Between Vectors \u00b6 The angle \\(\\theta\\) between two non-zero vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is defined via: \\[\\cos \\theta = \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\| \\cdot \\|\\mathbf{y}\\|}\\] The Cauchy-Schwarz inequality guarantees that the right-hand side lies in \\([-1, 1]\\) , so \\(\\theta\\) is well-defined. Example: Find the angle between \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\) and \\(\\mathbf{y} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) . \\[\\cos \\theta = \\frac{1(1) + 0(1)}{\\sqrt{1}\\,\\sqrt{2}} = \\frac{1}{\\sqrt{2}} \\implies \\theta = \\frac{\\pi}{4} = 45^\\circ\\] 4.2 Orthogonality \u00b6 Two vectors are orthogonal (perpendicular) if their inner product is zero: \\[\\mathbf{x} \\perp \\mathbf{y} \\iff \\langle \\mathbf{x}, \\mathbf{y} \\rangle = 0\\] Think of it as... orthogonal vectors carry completely independent information \u2014 knowing one tells you nothing about the other. This is exactly the idea behind \"uncorrelated features\" in machine learning. Example: $ \\(\\left\\langle \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\right\\rangle = 1(1) + (-1)(1) = 0 \\quad \\checkmark \\text{ Orthogonal!}\\) $ 4.3 Orthogonal and Orthonormal Sets \u00b6 A set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}\\) is: Term Condition Orthogonal \\(\\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle = 0\\) for all \\(i \\neq j\\) Orthonormal Orthogonal and \\(\\|\\mathbf{v}_i\\| = 1\\) for all \\(i\\) Example of an orthonormal set in \\(\\mathbb{R}^2\\) : $ \\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) $ \\(\\langle \\mathbf{e}_1, \\mathbf{e}_2 \\rangle = 0\\) (orthogonal) \\(\\|\\mathbf{e}_1\\| = 1\\) and \\(\\|\\mathbf{e}_2\\| = 1\\) (unit length) Part 5: Orthogonal Matrices \u00b6 5.1 Definition \u00b6 A square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) is orthogonal if its columns form an orthonormal set. Equivalently: \\[A^T A = I \\implies A^{-1} = A^T\\] Think of it as... an orthogonal matrix performs a \"rigid\" transformation \u2014 it can rotate or reflect vectors but never stretches or squishes them. 5.2 Key Properties \u00b6 Property Statement Inverse equals transpose \\(A^{-1} = A^T\\) Columns are orthonormal \\(\\langle \\mathbf{a}_i, \\mathbf{a}_j \\rangle = \\delta_{ij}\\) Rows are orthonormal \\(A A^T = I\\) Preserves lengths \\(\\|A\\mathbf{x}\\| = \\|\\mathbf{x}\\|\\) Preserves angles \\(\\langle A\\mathbf{x}, A\\mathbf{y} \\rangle = \\langle \\mathbf{x}, \\mathbf{y} \\rangle\\) Determinant \\(\\det(A) = \\pm 1\\) Product is orthogonal If \\(A, B\\) orthogonal, then \\(AB\\) is orthogonal Proof that orthogonal matrices preserve lengths: $ \\(\\|A\\mathbf{x}\\|^2 = (A\\mathbf{x})^T(A\\mathbf{x}) = \\mathbf{x}^T A^T A \\mathbf{x} = \\mathbf{x}^T I \\mathbf{x} = \\mathbf{x}^T \\mathbf{x} = \\|\\mathbf{x}\\|^2\\) $ 5.3 Example \u00b6 \\[A = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\[4pt] \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix}\\] Verify \\(A^T A = I\\) : $ \\(A^T A = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\[4pt] -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\[4pt] \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I \\quad \\checkmark\\) $ Part 6: Orthonormal Basis \u00b6 6.1 Definition \u00b6 An orthonormal basis (ONB) for a subspace \\(U \\subseteq \\mathbb{R}^n\\) is a basis \\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\}\\) such that: \\[\\langle \\mathbf{u}_i, \\mathbf{u}_j \\rangle = \\delta_{ij} = \\begin{cases} 1 & \\text{if } i = j \\\\ 0 & \\text{if } i \\neq j \\end{cases}\\] 6.2 Why Orthonormal Bases are Useful \u00b6 With an orthonormal basis, finding coordinates becomes trivially easy. If \\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\}\\) is an ONB for \\(U\\) and \\(\\mathbf{x} \\in U\\) , then: \\[\\mathbf{x} = \\sum_{i=1}^{k} \\langle \\mathbf{x}, \\mathbf{u}_i \\rangle \\, \\mathbf{u}_i\\] Think of it as... with an orthonormal basis, you find each coordinate by simply taking a dot product \u2014 no system of equations to solve. It is the easiest possible coordinate system. 6.3 How to Find an Orthonormal Basis \u00b6 Given any basis, use the Gram-Schmidt process (covered in Part 9) to convert it into an orthonormal basis. Part 7: Orthogonal Complement \u00b6 7.1 Definition \u00b6 Let \\(U\\) be a subspace of \\(\\mathbb{R}^n\\) . The orthogonal complement \\(U^\\perp\\) is the set of all vectors orthogonal to every vector in \\(U\\) : \\[U^\\perp = \\{\\mathbf{v} \\in \\mathbb{R}^n : \\langle \\mathbf{v}, \\mathbf{u} \\rangle = 0 \\text{ for all } \\mathbf{u} \\in U\\}\\] Think of it as... if \\(U\\) is a plane through the origin in 3D, then \\(U^\\perp\\) is the line perpendicular to that plane. Together they account for all of \\(\\mathbb{R}^3\\) . 7.2 Key Properties \u00b6 Property Statement Subspace \\(U^\\perp\\) is itself a subspace Dimension \\(\\dim(U) + \\dim(U^\\perp) = n\\) Double complement \\((U^\\perp)^\\perp = U\\) Direct sum \\(\\mathbb{R}^n = U \\oplus U^\\perp\\) (every vector splits uniquely) 7.3 Connection to the Kernel and Row Space \u00b6 For a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) : \\[\\ker(A) = \\text{row}(A)^\\perp\\] This means: a vector \\(\\mathbf{x}\\) is in the null space of \\(A\\) if and only if \\(\\mathbf{x}\\) is orthogonal to every row of \\(A\\) . Example: Let \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\end{bmatrix}\\) . The row space is \\(\\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\right\\}\\) (the rows are linearly dependent). The null space is \\(\\ker(A) = \\text{span}\\left\\{\\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\right\\}\\) . Check: \\(\\left\\langle \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix} \\right\\rangle = 1(-2) + 2(1) = 0\\) \u2713 Part 8: Orthogonal Projections \u00b6 8.1 Projection onto a Line \u00b6 Given a non-zero vector \\(\\mathbf{b}\\) (defining a line through the origin), the projection of \\(\\mathbf{x}\\) onto the line spanned by \\(\\mathbf{b}\\) is: \\[\\pi_{\\mathbf{b}}(\\mathbf{x}) = \\frac{\\langle \\mathbf{x}, \\mathbf{b} \\rangle}{\\langle \\mathbf{b}, \\mathbf{b} \\rangle} \\mathbf{b} = \\frac{\\mathbf{b}\\mathbf{b}^T}{\\mathbf{b}^T\\mathbf{b}} \\mathbf{x}\\] The projection matrix is: \\[P_\\pi = \\frac{\\mathbf{b}\\mathbf{b}^T}{\\mathbf{b}^T\\mathbf{b}}\\] Think of it as... shining a flashlight straight down onto a line and seeing where the shadow of your vector lands. The projection is that shadow. Example: Project \\(\\mathbf{x} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}\\) onto \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) . \\[\\pi_{\\mathbf{b}}(\\mathbf{x}) = \\frac{\\mathbf{x}^T\\mathbf{b}}{\\mathbf{b}^T\\mathbf{b}} \\mathbf{b} = \\frac{3(1) + 1(2)}{1^2 + 2^2} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\frac{5}{5} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\] 8.2 Projection onto a General Subspace \u00b6 Let \\(U = \\text{span}\\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\}\\) and define \\(B = [\\mathbf{b}_1 \\mid \\cdots \\mid \\mathbf{b}_k]\\) . The projection of \\(\\mathbf{x}\\) onto \\(U\\) is: \\[\\pi_U(\\mathbf{x}) = B(B^T B)^{-1} B^T \\mathbf{x}\\] The projection matrix is: \\[P = B(B^T B)^{-1} B^T\\] Properties of projection matrices: Property Statement Idempotent \\(P^2 = P\\) (projecting twice is the same as projecting once) Symmetric \\(P^T = P\\) Residual \\(\\mathbf{x} - P\\mathbf{x}\\) is orthogonal to \\(U\\) 8.3 Connection to the Pseudo-Inverse \u00b6 The Moore-Penrose pseudo-inverse of \\(B\\) is: \\[B^\\dagger = (B^T B)^{-1} B^T\\] So the projection simplifies to: \\[\\pi_U(\\mathbf{x}) = B B^\\dagger \\mathbf{x}\\] The pseudo-inverse is central to solving least-squares problems: when \\(A\\mathbf{x} = \\mathbf{b}\\) has no exact solution, the best approximate solution is \\(\\hat{\\mathbf{x}} = A^\\dagger \\mathbf{b}\\) . 8.4 Worked Example: Projection onto a Subspace \u00b6 Project \\(\\mathbf{x} = \\begin{bmatrix} 6 \\\\ 0 \\\\ 0 \\end{bmatrix}\\) onto \\(U = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}\\right\\}\\) . Step 1: Form the matrix \\(B\\) : $ \\(B = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}\\) $ Step 2: Compute \\(B^T B\\) : $ \\(B^T B = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\) $ Step 3: Compute \\((B^T B)^{-1}\\) : $ \\((B^T B)^{-1} = \\frac{1}{2(2) - 1(1)} \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix} = \\frac{1}{3} \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}\\) $ Step 4: Compute \\(B^T \\mathbf{x}\\) : $ \\(B^T \\mathbf{x} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 6 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix}\\) $ Step 5: Compute \\((B^T B)^{-1} B^T \\mathbf{x}\\) : $ \\(\\frac{1}{3}\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}\\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix} 12 \\\\ -6 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ -2 \\end{bmatrix}\\) $ Step 6: Compute the projection: $ \\(\\pi_U(\\mathbf{x}) = B \\begin{bmatrix} 4 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}\\begin{bmatrix} 4 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ -2 \\\\ 2 \\end{bmatrix}\\) $ Verify: The residual \\(\\mathbf{x} - \\pi_U(\\mathbf{x}) = \\begin{bmatrix} 2 \\\\ 2 \\\\ -2 \\end{bmatrix}\\) should be orthogonal to both basis vectors: \\(\\langle \\begin{bmatrix} 2 \\\\ 2 \\\\ -2 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} \\rangle = 2 + 0 - 2 = 0\\) \u2713 \\(\\langle \\begin{bmatrix} 2 \\\\ 2 \\\\ -2 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} \\rangle = 0 + 2 - 2 = 0\\) \u2713 Part 9: Gram-Schmidt Process \u00b6 9.1 The Algorithm \u00b6 The Gram-Schmidt process takes any set of linearly independent vectors and produces an orthonormal set spanning the same subspace. Given linearly independent vectors \\(\\{\\mathbf{b}_1, \\mathbf{b}_2, \\ldots, \\mathbf{b}_k\\}\\) : Step 1: Orthogonalize (produce orthogonal vectors \\(\\mathbf{u}_i\\) ) \\[\\mathbf{u}_1 = \\mathbf{b}_1\\] \\[\\mathbf{u}_2 = \\mathbf{b}_2 - \\frac{\\langle \\mathbf{b}_2, \\mathbf{u}_1 \\rangle}{\\langle \\mathbf{u}_1, \\mathbf{u}_1 \\rangle} \\mathbf{u}_1\\] \\[\\mathbf{u}_3 = \\mathbf{b}_3 - \\frac{\\langle \\mathbf{b}_3, \\mathbf{u}_1 \\rangle}{\\langle \\mathbf{u}_1, \\mathbf{u}_1 \\rangle} \\mathbf{u}_1 - \\frac{\\langle \\mathbf{b}_3, \\mathbf{u}_2 \\rangle}{\\langle \\mathbf{u}_2, \\mathbf{u}_2 \\rangle} \\mathbf{u}_2\\] In general: $ \\(\\mathbf{u}_i = \\mathbf{b}_i - \\sum_{j=1}^{i-1} \\frac{\\langle \\mathbf{b}_i, \\mathbf{u}_j \\rangle}{\\langle \\mathbf{u}_j, \\mathbf{u}_j \\rangle} \\mathbf{u}_j\\) $ Step 2: Normalize (produce unit vectors \\(\\mathbf{e}_i\\) ) \\[\\mathbf{e}_i = \\frac{\\mathbf{u}_i}{\\|\\mathbf{u}_i\\|}\\] Think of it as... taking each new vector and \"subtracting off\" all the parts that point in the directions you have already handled. What remains is the genuinely new direction. Then you scale it to length 1. 9.2 Worked Example \u00b6 Apply Gram-Schmidt to \\(\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\) and \\(\\mathbf{b}_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\) . Step 1: Set \\(\\mathbf{u}_1 = \\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\) . Step 2: Compute the projection coefficient: $ \\(\\frac{\\langle \\mathbf{b}_2, \\mathbf{u}_1 \\rangle}{\\langle \\mathbf{u}_1, \\mathbf{u}_1 \\rangle} = \\frac{1(1) + 0(1) + 1(0)}{1^2 + 1^2 + 0^2} = \\frac{1}{2}\\) $ Step 3: Subtract the projection: $ \\(\\mathbf{u}_2 = \\mathbf{b}_2 - \\frac{1}{2}\\mathbf{u}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} - \\frac{1}{2}\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ -1/2 \\\\ 1 \\end{bmatrix}\\) $ Verify orthogonality: $ \\(\\langle \\mathbf{u}_1, \\mathbf{u}_2 \\rangle = 1\\!\\left(\\tfrac{1}{2}\\right) + 1\\!\\left(-\\tfrac{1}{2}\\right) + 0(1) = 0 \\quad \\checkmark\\) $ Step 4: Normalize: $ \\(\\|\\mathbf{u}_1\\| = \\sqrt{1+1+0} = \\sqrt{2}, \\quad \\mathbf{e}_1 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\) $ \\[\\|\\mathbf{u}_2\\| = \\sqrt{\\tfrac{1}{4}+\\tfrac{1}{4}+1} = \\sqrt{\\tfrac{3}{2}} = \\frac{\\sqrt{6}}{2}, \\quad \\mathbf{e}_2 = \\frac{2}{\\sqrt{6}}\\begin{bmatrix} 1/2 \\\\ -1/2 \\\\ 1 \\end{bmatrix} = \\frac{1}{\\sqrt{6}}\\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}\\] Result: The orthonormal basis is: $ \\(\\mathbf{e}_1 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\qquad \\mathbf{e}_2 = \\frac{1}{\\sqrt{6}}\\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}\\) $ Part 10: Rotations \u00b6 10.1 Rotation Matrix in 2D \u00b6 A rotation by angle \\(\\theta\\) (counter-clockwise) in \\(\\mathbb{R}^2\\) is given by: \\[R(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}\\] Think of it as... every point in the plane is swung around the origin by the angle \\(\\theta\\) . The matrix encodes where the two standard basis vectors land after the rotation. 10.2 Properties of Rotation Matrices \u00b6 Property Statement Orthogonal \\(R(\\theta)^T R(\\theta) = I\\) Determinant \\(\\det(R(\\theta)) = 1\\) (no reflection) Inverse is reverse rotation \\(R(\\theta)^{-1} = R(-\\theta) = R(\\theta)^T\\) Composition \\(R(\\alpha) R(\\beta) = R(\\alpha + \\beta)\\) Preserves lengths \\(\\|R(\\theta)\\mathbf{x}\\| = \\|\\mathbf{x}\\|\\) Preserves angles Angles between vectors are unchanged 10.3 Worked Example \u00b6 Rotate \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\) by \\(\\theta = 90^\\circ\\) . \\[R(90^\\circ) = \\begin{bmatrix} \\cos 90^\\circ & -\\sin 90^\\circ \\\\ \\sin 90^\\circ & \\cos 90^\\circ \\end{bmatrix} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\] \\[R(90^\\circ)\\mathbf{x} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\] This is exactly the unit vector pointing straight up \u2014 a \\(90^\\circ\\) counter-clockwise rotation of the unit vector pointing right. \u2713 10.4 Rotations in 3D (Preview) \u00b6 In \\(\\mathbb{R}^3\\) , a rotation about the \\(z\\) -axis by angle \\(\\theta\\) is: \\[R_z(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta & 0 \\\\ \\sin\\theta & \\cos\\theta & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\] General 3D rotations can be composed from rotations about the three coordinate axes. Summary: Key Takeaways \u00b6 Norms and Inner Products \u00b6 The \\(\\ell_1\\) , \\(\\ell_2\\) , and \\(\\ell_\\infty\\) norms each measure vector size differently An inner product \\(\\langle \\cdot, \\cdot \\rangle\\) must satisfy symmetry, linearity, and positive definiteness Every inner product induces a norm: \\(\\|\\mathbf{x}\\| = \\sqrt{\\langle \\mathbf{x}, \\mathbf{x} \\rangle}\\) Geometry from Inner Products \u00b6 Angles: \\(\\cos\\theta = \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}\\) Orthogonality: \\(\\langle \\mathbf{x}, \\mathbf{y} \\rangle = 0\\) Cauchy-Schwarz: \\(|\\langle \\mathbf{x}, \\mathbf{y} \\rangle| \\leq \\|\\mathbf{x}\\|\\|\\mathbf{y}\\|\\) Orthogonal Structures \u00b6 Orthogonal matrices satisfy \\(A^{-1} = A^T\\) and preserve geometry Orthogonal complements: \\(\\ker(A) = \\text{row}(A)^\\perp\\) Gram-Schmidt converts any basis to an orthonormal basis Projections and Rotations \u00b6 Projection onto a line: \\(P_\\pi = \\frac{\\mathbf{b}\\mathbf{b}^T}{\\mathbf{b}^T\\mathbf{b}}\\) Projection onto a subspace: \\(P = B(B^TB)^{-1}B^T\\) 2D rotation: \\(R(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}\\) Practice Problems \u00b6 Problem 1 \u00b6 Compute the \\(\\ell_1\\) , \\(\\ell_2\\) , and \\(\\ell_\\infty\\) norms of: $ \\(\\mathbf{x} = \\begin{bmatrix} -2 \\\\ 6 \\\\ -3 \\end{bmatrix}\\) $ Problem 2 \u00b6 Let \\(\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 1 \\\\ -1 \\end{bmatrix}\\) and \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}\\) . Compute the angle \\(\\theta\\) between them. Problem 3 \u00b6 Verify that \\(A = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\\) is an orthogonal matrix and determine whether it represents a rotation or a reflection. Problem 4 \u00b6 Project \\(\\mathbf{x} = \\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix}\\) onto the line spanned by \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) . Problem 5 \u00b6 Apply the Gram-Schmidt process to the vectors \\(\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{b}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}\\) to produce an orthonormal basis. Problem 6 \u00b6 Let \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\) . Find \\(R(\\theta)\\mathbf{x}\\) for \\(\\theta = 60^\\circ\\) and verify that the result has the same norm as \\(\\mathbf{x}\\) . Solutions \u00b6 Solution 1: \\[\\|\\mathbf{x}\\|_1 = |-2| + |6| + |-3| = 2 + 6 + 3 = 11\\] \\[\\|\\mathbf{x}\\|_2 = \\sqrt{(-2)^2 + 6^2 + (-3)^2} = \\sqrt{4 + 36 + 9} = \\sqrt{49} = 7\\] \\[\\|\\mathbf{x}\\|_\\infty = \\max\\{|-2|, |6|, |-3|\\} = 6\\] Solution 2: First compute the dot product: $ \\(\\langle \\mathbf{a}, \\mathbf{b} \\rangle = 2(1) + 1(-2) + (-1)(3) = 2 - 2 - 3 = -3\\) $ Then compute the norms: $ \\(\\|\\mathbf{a}\\| = \\sqrt{4 + 1 + 1} = \\sqrt{6}, \\quad \\|\\mathbf{b}\\| = \\sqrt{1 + 4 + 9} = \\sqrt{14}\\) $ Therefore: $ \\(\\cos\\theta = \\frac{-3}{\\sqrt{6}\\sqrt{14}} = \\frac{-3}{\\sqrt{84}} = \\frac{-3}{2\\sqrt{21}}\\) $ \\[\\theta = \\arccos\\!\\left(\\frac{-3}{2\\sqrt{21}}\\right) \\approx \\arccos(-0.327) \\approx 109.1^\\circ\\] Solution 3: Check \\(A^T A = I\\) : $ \\(A^T A = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0(0)+(-1)(-1) & 0(1)+(-1)(0) \\\\ 1(0)+0(-1) & 1(1)+0(0) \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I \\quad \\checkmark\\) $ So \\(A\\) is orthogonal. Determine rotation vs. reflection: $ \\(\\det(A) = 0(0) - (1)(-1) = 1\\) $ Since \\(\\det(A) = +1\\) , this is a rotation (not a reflection). Specifically, this is a rotation by \\(-90^\\circ\\) (or equivalently \\(270^\\circ\\) counter-clockwise). Solution 4: \\[\\pi_{\\mathbf{b}}(\\mathbf{x}) = \\frac{\\mathbf{x}^T\\mathbf{b}}{\\mathbf{b}^T\\mathbf{b}} \\mathbf{b} = \\frac{4(1) + 3(1)}{1^2 + 1^2}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{7}{2}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 7/2 \\\\ 7/2 \\end{bmatrix}\\] Verify: The residual \\(\\mathbf{x} - \\pi_{\\mathbf{b}}(\\mathbf{x}) = \\begin{bmatrix} 4 - 7/2 \\\\ 3 - 7/2 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ -1/2 \\end{bmatrix}\\) should be orthogonal to \\(\\mathbf{b}\\) : \\[\\left\\langle \\begin{bmatrix} 1/2 \\\\ -1/2 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\right\\rangle = \\frac{1}{2} - \\frac{1}{2} = 0 \\quad \\checkmark\\] Solution 5: Step 1: Set \\(\\mathbf{u}_1 = \\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) . Step 2: Compute the projection coefficient: $ \\(\\frac{\\langle \\mathbf{b}_2, \\mathbf{u}_1 \\rangle}{\\langle \\mathbf{u}_1, \\mathbf{u}_1 \\rangle} = \\frac{0(1) + 1(1) + 2(1)}{1+1+1} = \\frac{3}{3} = 1\\) $ Step 3: Subtract the projection: $ \\(\\mathbf{u}_2 = \\mathbf{b}_2 - 1 \\cdot \\mathbf{u}_1 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\) $ Check orthogonality: \\(\\langle \\mathbf{u}_1, \\mathbf{u}_2 \\rangle = -1 + 0 + 1 = 0\\) \u2713 Step 4: Normalize: $ \\(\\mathbf{e}_1 = \\frac{\\mathbf{u}_1}{\\|\\mathbf{u}_1\\|} = \\frac{1}{\\sqrt{3}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) $ \\[\\mathbf{e}_2 = \\frac{\\mathbf{u}_2}{\\|\\mathbf{u}_2\\|} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\] Orthonormal basis: \\(\\left\\{\\frac{1}{\\sqrt{3}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix},\\ \\frac{1}{\\sqrt{2}}\\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\right\\}\\) Solution 6: \\[R(60^\\circ) = \\begin{bmatrix} \\cos 60^\\circ & -\\sin 60^\\circ \\\\ \\sin 60^\\circ & \\cos 60^\\circ \\end{bmatrix} = \\begin{bmatrix} 1/2 & -\\sqrt{3}/2 \\\\ \\sqrt{3}/2 & 1/2 \\end{bmatrix}\\] \\[R(60^\\circ)\\mathbf{x} = \\begin{bmatrix} 1/2 & -\\sqrt{3}/2 \\\\ \\sqrt{3}/2 & 1/2 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ \\sqrt{3}/2 \\end{bmatrix}\\] Verify the norm is preserved: \\[\\|\\mathbf{x}\\| = \\sqrt{1^2 + 0^2} = 1\\] \\[\\|R(60^\\circ)\\mathbf{x}\\| = \\sqrt{\\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{\\sqrt{3}}{2}\\right)^2} = \\sqrt{\\frac{1}{4} + \\frac{3}{4}} = \\sqrt{1} = 1 \\quad \\checkmark\\] The norm is preserved, confirming that \\(R(60^\\circ)\\) is an orthogonal (length-preserving) transformation. Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Next: Tutorial 3 - Matrix Decompositions","title":"Tutorial 2: Analytic Geometry"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#tutorial-2-analytic-geometry","text":"Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari","title":"Tutorial 2: Analytic Geometry"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#learning-objectives","text":"By the end of this tutorial, you will understand: Norms and their role in measuring vector magnitude Inner products and their defining axioms How lengths, distances, angles, and orthogonality arise from inner products Orthogonal matrices, orthonormal bases, and orthogonal complements Orthogonal projections and the Gram-Schmidt process Rotation matrices and their geometric meaning","title":"\ud83d\udcda Learning Objectives"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#part-1-norms","text":"","title":"Part 1: Norms"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#11-what-is-a-norm","text":"A norm is a function \\(\\|\\cdot\\| : \\mathbb{R}^n \\to \\mathbb{R}\\) that assigns a non-negative \"length\" to every vector. Think of it as... a ruler for vectors. Different norms are like different ways of measuring distance \u2014 walking along city blocks versus flying in a straight line. A norm must satisfy these properties for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) and all \\(\\lambda \\in \\mathbb{R}\\) : Property Statement Intuition Non-negativity \\(\\|\\mathbf{x}\\| \\geq 0\\) Lengths are never negative Definiteness \\(\\|\\mathbf{x}\\| = 0 \\iff \\mathbf{x} = \\mathbf{0}\\) Only the zero vector has zero length Absolute homogeneity $|\\lambda \\mathbf{x}| = \\lambda Triangle inequality \\(\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|\\) The shortcut is never longer than going around","title":"1.1 What is a Norm?"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#12-common-norms","text":"Norm Name Formula Also Called \\(\\ell_1\\) Manhattan norm \\(\\|\\mathbf{x}\\|_1 = \\displaystyle\\sum_{i=1}^{n} \\|x_i\\|\\) Taxicab norm \\(\\ell_2\\) Euclidean norm \\(\\|\\mathbf{x}\\|_2 = \\sqrt{\\displaystyle\\sum_{i=1}^{n} x_i^2}\\) Standard norm \\(\\ell_\\infty\\) Max norm $|\\mathbf{x}| \\infty = \\max x_i","title":"1.2 Common Norms"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#13-worked-example-computing-norms","text":"Let \\(\\mathbf{x} = \\begin{bmatrix} 3 \\\\ -4 \\\\ 2 \\end{bmatrix}\\) . \\(\\ell_1\\) norm: $ \\(\\|\\mathbf{x}\\|_1 = |3| + |-4| + |2| = 3 + 4 + 2 = 9\\) $ \\(\\ell_2\\) norm: $ \\(\\|\\mathbf{x}\\|_2 = \\sqrt{3^2 + (-4)^2 + 2^2} = \\sqrt{9 + 16 + 4} = \\sqrt{29} \\approx 5.39\\) $ \\(\\ell_\\infty\\) norm: $ \\(\\|\\mathbf{x}\\|_\\infty = \\max\\{|3|, |-4|, |2|\\} = 4\\) $ Think of it as... The \\(\\ell_1\\) norm counts total blocks walked in a grid city. The \\(\\ell_2\\) norm is the straight-line (as the crow flies) distance. The \\(\\ell_\\infty\\) norm is the longest single step you take along any one axis.","title":"1.3 Worked Example: Computing Norms"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#part-2-inner-products","text":"","title":"Part 2: Inner Products"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#21-definition","text":"An inner product on a vector space \\(V\\) is a function \\(\\langle \\cdot, \\cdot \\rangle : V \\times V \\to \\mathbb{R}\\) that satisfies four axioms: Axiom Statement For all Symmetry \\(\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\langle \\mathbf{y}, \\mathbf{x} \\rangle\\) \\(\\mathbf{x}, \\mathbf{y} \\in V\\) Linearity in 1st argument \\(\\langle \\lambda\\mathbf{x} + \\mathbf{z}, \\mathbf{y} \\rangle = \\lambda\\langle \\mathbf{x}, \\mathbf{y} \\rangle + \\langle \\mathbf{z}, \\mathbf{y} \\rangle\\) \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in V,\\ \\lambda \\in \\mathbb{R}\\) Positive semi-definiteness \\(\\langle \\mathbf{x}, \\mathbf{x} \\rangle \\geq 0\\) \\(\\mathbf{x} \\in V\\) Positive definiteness \\(\\langle \\mathbf{x}, \\mathbf{x} \\rangle = 0 \\iff \\mathbf{x} = \\mathbf{0}\\) \\(\\mathbf{x} \\in V\\) Think of it as... an inner product is a generalized way of multiplying two vectors together to get a single number that tells you \"how much\" the vectors agree in direction.","title":"2.1 Definition"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#22-the-dot-product","text":"The most common inner product in \\(\\mathbb{R}^n\\) is the dot product : \\[\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\mathbf{x}^T \\mathbf{y} = \\sum_{i=1}^{n} x_i y_i\\] Example: $ \\(\\left\\langle \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\begin{bmatrix} 4 \\\\ 0 \\\\ -1 \\end{bmatrix} \\right\\rangle = 1(4) + 2(0) + 3(-1) = 4 + 0 - 3 = 1\\) $","title":"2.2 The Dot Product"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#23-general-inner-products-and-positive-definite-matrices","text":"Not every inner product is the dot product. We can define a more general inner product using a symmetric positive definite matrix \\(A\\) : \\[\\langle \\mathbf{x}, \\mathbf{y} \\rangle_A = \\mathbf{x}^T A \\mathbf{y}\\] A symmetric matrix \\(A\\) is positive definite if: $ \\(\\mathbf{x}^T A \\mathbf{x} > 0 \\quad \\text{for all } \\mathbf{x} \\neq \\mathbf{0}\\) $ Example: Let \\(A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) . \\[\\mathbf{x}^T A \\mathbf{x} = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3 & 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 6 > 0\\] Think of it as... the standard dot product uses the identity matrix \\(I\\) as \\(A\\) . Choosing a different positive definite \\(A\\) stretches or skews the geometry, like measuring distance on a tilted surface instead of a flat table.","title":"2.3 General Inner Products and Positive Definite Matrices"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#part-3-lengths-and-distances","text":"","title":"Part 3: Lengths and Distances"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#31-induced-norm","text":"Every inner product induces a norm: \\[\\|\\mathbf{x}\\| = \\sqrt{\\langle \\mathbf{x}, \\mathbf{x} \\rangle}\\] For the standard dot product this gives the Euclidean norm: \\[\\|\\mathbf{x}\\| = \\sqrt{\\mathbf{x}^T \\mathbf{x}} = \\sqrt{\\sum_{i=1}^{n} x_i^2}\\]","title":"3.1 Induced Norm"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#32-distance","text":"The distance between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is: \\[d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\| = \\sqrt{\\langle \\mathbf{x} - \\mathbf{y}, \\mathbf{x} - \\mathbf{y} \\rangle}\\] A distance function (metric) satisfies: Property Statement Non-negativity \\(d(\\mathbf{x}, \\mathbf{y}) \\geq 0\\) Identity \\(d(\\mathbf{x}, \\mathbf{y}) = 0 \\iff \\mathbf{x} = \\mathbf{y}\\) Symmetry \\(d(\\mathbf{x}, \\mathbf{y}) = d(\\mathbf{y}, \\mathbf{x})\\) Triangle inequality \\(d(\\mathbf{x}, \\mathbf{z}) \\leq d(\\mathbf{x}, \\mathbf{y}) + d(\\mathbf{y}, \\mathbf{z})\\)","title":"3.2 Distance"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#33-cauchy-schwarz-inequality","text":"One of the most important inequalities in all of mathematics: \\[|\\langle \\mathbf{x}, \\mathbf{y} \\rangle| \\leq \\|\\mathbf{x}\\| \\cdot \\|\\mathbf{y}\\|\\] Equality holds if and only if \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are linearly dependent (i.e., one is a scalar multiple of the other). Think of it as... the dot product can never exceed the product of the lengths. This is what guarantees that the cosine of the angle between two vectors always stays between \\(-1\\) and \\(1\\) . Example: Let \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) and \\(\\mathbf{y} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}\\) . \\(|\\langle \\mathbf{x}, \\mathbf{y} \\rangle| = |1(3) + 2(1)| = |5| = 5\\) \\(\\|\\mathbf{x}\\| \\cdot \\|\\mathbf{y}\\| = \\sqrt{1+4}\\,\\sqrt{9+1} = \\sqrt{5}\\,\\sqrt{10} = \\sqrt{50} \\approx 7.07\\) Check: \\(5 \\leq 7.07\\) \u2713","title":"3.3 Cauchy-Schwarz Inequality"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#part-4-angles-and-orthogonality","text":"","title":"Part 4: Angles and Orthogonality"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#41-angle-between-vectors","text":"The angle \\(\\theta\\) between two non-zero vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is defined via: \\[\\cos \\theta = \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\| \\cdot \\|\\mathbf{y}\\|}\\] The Cauchy-Schwarz inequality guarantees that the right-hand side lies in \\([-1, 1]\\) , so \\(\\theta\\) is well-defined. Example: Find the angle between \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\) and \\(\\mathbf{y} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) . \\[\\cos \\theta = \\frac{1(1) + 0(1)}{\\sqrt{1}\\,\\sqrt{2}} = \\frac{1}{\\sqrt{2}} \\implies \\theta = \\frac{\\pi}{4} = 45^\\circ\\]","title":"4.1 Angle Between Vectors"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#42-orthogonality","text":"Two vectors are orthogonal (perpendicular) if their inner product is zero: \\[\\mathbf{x} \\perp \\mathbf{y} \\iff \\langle \\mathbf{x}, \\mathbf{y} \\rangle = 0\\] Think of it as... orthogonal vectors carry completely independent information \u2014 knowing one tells you nothing about the other. This is exactly the idea behind \"uncorrelated features\" in machine learning. Example: $ \\(\\left\\langle \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\right\\rangle = 1(1) + (-1)(1) = 0 \\quad \\checkmark \\text{ Orthogonal!}\\) $","title":"4.2 Orthogonality"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#43-orthogonal-and-orthonormal-sets","text":"A set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}\\) is: Term Condition Orthogonal \\(\\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle = 0\\) for all \\(i \\neq j\\) Orthonormal Orthogonal and \\(\\|\\mathbf{v}_i\\| = 1\\) for all \\(i\\) Example of an orthonormal set in \\(\\mathbb{R}^2\\) : $ \\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) $ \\(\\langle \\mathbf{e}_1, \\mathbf{e}_2 \\rangle = 0\\) (orthogonal) \\(\\|\\mathbf{e}_1\\| = 1\\) and \\(\\|\\mathbf{e}_2\\| = 1\\) (unit length)","title":"4.3 Orthogonal and Orthonormal Sets"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#part-5-orthogonal-matrices","text":"","title":"Part 5: Orthogonal Matrices"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#51-definition","text":"A square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) is orthogonal if its columns form an orthonormal set. Equivalently: \\[A^T A = I \\implies A^{-1} = A^T\\] Think of it as... an orthogonal matrix performs a \"rigid\" transformation \u2014 it can rotate or reflect vectors but never stretches or squishes them.","title":"5.1 Definition"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#52-key-properties","text":"Property Statement Inverse equals transpose \\(A^{-1} = A^T\\) Columns are orthonormal \\(\\langle \\mathbf{a}_i, \\mathbf{a}_j \\rangle = \\delta_{ij}\\) Rows are orthonormal \\(A A^T = I\\) Preserves lengths \\(\\|A\\mathbf{x}\\| = \\|\\mathbf{x}\\|\\) Preserves angles \\(\\langle A\\mathbf{x}, A\\mathbf{y} \\rangle = \\langle \\mathbf{x}, \\mathbf{y} \\rangle\\) Determinant \\(\\det(A) = \\pm 1\\) Product is orthogonal If \\(A, B\\) orthogonal, then \\(AB\\) is orthogonal Proof that orthogonal matrices preserve lengths: $ \\(\\|A\\mathbf{x}\\|^2 = (A\\mathbf{x})^T(A\\mathbf{x}) = \\mathbf{x}^T A^T A \\mathbf{x} = \\mathbf{x}^T I \\mathbf{x} = \\mathbf{x}^T \\mathbf{x} = \\|\\mathbf{x}\\|^2\\) $","title":"5.2 Key Properties"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#53-example","text":"\\[A = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\[4pt] \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix}\\] Verify \\(A^T A = I\\) : $ \\(A^T A = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\[4pt] -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\[4pt] \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I \\quad \\checkmark\\) $","title":"5.3 Example"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#part-6-orthonormal-basis","text":"","title":"Part 6: Orthonormal Basis"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#61-definition","text":"An orthonormal basis (ONB) for a subspace \\(U \\subseteq \\mathbb{R}^n\\) is a basis \\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\}\\) such that: \\[\\langle \\mathbf{u}_i, \\mathbf{u}_j \\rangle = \\delta_{ij} = \\begin{cases} 1 & \\text{if } i = j \\\\ 0 & \\text{if } i \\neq j \\end{cases}\\]","title":"6.1 Definition"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#62-why-orthonormal-bases-are-useful","text":"With an orthonormal basis, finding coordinates becomes trivially easy. If \\(\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\}\\) is an ONB for \\(U\\) and \\(\\mathbf{x} \\in U\\) , then: \\[\\mathbf{x} = \\sum_{i=1}^{k} \\langle \\mathbf{x}, \\mathbf{u}_i \\rangle \\, \\mathbf{u}_i\\] Think of it as... with an orthonormal basis, you find each coordinate by simply taking a dot product \u2014 no system of equations to solve. It is the easiest possible coordinate system.","title":"6.2 Why Orthonormal Bases are Useful"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#63-how-to-find-an-orthonormal-basis","text":"Given any basis, use the Gram-Schmidt process (covered in Part 9) to convert it into an orthonormal basis.","title":"6.3 How to Find an Orthonormal Basis"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#part-7-orthogonal-complement","text":"","title":"Part 7: Orthogonal Complement"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#71-definition","text":"Let \\(U\\) be a subspace of \\(\\mathbb{R}^n\\) . The orthogonal complement \\(U^\\perp\\) is the set of all vectors orthogonal to every vector in \\(U\\) : \\[U^\\perp = \\{\\mathbf{v} \\in \\mathbb{R}^n : \\langle \\mathbf{v}, \\mathbf{u} \\rangle = 0 \\text{ for all } \\mathbf{u} \\in U\\}\\] Think of it as... if \\(U\\) is a plane through the origin in 3D, then \\(U^\\perp\\) is the line perpendicular to that plane. Together they account for all of \\(\\mathbb{R}^3\\) .","title":"7.1 Definition"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#72-key-properties","text":"Property Statement Subspace \\(U^\\perp\\) is itself a subspace Dimension \\(\\dim(U) + \\dim(U^\\perp) = n\\) Double complement \\((U^\\perp)^\\perp = U\\) Direct sum \\(\\mathbb{R}^n = U \\oplus U^\\perp\\) (every vector splits uniquely)","title":"7.2 Key Properties"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#73-connection-to-the-kernel-and-row-space","text":"For a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) : \\[\\ker(A) = \\text{row}(A)^\\perp\\] This means: a vector \\(\\mathbf{x}\\) is in the null space of \\(A\\) if and only if \\(\\mathbf{x}\\) is orthogonal to every row of \\(A\\) . Example: Let \\(A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 6 \\end{bmatrix}\\) . The row space is \\(\\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\right\\}\\) (the rows are linearly dependent). The null space is \\(\\ker(A) = \\text{span}\\left\\{\\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\right\\}\\) . Check: \\(\\left\\langle \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix} \\right\\rangle = 1(-2) + 2(1) = 0\\) \u2713","title":"7.3 Connection to the Kernel and Row Space"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#part-8-orthogonal-projections","text":"","title":"Part 8: Orthogonal Projections"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#81-projection-onto-a-line","text":"Given a non-zero vector \\(\\mathbf{b}\\) (defining a line through the origin), the projection of \\(\\mathbf{x}\\) onto the line spanned by \\(\\mathbf{b}\\) is: \\[\\pi_{\\mathbf{b}}(\\mathbf{x}) = \\frac{\\langle \\mathbf{x}, \\mathbf{b} \\rangle}{\\langle \\mathbf{b}, \\mathbf{b} \\rangle} \\mathbf{b} = \\frac{\\mathbf{b}\\mathbf{b}^T}{\\mathbf{b}^T\\mathbf{b}} \\mathbf{x}\\] The projection matrix is: \\[P_\\pi = \\frac{\\mathbf{b}\\mathbf{b}^T}{\\mathbf{b}^T\\mathbf{b}}\\] Think of it as... shining a flashlight straight down onto a line and seeing where the shadow of your vector lands. The projection is that shadow. Example: Project \\(\\mathbf{x} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}\\) onto \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\) . \\[\\pi_{\\mathbf{b}}(\\mathbf{x}) = \\frac{\\mathbf{x}^T\\mathbf{b}}{\\mathbf{b}^T\\mathbf{b}} \\mathbf{b} = \\frac{3(1) + 1(2)}{1^2 + 2^2} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\frac{5}{5} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\]","title":"8.1 Projection onto a Line"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#82-projection-onto-a-general-subspace","text":"Let \\(U = \\text{span}\\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_k\\}\\) and define \\(B = [\\mathbf{b}_1 \\mid \\cdots \\mid \\mathbf{b}_k]\\) . The projection of \\(\\mathbf{x}\\) onto \\(U\\) is: \\[\\pi_U(\\mathbf{x}) = B(B^T B)^{-1} B^T \\mathbf{x}\\] The projection matrix is: \\[P = B(B^T B)^{-1} B^T\\] Properties of projection matrices: Property Statement Idempotent \\(P^2 = P\\) (projecting twice is the same as projecting once) Symmetric \\(P^T = P\\) Residual \\(\\mathbf{x} - P\\mathbf{x}\\) is orthogonal to \\(U\\)","title":"8.2 Projection onto a General Subspace"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#83-connection-to-the-pseudo-inverse","text":"The Moore-Penrose pseudo-inverse of \\(B\\) is: \\[B^\\dagger = (B^T B)^{-1} B^T\\] So the projection simplifies to: \\[\\pi_U(\\mathbf{x}) = B B^\\dagger \\mathbf{x}\\] The pseudo-inverse is central to solving least-squares problems: when \\(A\\mathbf{x} = \\mathbf{b}\\) has no exact solution, the best approximate solution is \\(\\hat{\\mathbf{x}} = A^\\dagger \\mathbf{b}\\) .","title":"8.3 Connection to the Pseudo-Inverse"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#84-worked-example-projection-onto-a-subspace","text":"Project \\(\\mathbf{x} = \\begin{bmatrix} 6 \\\\ 0 \\\\ 0 \\end{bmatrix}\\) onto \\(U = \\text{span}\\left\\{\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}\\right\\}\\) . Step 1: Form the matrix \\(B\\) : $ \\(B = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}\\) $ Step 2: Compute \\(B^T B\\) : $ \\(B^T B = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\) $ Step 3: Compute \\((B^T B)^{-1}\\) : $ \\((B^T B)^{-1} = \\frac{1}{2(2) - 1(1)} \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix} = \\frac{1}{3} \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}\\) $ Step 4: Compute \\(B^T \\mathbf{x}\\) : $ \\(B^T \\mathbf{x} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 6 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix}\\) $ Step 5: Compute \\((B^T B)^{-1} B^T \\mathbf{x}\\) : $ \\(\\frac{1}{3}\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}\\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix} 12 \\\\ -6 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ -2 \\end{bmatrix}\\) $ Step 6: Compute the projection: $ \\(\\pi_U(\\mathbf{x}) = B \\begin{bmatrix} 4 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}\\begin{bmatrix} 4 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ -2 \\\\ 2 \\end{bmatrix}\\) $ Verify: The residual \\(\\mathbf{x} - \\pi_U(\\mathbf{x}) = \\begin{bmatrix} 2 \\\\ 2 \\\\ -2 \\end{bmatrix}\\) should be orthogonal to both basis vectors: \\(\\langle \\begin{bmatrix} 2 \\\\ 2 \\\\ -2 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} \\rangle = 2 + 0 - 2 = 0\\) \u2713 \\(\\langle \\begin{bmatrix} 2 \\\\ 2 \\\\ -2 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} \\rangle = 0 + 2 - 2 = 0\\) \u2713","title":"8.4 Worked Example: Projection onto a Subspace"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#part-9-gram-schmidt-process","text":"","title":"Part 9: Gram-Schmidt Process"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#91-the-algorithm","text":"The Gram-Schmidt process takes any set of linearly independent vectors and produces an orthonormal set spanning the same subspace. Given linearly independent vectors \\(\\{\\mathbf{b}_1, \\mathbf{b}_2, \\ldots, \\mathbf{b}_k\\}\\) : Step 1: Orthogonalize (produce orthogonal vectors \\(\\mathbf{u}_i\\) ) \\[\\mathbf{u}_1 = \\mathbf{b}_1\\] \\[\\mathbf{u}_2 = \\mathbf{b}_2 - \\frac{\\langle \\mathbf{b}_2, \\mathbf{u}_1 \\rangle}{\\langle \\mathbf{u}_1, \\mathbf{u}_1 \\rangle} \\mathbf{u}_1\\] \\[\\mathbf{u}_3 = \\mathbf{b}_3 - \\frac{\\langle \\mathbf{b}_3, \\mathbf{u}_1 \\rangle}{\\langle \\mathbf{u}_1, \\mathbf{u}_1 \\rangle} \\mathbf{u}_1 - \\frac{\\langle \\mathbf{b}_3, \\mathbf{u}_2 \\rangle}{\\langle \\mathbf{u}_2, \\mathbf{u}_2 \\rangle} \\mathbf{u}_2\\] In general: $ \\(\\mathbf{u}_i = \\mathbf{b}_i - \\sum_{j=1}^{i-1} \\frac{\\langle \\mathbf{b}_i, \\mathbf{u}_j \\rangle}{\\langle \\mathbf{u}_j, \\mathbf{u}_j \\rangle} \\mathbf{u}_j\\) $ Step 2: Normalize (produce unit vectors \\(\\mathbf{e}_i\\) ) \\[\\mathbf{e}_i = \\frac{\\mathbf{u}_i}{\\|\\mathbf{u}_i\\|}\\] Think of it as... taking each new vector and \"subtracting off\" all the parts that point in the directions you have already handled. What remains is the genuinely new direction. Then you scale it to length 1.","title":"9.1 The Algorithm"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#92-worked-example","text":"Apply Gram-Schmidt to \\(\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\) and \\(\\mathbf{b}_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\) . Step 1: Set \\(\\mathbf{u}_1 = \\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\) . Step 2: Compute the projection coefficient: $ \\(\\frac{\\langle \\mathbf{b}_2, \\mathbf{u}_1 \\rangle}{\\langle \\mathbf{u}_1, \\mathbf{u}_1 \\rangle} = \\frac{1(1) + 0(1) + 1(0)}{1^2 + 1^2 + 0^2} = \\frac{1}{2}\\) $ Step 3: Subtract the projection: $ \\(\\mathbf{u}_2 = \\mathbf{b}_2 - \\frac{1}{2}\\mathbf{u}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} - \\frac{1}{2}\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ -1/2 \\\\ 1 \\end{bmatrix}\\) $ Verify orthogonality: $ \\(\\langle \\mathbf{u}_1, \\mathbf{u}_2 \\rangle = 1\\!\\left(\\tfrac{1}{2}\\right) + 1\\!\\left(-\\tfrac{1}{2}\\right) + 0(1) = 0 \\quad \\checkmark\\) $ Step 4: Normalize: $ \\(\\|\\mathbf{u}_1\\| = \\sqrt{1+1+0} = \\sqrt{2}, \\quad \\mathbf{e}_1 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\\) $ \\[\\|\\mathbf{u}_2\\| = \\sqrt{\\tfrac{1}{4}+\\tfrac{1}{4}+1} = \\sqrt{\\tfrac{3}{2}} = \\frac{\\sqrt{6}}{2}, \\quad \\mathbf{e}_2 = \\frac{2}{\\sqrt{6}}\\begin{bmatrix} 1/2 \\\\ -1/2 \\\\ 1 \\end{bmatrix} = \\frac{1}{\\sqrt{6}}\\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}\\] Result: The orthonormal basis is: $ \\(\\mathbf{e}_1 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\qquad \\mathbf{e}_2 = \\frac{1}{\\sqrt{6}}\\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}\\) $","title":"9.2 Worked Example"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#part-10-rotations","text":"","title":"Part 10: Rotations"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#101-rotation-matrix-in-2d","text":"A rotation by angle \\(\\theta\\) (counter-clockwise) in \\(\\mathbb{R}^2\\) is given by: \\[R(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}\\] Think of it as... every point in the plane is swung around the origin by the angle \\(\\theta\\) . The matrix encodes where the two standard basis vectors land after the rotation.","title":"10.1 Rotation Matrix in 2D"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#102-properties-of-rotation-matrices","text":"Property Statement Orthogonal \\(R(\\theta)^T R(\\theta) = I\\) Determinant \\(\\det(R(\\theta)) = 1\\) (no reflection) Inverse is reverse rotation \\(R(\\theta)^{-1} = R(-\\theta) = R(\\theta)^T\\) Composition \\(R(\\alpha) R(\\beta) = R(\\alpha + \\beta)\\) Preserves lengths \\(\\|R(\\theta)\\mathbf{x}\\| = \\|\\mathbf{x}\\|\\) Preserves angles Angles between vectors are unchanged","title":"10.2 Properties of Rotation Matrices"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#103-worked-example","text":"Rotate \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\) by \\(\\theta = 90^\\circ\\) . \\[R(90^\\circ) = \\begin{bmatrix} \\cos 90^\\circ & -\\sin 90^\\circ \\\\ \\sin 90^\\circ & \\cos 90^\\circ \\end{bmatrix} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\] \\[R(90^\\circ)\\mathbf{x} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\] This is exactly the unit vector pointing straight up \u2014 a \\(90^\\circ\\) counter-clockwise rotation of the unit vector pointing right. \u2713","title":"10.3 Worked Example"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#104-rotations-in-3d-preview","text":"In \\(\\mathbb{R}^3\\) , a rotation about the \\(z\\) -axis by angle \\(\\theta\\) is: \\[R_z(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta & 0 \\\\ \\sin\\theta & \\cos\\theta & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\] General 3D rotations can be composed from rotations about the three coordinate axes.","title":"10.4 Rotations in 3D (Preview)"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#summary-key-takeaways","text":"","title":"Summary: Key Takeaways"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#norms-and-inner-products","text":"The \\(\\ell_1\\) , \\(\\ell_2\\) , and \\(\\ell_\\infty\\) norms each measure vector size differently An inner product \\(\\langle \\cdot, \\cdot \\rangle\\) must satisfy symmetry, linearity, and positive definiteness Every inner product induces a norm: \\(\\|\\mathbf{x}\\| = \\sqrt{\\langle \\mathbf{x}, \\mathbf{x} \\rangle}\\)","title":"Norms and Inner Products"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#geometry-from-inner-products","text":"Angles: \\(\\cos\\theta = \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}\\) Orthogonality: \\(\\langle \\mathbf{x}, \\mathbf{y} \\rangle = 0\\) Cauchy-Schwarz: \\(|\\langle \\mathbf{x}, \\mathbf{y} \\rangle| \\leq \\|\\mathbf{x}\\|\\|\\mathbf{y}\\|\\)","title":"Geometry from Inner Products"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#orthogonal-structures","text":"Orthogonal matrices satisfy \\(A^{-1} = A^T\\) and preserve geometry Orthogonal complements: \\(\\ker(A) = \\text{row}(A)^\\perp\\) Gram-Schmidt converts any basis to an orthonormal basis","title":"Orthogonal Structures"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#projections-and-rotations","text":"Projection onto a line: \\(P_\\pi = \\frac{\\mathbf{b}\\mathbf{b}^T}{\\mathbf{b}^T\\mathbf{b}}\\) Projection onto a subspace: \\(P = B(B^TB)^{-1}B^T\\) 2D rotation: \\(R(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}\\)","title":"Projections and Rotations"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#practice-problems","text":"","title":"Practice Problems"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#problem-1","text":"Compute the \\(\\ell_1\\) , \\(\\ell_2\\) , and \\(\\ell_\\infty\\) norms of: $ \\(\\mathbf{x} = \\begin{bmatrix} -2 \\\\ 6 \\\\ -3 \\end{bmatrix}\\) $","title":"Problem 1"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#problem-2","text":"Let \\(\\mathbf{a} = \\begin{bmatrix} 2 \\\\ 1 \\\\ -1 \\end{bmatrix}\\) and \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}\\) . Compute the angle \\(\\theta\\) between them.","title":"Problem 2"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#problem-3","text":"Verify that \\(A = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}\\) is an orthogonal matrix and determine whether it represents a rotation or a reflection.","title":"Problem 3"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#problem-4","text":"Project \\(\\mathbf{x} = \\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix}\\) onto the line spanned by \\(\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) .","title":"Problem 4"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#problem-5","text":"Apply the Gram-Schmidt process to the vectors \\(\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathbf{b}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}\\) to produce an orthonormal basis.","title":"Problem 5"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#problem-6","text":"Let \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\) . Find \\(R(\\theta)\\mathbf{x}\\) for \\(\\theta = 60^\\circ\\) and verify that the result has the same norm as \\(\\mathbf{x}\\) .","title":"Problem 6"},{"location":"tutorials/Tutorial_03_Analytic_Geometry/#solutions","text":"Solution 1: \\[\\|\\mathbf{x}\\|_1 = |-2| + |6| + |-3| = 2 + 6 + 3 = 11\\] \\[\\|\\mathbf{x}\\|_2 = \\sqrt{(-2)^2 + 6^2 + (-3)^2} = \\sqrt{4 + 36 + 9} = \\sqrt{49} = 7\\] \\[\\|\\mathbf{x}\\|_\\infty = \\max\\{|-2|, |6|, |-3|\\} = 6\\] Solution 2: First compute the dot product: $ \\(\\langle \\mathbf{a}, \\mathbf{b} \\rangle = 2(1) + 1(-2) + (-1)(3) = 2 - 2 - 3 = -3\\) $ Then compute the norms: $ \\(\\|\\mathbf{a}\\| = \\sqrt{4 + 1 + 1} = \\sqrt{6}, \\quad \\|\\mathbf{b}\\| = \\sqrt{1 + 4 + 9} = \\sqrt{14}\\) $ Therefore: $ \\(\\cos\\theta = \\frac{-3}{\\sqrt{6}\\sqrt{14}} = \\frac{-3}{\\sqrt{84}} = \\frac{-3}{2\\sqrt{21}}\\) $ \\[\\theta = \\arccos\\!\\left(\\frac{-3}{2\\sqrt{21}}\\right) \\approx \\arccos(-0.327) \\approx 109.1^\\circ\\] Solution 3: Check \\(A^T A = I\\) : $ \\(A^T A = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0(0)+(-1)(-1) & 0(1)+(-1)(0) \\\\ 1(0)+0(-1) & 1(1)+0(0) \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I \\quad \\checkmark\\) $ So \\(A\\) is orthogonal. Determine rotation vs. reflection: $ \\(\\det(A) = 0(0) - (1)(-1) = 1\\) $ Since \\(\\det(A) = +1\\) , this is a rotation (not a reflection). Specifically, this is a rotation by \\(-90^\\circ\\) (or equivalently \\(270^\\circ\\) counter-clockwise). Solution 4: \\[\\pi_{\\mathbf{b}}(\\mathbf{x}) = \\frac{\\mathbf{x}^T\\mathbf{b}}{\\mathbf{b}^T\\mathbf{b}} \\mathbf{b} = \\frac{4(1) + 3(1)}{1^2 + 1^2}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{7}{2}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 7/2 \\\\ 7/2 \\end{bmatrix}\\] Verify: The residual \\(\\mathbf{x} - \\pi_{\\mathbf{b}}(\\mathbf{x}) = \\begin{bmatrix} 4 - 7/2 \\\\ 3 - 7/2 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ -1/2 \\end{bmatrix}\\) should be orthogonal to \\(\\mathbf{b}\\) : \\[\\left\\langle \\begin{bmatrix} 1/2 \\\\ -1/2 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\right\\rangle = \\frac{1}{2} - \\frac{1}{2} = 0 \\quad \\checkmark\\] Solution 5: Step 1: Set \\(\\mathbf{u}_1 = \\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) . Step 2: Compute the projection coefficient: $ \\(\\frac{\\langle \\mathbf{b}_2, \\mathbf{u}_1 \\rangle}{\\langle \\mathbf{u}_1, \\mathbf{u}_1 \\rangle} = \\frac{0(1) + 1(1) + 2(1)}{1+1+1} = \\frac{3}{3} = 1\\) $ Step 3: Subtract the projection: $ \\(\\mathbf{u}_2 = \\mathbf{b}_2 - 1 \\cdot \\mathbf{u}_1 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\) $ Check orthogonality: \\(\\langle \\mathbf{u}_1, \\mathbf{u}_2 \\rangle = -1 + 0 + 1 = 0\\) \u2713 Step 4: Normalize: $ \\(\\mathbf{e}_1 = \\frac{\\mathbf{u}_1}{\\|\\mathbf{u}_1\\|} = \\frac{1}{\\sqrt{3}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\) $ \\[\\mathbf{e}_2 = \\frac{\\mathbf{u}_2}{\\|\\mathbf{u}_2\\|} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\] Orthonormal basis: \\(\\left\\{\\frac{1}{\\sqrt{3}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix},\\ \\frac{1}{\\sqrt{2}}\\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix}\\right\\}\\) Solution 6: \\[R(60^\\circ) = \\begin{bmatrix} \\cos 60^\\circ & -\\sin 60^\\circ \\\\ \\sin 60^\\circ & \\cos 60^\\circ \\end{bmatrix} = \\begin{bmatrix} 1/2 & -\\sqrt{3}/2 \\\\ \\sqrt{3}/2 & 1/2 \\end{bmatrix}\\] \\[R(60^\\circ)\\mathbf{x} = \\begin{bmatrix} 1/2 & -\\sqrt{3}/2 \\\\ \\sqrt{3}/2 & 1/2 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ \\sqrt{3}/2 \\end{bmatrix}\\] Verify the norm is preserved: \\[\\|\\mathbf{x}\\| = \\sqrt{1^2 + 0^2} = 1\\] \\[\\|R(60^\\circ)\\mathbf{x}\\| = \\sqrt{\\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{\\sqrt{3}}{2}\\right)^2} = \\sqrt{\\frac{1}{4} + \\frac{3}{4}} = \\sqrt{1} = 1 \\quad \\checkmark\\] The norm is preserved, confirming that \\(R(60^\\circ)\\) is an orthogonal (length-preserving) transformation. Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Next: Tutorial 3 - Matrix Decompositions","title":"Solutions"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/","text":"Tutorial 3: Matrix Decomposition \u00b6 Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari \ud83d\udcda Learning Objectives \u00b6 By the end of this tutorial, you will understand: How to compute determinants for 2x2 and 3x3 matrices The trace of a matrix and its properties Eigenvalues and eigenvectors and how to compute them Cholesky decomposition for symmetric positive definite matrices Eigendecomposition and diagonalization Singular Value Decomposition (SVD) and its geometric meaning Matrix approximation using truncated SVD Part 1: Determinants \u00b6 1.1 What is a Determinant? \u00b6 The determinant is a scalar value computed from a square matrix that captures important information about the matrix. Think of it as a single number that tells you: Whether the matrix is invertible (determinant is nonzero) How the matrix scales areas or volumes when used as a linear transformation The \"signed volume\" of the parallelepiped formed by the column vectors Notation: For a matrix \\(A\\) , the determinant is written as \\(\\det(A)\\) or \\(|A|\\) . 1.2 Determinant of a 2x2 Matrix \u00b6 For a \\(2 \\times 2\\) matrix: \\[A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\] The determinant is: \\[\\det(A) = ad - bc\\] In plain English: multiply the diagonals and subtract. The main diagonal product minus the off-diagonal product. Worked Example: \\[A = \\begin{bmatrix} 3 & 7 \\\\ 1 & 5 \\end{bmatrix}\\] \\[\\det(A) = 3(5) - 7(1) = 15 - 7 = 8\\] Since \\(\\det(A) = 8 \\neq 0\\) , the matrix \\(A\\) is invertible. 1.3 Determinant of a 3x3 Matrix \u00b6 For a \\(3 \\times 3\\) matrix: \\[A = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}\\] Method 1: Sarrus' Rule \u00b6 Write the matrix and repeat the first two columns to the right: \\[\\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} \\begin{matrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{matrix}\\] Then sum the products along the three downward diagonals and subtract the products along the three upward diagonals: \\[\\det(A) = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32}\\] Method 2: Cofactor Expansion (along the first row) \u00b6 \\[\\det(A) = a_{11} \\begin{vmatrix} a_{22} & a_{23} \\\\ a_{32} & a_{33} \\end{vmatrix} - a_{12} \\begin{vmatrix} a_{21} & a_{23} \\\\ a_{31} & a_{33} \\end{vmatrix} + a_{13} \\begin{vmatrix} a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{vmatrix}\\] Each smaller determinant is called a minor , and the signed minor is a cofactor . Worked Example: \\[B = \\begin{bmatrix} 2 & 1 & 3 \\\\ 0 & 4 & 5 \\\\ 1 & 0 & 2 \\end{bmatrix}\\] Using cofactor expansion along the first row: \\[\\det(B) = 2 \\begin{vmatrix} 4 & 5 \\\\ 0 & 2 \\end{vmatrix} - 1 \\begin{vmatrix} 0 & 5 \\\\ 1 & 2 \\end{vmatrix} + 3 \\begin{vmatrix} 0 & 4 \\\\ 1 & 0 \\end{vmatrix}\\] \\[= 2(4 \\cdot 2 - 5 \\cdot 0) - 1(0 \\cdot 2 - 5 \\cdot 1) + 3(0 \\cdot 0 - 4 \\cdot 1)\\] \\[= 2(8) - 1(-5) + 3(-4)\\] \\[= 16 + 5 - 12 = 9\\] 1.4 Properties of Determinants \u00b6 Property Statement Example / Note Identity \\(\\det(I) = 1\\) The identity matrix always has determinant 1 Transpose \\(\\det(A^T) = \\det(A)\\) Transposing does not change the determinant Product \\(\\det(AB) = \\det(A) \\cdot \\det(B)\\) Determinant of a product is the product of determinants Inverse \\(\\det(A^{-1}) = \\frac{1}{\\det(A)}\\) Only defined when \\(\\det(A) \\neq 0\\) Scalar multiple \\(\\det(cA) = c^n \\det(A)\\) For an \\(n \\times n\\) matrix Singular matrix \\(\\det(A) = 0\\) Matrix is not invertible Row swap Swapping two rows flips the sign \\(\\det(\\text{swapped}) = -\\det(A)\\) Triangular \\(\\det(A) = \\prod_{i=1}^{n} a_{ii}\\) Product of diagonal entries for triangular matrices Part 2: Trace \u00b6 2.1 Definition \u00b6 The trace of a square matrix \\(A\\) is the sum of its diagonal entries: \\[\\text{tr}(A) = \\sum_{i=1}^{n} a_{ii} = a_{11} + a_{22} + \\cdots + a_{nn}\\] In plain English: just add up all the numbers on the main diagonal. Example: \\[A = \\begin{bmatrix} 5 & 2 & 1 \\\\ 0 & 3 & 4 \\\\ 7 & 6 & 8 \\end{bmatrix}\\] \\[\\text{tr}(A) = 5 + 3 + 8 = 16\\] 2.2 Properties of the Trace \u00b6 Property Statement Linearity \\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\) Scalar multiplication \\(\\text{tr}(cA) = c \\cdot \\text{tr}(A)\\) Transpose \\(\\text{tr}(A^T) = \\text{tr}(A)\\) Cyclic property \\(\\text{tr}(AB) = \\text{tr}(BA)\\) Cyclic property (3 matrices) \\(\\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)\\) Sum of eigenvalues \\(\\text{tr}(A) = \\sum_{i=1}^{n} \\lambda_i\\) Frobenius norm \\(\\text{tr}(A^T A) = \\sum_{i,j} a_{ij}^2 = \\|A\\|_F^2\\) The cyclic property \\(\\text{tr}(AB) = \\text{tr}(BA)\\) is especially important in machine learning. It lets you rearrange matrix products inside a trace, which simplifies many derivations in optimization and statistics. Worked Example: Verify \\(\\text{tr}(AB) = \\text{tr}(BA)\\) . \\[A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\] \\[AB = \\begin{bmatrix} 1(5)+2(7) & 1(6)+2(8) \\\\ 3(5)+4(7) & 3(6)+4(8) \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\\] \\[\\text{tr}(AB) = 19 + 50 = 69\\] \\[BA = \\begin{bmatrix} 5(1)+6(3) & 5(2)+6(4) \\\\ 7(1)+8(3) & 7(2)+8(4) \\end{bmatrix} = \\begin{bmatrix} 23 & 34 \\\\ 31 & 46 \\end{bmatrix}\\] \\[\\text{tr}(BA) = 23 + 46 = 69\\] Both sides give 69, confirming the property. Part 3: Eigenvalues and Eigenvectors \u00b6 3.1 Definition \u00b6 Given a square matrix \\(A\\) , a nonzero vector \\(\\mathbf{v}\\) is an eigenvector of \\(A\\) if multiplying \\(A\\) by \\(\\mathbf{v}\\) simply scales \\(\\mathbf{v}\\) by some scalar \\(\\lambda\\) : \\[A\\mathbf{v} = \\lambda \\mathbf{v}\\] \\(\\lambda\\) is called the eigenvalue corresponding to \\(\\mathbf{v}\\) \\(\\mathbf{v}\\) is the eigenvector corresponding to \\(\\lambda\\) In plain English: an eigenvector is a special direction that the matrix only stretches (or flips), but does not rotate. The eigenvalue tells you by how much it stretches. 3.2 The Characteristic Polynomial \u00b6 To find eigenvalues, we rearrange \\(A\\mathbf{v} = \\lambda \\mathbf{v}\\) : \\[A\\mathbf{v} - \\lambda \\mathbf{v} = \\mathbf{0}\\] \\[(A - \\lambda I)\\mathbf{v} = \\mathbf{0}\\] For a nonzero solution \\(\\mathbf{v}\\) to exist, the matrix \\((A - \\lambda I)\\) must be singular, meaning: \\[\\det(A - \\lambda I) = 0\\] This equation is called the characteristic equation , and the polynomial on the left side is the characteristic polynomial . Solving it gives us the eigenvalues. 3.3 Worked Example: Finding Eigenvalues and Eigenvectors \u00b6 Find the eigenvalues and eigenvectors of: \\[A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix}\\] Step 1: Characteristic polynomial. \\[A - \\lambda I = \\begin{bmatrix} 4 - \\lambda & 1 \\\\ 2 & 3 - \\lambda \\end{bmatrix}\\] \\[\\det(A - \\lambda I) = (4 - \\lambda)(3 - \\lambda) - (1)(2)\\] \\[= 12 - 4\\lambda - 3\\lambda + \\lambda^2 - 2\\] \\[= \\lambda^2 - 7\\lambda + 10\\] Step 2: Solve the characteristic equation. \\[\\lambda^2 - 7\\lambda + 10 = 0\\] \\[(\\lambda - 5)(\\lambda - 2) = 0\\] \\[\\lambda_1 = 5, \\quad \\lambda_2 = 2\\] Step 3: Find eigenvectors for each eigenvalue. For \\(\\lambda_1 = 5\\) : \\[(A - 5I)\\mathbf{v} = \\mathbf{0}\\] \\[\\begin{bmatrix} -1 & 1 \\\\ 2 & -2 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\] From the first row: \\(-v_1 + v_2 = 0\\) , so \\(v_2 = v_1\\) . Choosing \\(v_1 = 1\\) : \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\] For \\(\\lambda_2 = 2\\) : \\[(A - 2I)\\mathbf{v} = \\mathbf{0}\\] \\[\\begin{bmatrix} 2 & 1 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\] From the first row: \\(2v_1 + v_2 = 0\\) , so \\(v_2 = -2v_1\\) . Choosing \\(v_1 = 1\\) : \\[\\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}\\] Quick check: \\(\\text{tr}(A) = 4 + 3 = 7 = 5 + 2 = \\lambda_1 + \\lambda_2\\) and \\(\\det(A) = 4(3) - 1(2) = 10 = 5 \\times 2 = \\lambda_1 \\cdot \\lambda_2\\) . Both checks pass. 3.4 Key Facts About Eigenvalues \u00b6 Fact Statement Sum of eigenvalues \\(\\sum \\lambda_i = \\text{tr}(A)\\) Product of eigenvalues \\(\\prod \\lambda_i = \\det(A)\\) Symmetric matrices All eigenvalues are real; eigenvectors are orthogonal Positive definite All eigenvalues are strictly positive Singular matrix At least one eigenvalue is zero Part 4: Cholesky Decomposition \u00b6 4.1 What is Cholesky Decomposition? \u00b6 For a symmetric positive definite (SPD) matrix \\(A\\) , the Cholesky decomposition factors \\(A\\) into: \\[A = LL^T\\] where \\(L\\) is a lower triangular matrix with positive diagonal entries. Think of it as the \"square root\" of a matrix. Just as every positive number has a square root, every SPD matrix has a Cholesky factor. 4.2 What Does \"Symmetric Positive Definite\" Mean? \u00b6 A matrix \\(A\\) is symmetric positive definite if: Symmetric: \\(A = A^T\\) (the matrix equals its transpose) Positive definite: \\(\\mathbf{x}^T A \\mathbf{x} > 0\\) for all nonzero vectors \\(\\mathbf{x}\\) Equivalently, \\(A\\) is SPD if it is symmetric and all its eigenvalues are strictly positive. 4.3 The Cholesky Algorithm (for 2x2) \u00b6 Given: \\[A = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{12} & a_{22} \\end{bmatrix} = \\begin{bmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{bmatrix} \\begin{bmatrix} l_{11} & l_{21} \\\\ 0 & l_{22} \\end{bmatrix}\\] Expanding the right side and matching entries: \\[l_{11} = \\sqrt{a_{11}}\\] \\[l_{21} = \\frac{a_{12}}{l_{11}}\\] \\[l_{22} = \\sqrt{a_{22} - l_{21}^2}\\] 4.4 The Cholesky Algorithm (general) \u00b6 For an \\(n \\times n\\) SPD matrix, the entries of \\(L\\) are computed as: \\[l_{jj} = \\sqrt{a_{jj} - \\sum_{k=1}^{j-1} l_{jk}^2}\\] \\[l_{ij} = \\frac{1}{l_{jj}} \\left( a_{ij} - \\sum_{k=1}^{j-1} l_{ik} l_{jk} \\right), \\quad \\text{for } i > j\\] 4.5 Worked Example \u00b6 Find the Cholesky decomposition of: \\[A = \\begin{bmatrix} 4 & 2 \\\\ 2 & 5 \\end{bmatrix}\\] Step 1: Check that \\(A\\) is SPD. It is symmetric ( \\(A = A^T\\) ). Its eigenvalues are \\(\\lambda = \\frac{9 \\pm \\sqrt{1}}{2}\\) , giving \\(\\lambda_1 \\approx 5.56\\) and \\(\\lambda_2 \\approx 3.44\\) , both positive. (Alternatively, \\(\\det(A) = 16 > 0\\) and \\(a_{11} = 4 > 0\\) .) Step 2: Compute \\(L\\) . \\[l_{11} = \\sqrt{4} = 2\\] \\[l_{21} = \\frac{2}{2} = 1\\] \\[l_{22} = \\sqrt{5 - 1^2} = \\sqrt{4} = 2\\] Result: \\[L = \\begin{bmatrix} 2 & 0 \\\\ 1 & 2 \\end{bmatrix}\\] Verification: \\[LL^T = \\begin{bmatrix} 2 & 0 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 4 & 2 \\\\ 2 & 5 \\end{bmatrix} = A \\quad \\checkmark\\] 4.6 Why Cholesky Decomposition Matters \u00b6 Application How It Helps Solving linear systems \\(Ax = b\\) becomes two easy triangular solves: \\(Ly = b\\) , then \\(L^Tx = y\\) Sampling from multivariate Gaussians If \\(\\Sigma = LL^T\\) , generate \\(\\mathbf{x} = L\\mathbf{z} + \\boldsymbol{\\mu}\\) where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) Numerical stability More stable and about twice as fast as general LU decomposition for SPD matrices Part 5: Eigendecomposition \u00b6 5.1 Definition \u00b6 If a square matrix \\(A\\) has \\(n\\) linearly independent eigenvectors, then \\(A\\) can be factored as: \\[A = PDP^{-1}\\] where: - \\(P\\) is the matrix whose columns are the eigenvectors of \\(A\\) : \\(P = [\\mathbf{v}_1 \\mid \\mathbf{v}_2 \\mid \\cdots \\mid \\mathbf{v}_n]\\) - \\(D\\) is a diagonal matrix with the corresponding eigenvalues on the diagonal: \\[D = \\begin{bmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{bmatrix}\\] This factorization is called the eigendecomposition or spectral decomposition (for symmetric matrices). 5.2 When Does Eigendecomposition Exist? \u00b6 Condition Eigendecomposition Exists? \\(A\\) has \\(n\\) distinct eigenvalues Always yes \\(A\\) is symmetric ( \\(A = A^T\\) ) Always yes, and \\(P\\) is orthogonal ( \\(P^{-1} = P^T\\) ) \\(A\\) has repeated eigenvalues Sometimes (depends on geometric multiplicity) \\(A\\) is defective (not enough independent eigenvectors) No For symmetric matrices , the decomposition simplifies to: \\[A = PDP^T\\] because the eigenvectors are orthogonal. 5.3 Worked Example \u00b6 Diagonalize the matrix from Part 3: \\[A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix}\\] We already found \\(\\lambda_1 = 5\\) , \\(\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) and \\(\\lambda_2 = 2\\) , \\(\\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}\\) . \\[P = \\begin{bmatrix} 1 & 1 \\\\ 1 & -2 \\end{bmatrix}, \\quad D = \\begin{bmatrix} 5 & 0 \\\\ 0 & 2 \\end{bmatrix}\\] Compute \\(P^{-1}\\) . For a \\(2 \\times 2\\) matrix \\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) , the inverse is \\(\\frac{1}{ad - bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\) : \\[P^{-1} = \\frac{1}{(1)(-2) - (1)(1)} \\begin{bmatrix} -2 & -1 \\\\ -1 & 1 \\end{bmatrix} = \\frac{1}{-3} \\begin{bmatrix} -2 & -1 \\\\ -1 & 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & -\\frac{1}{3} \\end{bmatrix}\\] Verification: \\[PDP^{-1} = \\begin{bmatrix} 1 & 1 \\\\ 1 & -2 \\end{bmatrix} \\begin{bmatrix} 5 & 0 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & -\\frac{1}{3} \\end{bmatrix}\\] \\[= \\begin{bmatrix} 5 & 2 \\\\ 5 & -4 \\end{bmatrix} \\begin{bmatrix} \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & -\\frac{1}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{10}{3} + \\frac{2}{3} & \\frac{5}{3} - \\frac{2}{3} \\\\ \\frac{10}{3} - \\frac{4}{3} & \\frac{5}{3} + \\frac{4}{3} \\end{bmatrix} = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix} = A \\quad \\checkmark\\] 5.4 Why Eigendecomposition is Useful \u00b6 Computing matrix powers: If \\(A = PDP^{-1}\\) , then: \\[A^k = PD^kP^{-1}\\] Since \\(D\\) is diagonal, \\(D^k\\) is just each diagonal entry raised to the \\(k\\) -th power: \\[D^k = \\begin{bmatrix} \\lambda_1^k & 0 \\\\ 0 & \\lambda_2^k \\end{bmatrix}\\] This makes computing \\(A^{100}\\) just as easy as computing \\(A^2\\) . Part 6: Singular Value Decomposition (SVD) \u00b6 6.1 Definition \u00b6 Every matrix \\(A\\) (of any shape) can be decomposed as: \\[A = U \\Sigma V^T\\] where: - \\(A\\) is \\(m \\times n\\) - \\(U\\) is \\(m \\times m\\) orthogonal matrix (columns are left singular vectors ) - \\(\\Sigma\\) is \\(m \\times n\\) diagonal matrix (diagonal entries are singular values \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq 0\\) ) - \\(V\\) is \\(n \\times n\\) orthogonal matrix (columns are right singular vectors ) This is the most important matrix decomposition in applied mathematics and machine learning. 6.2 Geometric Interpretation \u00b6 Any linear transformation \\(A\\) can be broken down into three simple steps: \\(V^T\\) : Rotate (in the input space) \\(\\Sigma\\) : Scale along each axis (possibly changing dimensions) \\(U\\) : Rotate (in the output space) In plain English: every matrix transformation is just a rotation, followed by a stretch, followed by another rotation. 6.3 Relationship to Eigendecomposition \u00b6 The singular values and vectors are connected to eigenvalues: SVD Component Obtained From \\(V\\) (right singular vectors) Eigenvectors of \\(A^T A\\) \\(U\\) (left singular vectors) Eigenvectors of \\(A A^T\\) \\(\\sigma_i\\) (singular values) \\(\\sigma_i = \\sqrt{\\lambda_i}\\) where \\(\\lambda_i\\) are eigenvalues of \\(A^T A\\) 6.4 Worked Example: Computing the SVD \u00b6 Find the SVD of: \\[A = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}\\] This is already a diagonal matrix, so the SVD is straightforward. Step 1: Compute \\(A^T A\\) . \\[A^T A = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 9 & 0 \\\\ 0 & 4 \\end{bmatrix}\\] Step 2: Find eigenvalues of \\(A^T A\\) . Eigenvalues: \\(\\lambda_1 = 9\\) , \\(\\lambda_2 = 4\\) . Singular values: \\(\\sigma_1 = \\sqrt{9} = 3\\) , \\(\\sigma_2 = \\sqrt{4} = 2\\) . Step 3: Find \\(V\\) (eigenvectors of \\(A^T A\\) ). \\[V = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I\\] Step 4: Find \\(U\\) (eigenvectors of \\(A A^T\\) ). \\[A A^T = \\begin{bmatrix} 9 & 0 \\\\ 0 & 4 \\end{bmatrix}\\] \\[U = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I\\] Result: \\[A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I \\Sigma I = \\Sigma\\] 6.5 Worked Example: Non-Diagonal Matrix \u00b6 Find the SVD of: \\[A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\] Step 1: Compute \\(A^T A\\) . \\[A^T A = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\end{bmatrix}\\] Step 2: Find eigenvalues of \\(A^T A\\) . \\[\\det(A^T A - \\lambda I) = (1 - \\lambda)(2 - \\lambda) - 1 = \\lambda^2 - 3\\lambda + 1 = 0\\] \\[\\lambda = \\frac{3 \\pm \\sqrt{5}}{2}\\] \\[\\lambda_1 = \\frac{3 + \\sqrt{5}}{2} \\approx 2.618, \\quad \\lambda_2 = \\frac{3 - \\sqrt{5}}{2} \\approx 0.382\\] Singular values: \\[\\sigma_1 = \\sqrt{\\frac{3 + \\sqrt{5}}{2}} \\approx 1.618, \\quad \\sigma_2 = \\sqrt{\\frac{3 - \\sqrt{5}}{2}} \\approx 0.618\\] (Notice: \\(\\sigma_1 \\approx \\phi\\) , the golden ratio, and \\(\\sigma_2 \\approx 1/\\phi\\) .) Step 3: Find \\(V\\) . For \\(\\lambda_1 = \\frac{3+\\sqrt{5}}{2}\\) , solve \\((A^TA - \\lambda_1 I)\\mathbf{v} = 0\\) : \\[\\begin{bmatrix} 1 - \\lambda_1 & 1 \\\\ 1 & 2 - \\lambda_1 \\end{bmatrix}\\mathbf{v} = 0\\] This gives \\(v_2 = (\\lambda_1 - 1) v_1\\) . Normalizing: \\[\\mathbf{v}_1 = \\frac{1}{\\sqrt{1 + (\\lambda_1 - 1)^2}}\\begin{bmatrix} 1 \\\\ \\lambda_1 - 1 \\end{bmatrix}\\] Similarly for \\(\\lambda_2\\) . Then \\(V = [\\mathbf{v}_1 \\mid \\mathbf{v}_2]\\) . Step 4: Find \\(U\\) . Compute \\(\\mathbf{u}_i = \\frac{1}{\\sigma_i} A \\mathbf{v}_i\\) for each singular vector. The full numerical result gives \\(A = U\\Sigma V^T\\) as desired. 6.6 Key Properties of SVD \u00b6 Property Statement Existence Every matrix has an SVD (unlike eigendecomposition) Rank \\(\\text{rank}(A) =\\) number of nonzero singular values Norm \\(\\|A\\|_2 = \\sigma_1\\) (largest singular value) Frobenius norm \\(\\|A\\|_F = \\sqrt{\\sigma_1^2 + \\sigma_2^2 + \\cdots + \\sigma_r^2}\\) Condition number \\(\\kappa(A) = \\sigma_1 / \\sigma_n\\) (ratio of largest to smallest) Part 7: Matrix Approximation \u00b6 7.1 Truncated SVD \u00b6 Given the full SVD \\(A = U\\Sigma V^T\\) with \\(r\\) nonzero singular values, we can approximate \\(A\\) by keeping only the \\(k\\) largest singular values (where \\(k < r\\) ): \\[A_k = U_k \\Sigma_k V_k^T = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T\\] where: - \\(U_k\\) is the first \\(k\\) columns of \\(U\\) - \\(\\Sigma_k\\) is the top-left \\(k \\times k\\) block of \\(\\Sigma\\) - \\(V_k\\) is the first \\(k\\) columns of \\(V\\) In plain English: keep the \\(k\\) most important \"layers\" of the matrix and throw away the rest. Each layer is a rank-1 matrix \\(\\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T\\) weighted by its singular value. 7.2 The Eckart-Young Theorem \u00b6 The truncated SVD gives the best rank- \\(k\\) approximation to \\(A\\) : \\[A_k = \\arg\\min_{\\text{rank}(B) \\leq k} \\|A - B\\|_F\\] In other words, among all matrices with rank at most \\(k\\) , \\(A_k\\) is the closest to \\(A\\) in the Frobenius norm. The approximation error is: \\[\\|A - A_k\\|_F = \\sqrt{\\sigma_{k+1}^2 + \\sigma_{k+2}^2 + \\cdots + \\sigma_r^2}\\] This is a remarkable result: the best low-rank approximation is obtained simply by discarding the smallest singular values. 7.3 Worked Example: Rank-1 Approximation \u00b6 Find the best rank-1 approximation to: \\[A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}\\] Step 1: Compute the SVD. \\[A^T A = \\begin{bmatrix} 10 & 6 \\\\ 6 & 10 \\end{bmatrix}\\] Eigenvalues of \\(A^T A\\) : \\(\\lambda_1 = 16\\) , \\(\\lambda_2 = 4\\) . Singular values: \\(\\sigma_1 = 4\\) , \\(\\sigma_2 = 2\\) . Eigenvectors of \\(A^T A\\) : For \\(\\lambda_1 = 16\\) : \\(\\mathbf{v}_1 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) For \\(\\lambda_2 = 4\\) : \\(\\mathbf{v}_2 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\) Left singular vectors: \\(\\mathbf{u}_i = \\frac{1}{\\sigma_i}A\\mathbf{v}_i\\) \\[\\mathbf{u}_1 = \\frac{1}{4} \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{4\\sqrt{2}} \\begin{bmatrix} 4 \\\\ 4 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\] \\[\\mathbf{u}_2 = \\frac{1}{2} \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\frac{1}{2\\sqrt{2}} \\begin{bmatrix} 2 \\\\ -2 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\] Step 2: Compute the rank-1 approximation. \\[A_1 = \\sigma_1 \\mathbf{u}_1 \\mathbf{v}_1^T = 4 \\cdot \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\cdot \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\end{bmatrix} = 4 \\cdot \\frac{1}{2}\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\\] Step 3: Check the error. \\[\\|A - A_1\\|_F = \\sigma_2 = 2\\] \\[A - A_1 = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} - \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix} = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\\] \\[\\|A - A_1\\|_F = \\sqrt{1 + 1 + 1 + 1} = \\sqrt{4} = 2 \\quad \\checkmark\\] 7.4 Applications of Low-Rank Approximation \u00b6 Application How Truncated SVD Helps Image compression An \\(m \\times n\\) image stored as a matrix can be approximated with rank \\(k\\) , requiring only \\(k(m + n + 1)\\) numbers instead of \\(mn\\) Dimensionality reduction (PCA) Principal Component Analysis keeps the top \\(k\\) singular vectors to reduce feature dimensions Recommender systems User-item rating matrices are approximated at low rank to predict missing ratings Noise reduction Small singular values often correspond to noise; discarding them denoises the data Latent semantic analysis In NLP, document-term matrices are approximated to find semantic structure Image compression example: Suppose you have a \\(1000 \\times 1000\\) grayscale image (1,000,000 pixel values). A rank-50 SVD approximation stores only \\(50 \\times (1000 + 1000 + 1) = 100{,}050\\) numbers, which is about 10% of the original, yet often captures the essential visual content. Summary: Key Takeaways \u00b6 Matrix Scalars \u00b6 Determinant \\(\\det(A)\\) : tells you invertibility and volume scaling Trace \\(\\text{tr}(A)\\) : sum of diagonal entries = sum of eigenvalues Eigenvalues and Eigenvectors \u00b6 \\(A\\mathbf{v} = \\lambda \\mathbf{v}\\) : eigenvectors are special directions, eigenvalues are scale factors Found by solving \\(\\det(A - \\lambda I) = 0\\) Matrix Decompositions \u00b6 Cholesky: \\(A = LL^T\\) for symmetric positive definite matrices Eigendecomposition: \\(A = PDP^{-1}\\) for diagonalizable square matrices SVD: \\(A = U\\Sigma V^T\\) for any matrix (the universal decomposition) Matrix Approximation \u00b6 Truncated SVD gives the best rank- \\(k\\) approximation (Eckart-Young theorem) Central to PCA, image compression, recommender systems, and denoising Practice Problems \u00b6 Problem 1 \u00b6 Compute the determinant: \\[A = \\begin{bmatrix} 1 & 3 & 2 \\\\ 4 & 1 & 3 \\\\ 2 & 5 & 2 \\end{bmatrix}\\] Problem 2 \u00b6 Let \\(A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 0 & 4 \\\\ 1 & 2 \\end{bmatrix}\\) . Verify that \\(\\text{tr}(AB) = \\text{tr}(BA)\\) . Problem 3 \u00b6 Find the eigenvalues and eigenvectors of: \\[C = \\begin{bmatrix} 5 & 4 \\\\ 2 & 3 \\end{bmatrix}\\] Problem 4 \u00b6 Find the Cholesky decomposition of: \\[A = \\begin{bmatrix} 9 & 6 \\\\ 6 & 8 \\end{bmatrix}\\] Problem 5 \u00b6 Diagonalize the matrix from Problem 3. That is, find \\(P\\) and \\(D\\) such that \\(C = PDP^{-1}\\) . Problem 6 \u00b6 The singular values of a \\(3 \\times 3\\) matrix \\(M\\) are \\(\\sigma_1 = 10\\) , \\(\\sigma_2 = 5\\) , \\(\\sigma_3 = 1\\) . (a) What is \\(\\|M\\|_F\\) ? (b) What is the Frobenius-norm error of the best rank-2 approximation \\(M_2\\) ? (c) What percentage of \\(\\|M\\|_F^2\\) is captured by \\(M_2\\) ? Solutions \u00b6 Solution 1: Using cofactor expansion along the first row: \\[\\det(A) = 1 \\begin{vmatrix} 1 & 3 \\\\ 5 & 2 \\end{vmatrix} - 3 \\begin{vmatrix} 4 & 3 \\\\ 2 & 2 \\end{vmatrix} + 2 \\begin{vmatrix} 4 & 1 \\\\ 2 & 5 \\end{vmatrix}\\] \\[= 1(1 \\cdot 2 - 3 \\cdot 5) - 3(4 \\cdot 2 - 3 \\cdot 2) + 2(4 \\cdot 5 - 1 \\cdot 2)\\] \\[= 1(2 - 15) - 3(8 - 6) + 2(20 - 2)\\] \\[= 1(-13) - 3(2) + 2(18)\\] \\[= -13 - 6 + 36 = 17\\] Solution 2: \\[AB = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\\begin{bmatrix} 0 & 4 \\\\ 1 & 2 \\end{bmatrix} = \\begin{bmatrix} 2(0)+1(1) & 2(4)+1(2) \\\\ 1(0)+3(1) & 1(4)+3(2) \\end{bmatrix} = \\begin{bmatrix} 1 & 10 \\\\ 3 & 10 \\end{bmatrix}\\] \\[\\text{tr}(AB) = 1 + 10 = 11\\] \\[BA = \\begin{bmatrix} 0 & 4 \\\\ 1 & 2 \\end{bmatrix}\\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix} = \\begin{bmatrix} 0(2)+4(1) & 0(1)+4(3) \\\\ 1(2)+2(1) & 1(1)+2(3) \\end{bmatrix} = \\begin{bmatrix} 4 & 12 \\\\ 4 & 7 \\end{bmatrix}\\] \\[\\text{tr}(BA) = 4 + 7 = 11\\] Both traces equal 11, confirming \\(\\text{tr}(AB) = \\text{tr}(BA)\\) . Solution 3: Characteristic polynomial: \\[\\det(C - \\lambda I) = (5 - \\lambda)(3 - \\lambda) - (4)(2) = \\lambda^2 - 8\\lambda + 15 - 8 = \\lambda^2 - 8\\lambda + 7\\] \\[(\\lambda - 7)(\\lambda - 1) = 0\\] \\[\\lambda_1 = 7, \\quad \\lambda_2 = 1\\] Eigenvector for \\(\\lambda_1 = 7\\) : \\[(C - 7I)\\mathbf{v} = \\begin{bmatrix} -2 & 4 \\\\ 2 & -4 \\end{bmatrix}\\mathbf{v} = \\mathbf{0}\\] From the first row: \\(-2v_1 + 4v_2 = 0\\) , so \\(v_1 = 2v_2\\) . Choosing \\(v_2 = 1\\) : \\[\\mathbf{v}_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\] Eigenvector for \\(\\lambda_2 = 1\\) : \\[(C - I)\\mathbf{v} = \\begin{bmatrix} 4 & 4 \\\\ 2 & 2 \\end{bmatrix}\\mathbf{v} = \\mathbf{0}\\] From the first row: \\(4v_1 + 4v_2 = 0\\) , so \\(v_1 = -v_2\\) . Choosing \\(v_2 = 1\\) : \\[\\mathbf{v}_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\] Check: \\(\\text{tr}(C) = 5 + 3 = 8 = 7 + 1\\) and \\(\\det(C) = 15 - 8 = 7 = 7 \\times 1\\) . Both pass. Solution 4: \\[l_{11} = \\sqrt{9} = 3\\] \\[l_{21} = \\frac{6}{3} = 2\\] \\[l_{22} = \\sqrt{8 - 2^2} = \\sqrt{4} = 2\\] \\[L = \\begin{bmatrix} 3 & 0 \\\\ 2 & 2 \\end{bmatrix}\\] Verification: \\[LL^T = \\begin{bmatrix} 3 & 0 \\\\ 2 & 2 \\end{bmatrix}\\begin{bmatrix} 3 & 2 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 9 & 6 \\\\ 6 & 8 \\end{bmatrix} = A \\quad \\checkmark\\] Solution 5: From Solution 3: \\(\\lambda_1 = 7\\) , \\(\\mathbf{v}_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\) , \\(\\lambda_2 = 1\\) , \\(\\mathbf{v}_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\) . \\[P = \\begin{bmatrix} 2 & -1 \\\\ 1 & 1 \\end{bmatrix}, \\quad D = \\begin{bmatrix} 7 & 0 \\\\ 0 & 1 \\end{bmatrix}\\] \\[P^{-1} = \\frac{1}{2(1) - (-1)(1)} \\begin{bmatrix} 1 & 1 \\\\ -1 & 2 \\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix} 1 & 1 \\\\ -1 & 2 \\end{bmatrix}\\] Verification: \\[PDP^{-1} = \\begin{bmatrix} 2 & -1 \\\\ 1 & 1 \\end{bmatrix}\\begin{bmatrix} 7 & 0 \\\\ 0 & 1 \\end{bmatrix} \\cdot \\frac{1}{3}\\begin{bmatrix} 1 & 1 \\\\ -1 & 2 \\end{bmatrix}\\] \\[= \\begin{bmatrix} 14 & -1 \\\\ 7 & 1 \\end{bmatrix} \\cdot \\frac{1}{3}\\begin{bmatrix} 1 & 1 \\\\ -1 & 2 \\end{bmatrix}\\] \\[= \\frac{1}{3}\\begin{bmatrix} 14(1)+(-1)(-1) & 14(1)+(-1)(2) \\\\ 7(1)+1(-1) & 7(1)+1(2) \\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix} 15 & 12 \\\\ 6 & 9 \\end{bmatrix} = \\begin{bmatrix} 5 & 4 \\\\ 2 & 3 \\end{bmatrix} = C \\quad \\checkmark\\] Solution 6: (a) The Frobenius norm is: \\[\\|M\\|_F = \\sqrt{\\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2} = \\sqrt{100 + 25 + 1} = \\sqrt{126} = 3\\sqrt{14} \\approx 11.22\\] (b) The best rank-2 approximation discards \\(\\sigma_3 = 1\\) . The error is: \\[\\|M - M_2\\|_F = \\sqrt{\\sigma_3^2} = \\sigma_3 = 1\\] (c) The squared Frobenius norm captured by \\(M_2\\) is: \\[\\frac{\\sigma_1^2 + \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2} = \\frac{100 + 25}{100 + 25 + 1} = \\frac{125}{126} \\approx 99.2\\%\\] So the rank-2 approximation captures about 99.2% of the total \"energy\" of \\(M\\) . Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Next: Tutorial 4 - Matrix Calculus and Optimization","title":"Tutorial 3: Matrix Decomposition"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#tutorial-3-matrix-decomposition","text":"Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari","title":"Tutorial 3: Matrix Decomposition"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#learning-objectives","text":"By the end of this tutorial, you will understand: How to compute determinants for 2x2 and 3x3 matrices The trace of a matrix and its properties Eigenvalues and eigenvectors and how to compute them Cholesky decomposition for symmetric positive definite matrices Eigendecomposition and diagonalization Singular Value Decomposition (SVD) and its geometric meaning Matrix approximation using truncated SVD","title":"\ud83d\udcda Learning Objectives"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#part-1-determinants","text":"","title":"Part 1: Determinants"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#11-what-is-a-determinant","text":"The determinant is a scalar value computed from a square matrix that captures important information about the matrix. Think of it as a single number that tells you: Whether the matrix is invertible (determinant is nonzero) How the matrix scales areas or volumes when used as a linear transformation The \"signed volume\" of the parallelepiped formed by the column vectors Notation: For a matrix \\(A\\) , the determinant is written as \\(\\det(A)\\) or \\(|A|\\) .","title":"1.1 What is a Determinant?"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#12-determinant-of-a-2x2-matrix","text":"For a \\(2 \\times 2\\) matrix: \\[A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\] The determinant is: \\[\\det(A) = ad - bc\\] In plain English: multiply the diagonals and subtract. The main diagonal product minus the off-diagonal product. Worked Example: \\[A = \\begin{bmatrix} 3 & 7 \\\\ 1 & 5 \\end{bmatrix}\\] \\[\\det(A) = 3(5) - 7(1) = 15 - 7 = 8\\] Since \\(\\det(A) = 8 \\neq 0\\) , the matrix \\(A\\) is invertible.","title":"1.2 Determinant of a 2x2 Matrix"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#13-determinant-of-a-3x3-matrix","text":"For a \\(3 \\times 3\\) matrix: \\[A = \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix}\\]","title":"1.3 Determinant of a 3x3 Matrix"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#method-1-sarrus-rule","text":"Write the matrix and repeat the first two columns to the right: \\[\\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} \\begin{matrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{matrix}\\] Then sum the products along the three downward diagonals and subtract the products along the three upward diagonals: \\[\\det(A) = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32}\\]","title":"Method 1: Sarrus' Rule"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#method-2-cofactor-expansion-along-the-first-row","text":"\\[\\det(A) = a_{11} \\begin{vmatrix} a_{22} & a_{23} \\\\ a_{32} & a_{33} \\end{vmatrix} - a_{12} \\begin{vmatrix} a_{21} & a_{23} \\\\ a_{31} & a_{33} \\end{vmatrix} + a_{13} \\begin{vmatrix} a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{vmatrix}\\] Each smaller determinant is called a minor , and the signed minor is a cofactor . Worked Example: \\[B = \\begin{bmatrix} 2 & 1 & 3 \\\\ 0 & 4 & 5 \\\\ 1 & 0 & 2 \\end{bmatrix}\\] Using cofactor expansion along the first row: \\[\\det(B) = 2 \\begin{vmatrix} 4 & 5 \\\\ 0 & 2 \\end{vmatrix} - 1 \\begin{vmatrix} 0 & 5 \\\\ 1 & 2 \\end{vmatrix} + 3 \\begin{vmatrix} 0 & 4 \\\\ 1 & 0 \\end{vmatrix}\\] \\[= 2(4 \\cdot 2 - 5 \\cdot 0) - 1(0 \\cdot 2 - 5 \\cdot 1) + 3(0 \\cdot 0 - 4 \\cdot 1)\\] \\[= 2(8) - 1(-5) + 3(-4)\\] \\[= 16 + 5 - 12 = 9\\]","title":"Method 2: Cofactor Expansion (along the first row)"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#14-properties-of-determinants","text":"Property Statement Example / Note Identity \\(\\det(I) = 1\\) The identity matrix always has determinant 1 Transpose \\(\\det(A^T) = \\det(A)\\) Transposing does not change the determinant Product \\(\\det(AB) = \\det(A) \\cdot \\det(B)\\) Determinant of a product is the product of determinants Inverse \\(\\det(A^{-1}) = \\frac{1}{\\det(A)}\\) Only defined when \\(\\det(A) \\neq 0\\) Scalar multiple \\(\\det(cA) = c^n \\det(A)\\) For an \\(n \\times n\\) matrix Singular matrix \\(\\det(A) = 0\\) Matrix is not invertible Row swap Swapping two rows flips the sign \\(\\det(\\text{swapped}) = -\\det(A)\\) Triangular \\(\\det(A) = \\prod_{i=1}^{n} a_{ii}\\) Product of diagonal entries for triangular matrices","title":"1.4 Properties of Determinants"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#part-2-trace","text":"","title":"Part 2: Trace"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#21-definition","text":"The trace of a square matrix \\(A\\) is the sum of its diagonal entries: \\[\\text{tr}(A) = \\sum_{i=1}^{n} a_{ii} = a_{11} + a_{22} + \\cdots + a_{nn}\\] In plain English: just add up all the numbers on the main diagonal. Example: \\[A = \\begin{bmatrix} 5 & 2 & 1 \\\\ 0 & 3 & 4 \\\\ 7 & 6 & 8 \\end{bmatrix}\\] \\[\\text{tr}(A) = 5 + 3 + 8 = 16\\]","title":"2.1 Definition"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#22-properties-of-the-trace","text":"Property Statement Linearity \\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\) Scalar multiplication \\(\\text{tr}(cA) = c \\cdot \\text{tr}(A)\\) Transpose \\(\\text{tr}(A^T) = \\text{tr}(A)\\) Cyclic property \\(\\text{tr}(AB) = \\text{tr}(BA)\\) Cyclic property (3 matrices) \\(\\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)\\) Sum of eigenvalues \\(\\text{tr}(A) = \\sum_{i=1}^{n} \\lambda_i\\) Frobenius norm \\(\\text{tr}(A^T A) = \\sum_{i,j} a_{ij}^2 = \\|A\\|_F^2\\) The cyclic property \\(\\text{tr}(AB) = \\text{tr}(BA)\\) is especially important in machine learning. It lets you rearrange matrix products inside a trace, which simplifies many derivations in optimization and statistics. Worked Example: Verify \\(\\text{tr}(AB) = \\text{tr}(BA)\\) . \\[A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\\] \\[AB = \\begin{bmatrix} 1(5)+2(7) & 1(6)+2(8) \\\\ 3(5)+4(7) & 3(6)+4(8) \\end{bmatrix} = \\begin{bmatrix} 19 & 22 \\\\ 43 & 50 \\end{bmatrix}\\] \\[\\text{tr}(AB) = 19 + 50 = 69\\] \\[BA = \\begin{bmatrix} 5(1)+6(3) & 5(2)+6(4) \\\\ 7(1)+8(3) & 7(2)+8(4) \\end{bmatrix} = \\begin{bmatrix} 23 & 34 \\\\ 31 & 46 \\end{bmatrix}\\] \\[\\text{tr}(BA) = 23 + 46 = 69\\] Both sides give 69, confirming the property.","title":"2.2 Properties of the Trace"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#part-3-eigenvalues-and-eigenvectors","text":"","title":"Part 3: Eigenvalues and Eigenvectors"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#31-definition","text":"Given a square matrix \\(A\\) , a nonzero vector \\(\\mathbf{v}\\) is an eigenvector of \\(A\\) if multiplying \\(A\\) by \\(\\mathbf{v}\\) simply scales \\(\\mathbf{v}\\) by some scalar \\(\\lambda\\) : \\[A\\mathbf{v} = \\lambda \\mathbf{v}\\] \\(\\lambda\\) is called the eigenvalue corresponding to \\(\\mathbf{v}\\) \\(\\mathbf{v}\\) is the eigenvector corresponding to \\(\\lambda\\) In plain English: an eigenvector is a special direction that the matrix only stretches (or flips), but does not rotate. The eigenvalue tells you by how much it stretches.","title":"3.1 Definition"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#32-the-characteristic-polynomial","text":"To find eigenvalues, we rearrange \\(A\\mathbf{v} = \\lambda \\mathbf{v}\\) : \\[A\\mathbf{v} - \\lambda \\mathbf{v} = \\mathbf{0}\\] \\[(A - \\lambda I)\\mathbf{v} = \\mathbf{0}\\] For a nonzero solution \\(\\mathbf{v}\\) to exist, the matrix \\((A - \\lambda I)\\) must be singular, meaning: \\[\\det(A - \\lambda I) = 0\\] This equation is called the characteristic equation , and the polynomial on the left side is the characteristic polynomial . Solving it gives us the eigenvalues.","title":"3.2 The Characteristic Polynomial"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#33-worked-example-finding-eigenvalues-and-eigenvectors","text":"Find the eigenvalues and eigenvectors of: \\[A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix}\\] Step 1: Characteristic polynomial. \\[A - \\lambda I = \\begin{bmatrix} 4 - \\lambda & 1 \\\\ 2 & 3 - \\lambda \\end{bmatrix}\\] \\[\\det(A - \\lambda I) = (4 - \\lambda)(3 - \\lambda) - (1)(2)\\] \\[= 12 - 4\\lambda - 3\\lambda + \\lambda^2 - 2\\] \\[= \\lambda^2 - 7\\lambda + 10\\] Step 2: Solve the characteristic equation. \\[\\lambda^2 - 7\\lambda + 10 = 0\\] \\[(\\lambda - 5)(\\lambda - 2) = 0\\] \\[\\lambda_1 = 5, \\quad \\lambda_2 = 2\\] Step 3: Find eigenvectors for each eigenvalue. For \\(\\lambda_1 = 5\\) : \\[(A - 5I)\\mathbf{v} = \\mathbf{0}\\] \\[\\begin{bmatrix} -1 & 1 \\\\ 2 & -2 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\] From the first row: \\(-v_1 + v_2 = 0\\) , so \\(v_2 = v_1\\) . Choosing \\(v_1 = 1\\) : \\[\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\] For \\(\\lambda_2 = 2\\) : \\[(A - 2I)\\mathbf{v} = \\mathbf{0}\\] \\[\\begin{bmatrix} 2 & 1 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\] From the first row: \\(2v_1 + v_2 = 0\\) , so \\(v_2 = -2v_1\\) . Choosing \\(v_1 = 1\\) : \\[\\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}\\] Quick check: \\(\\text{tr}(A) = 4 + 3 = 7 = 5 + 2 = \\lambda_1 + \\lambda_2\\) and \\(\\det(A) = 4(3) - 1(2) = 10 = 5 \\times 2 = \\lambda_1 \\cdot \\lambda_2\\) . Both checks pass.","title":"3.3 Worked Example: Finding Eigenvalues and Eigenvectors"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#34-key-facts-about-eigenvalues","text":"Fact Statement Sum of eigenvalues \\(\\sum \\lambda_i = \\text{tr}(A)\\) Product of eigenvalues \\(\\prod \\lambda_i = \\det(A)\\) Symmetric matrices All eigenvalues are real; eigenvectors are orthogonal Positive definite All eigenvalues are strictly positive Singular matrix At least one eigenvalue is zero","title":"3.4 Key Facts About Eigenvalues"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#part-4-cholesky-decomposition","text":"","title":"Part 4: Cholesky Decomposition"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#41-what-is-cholesky-decomposition","text":"For a symmetric positive definite (SPD) matrix \\(A\\) , the Cholesky decomposition factors \\(A\\) into: \\[A = LL^T\\] where \\(L\\) is a lower triangular matrix with positive diagonal entries. Think of it as the \"square root\" of a matrix. Just as every positive number has a square root, every SPD matrix has a Cholesky factor.","title":"4.1 What is Cholesky Decomposition?"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#42-what-does-symmetric-positive-definite-mean","text":"A matrix \\(A\\) is symmetric positive definite if: Symmetric: \\(A = A^T\\) (the matrix equals its transpose) Positive definite: \\(\\mathbf{x}^T A \\mathbf{x} > 0\\) for all nonzero vectors \\(\\mathbf{x}\\) Equivalently, \\(A\\) is SPD if it is symmetric and all its eigenvalues are strictly positive.","title":"4.2 What Does \"Symmetric Positive Definite\" Mean?"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#43-the-cholesky-algorithm-for-2x2","text":"Given: \\[A = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{12} & a_{22} \\end{bmatrix} = \\begin{bmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{bmatrix} \\begin{bmatrix} l_{11} & l_{21} \\\\ 0 & l_{22} \\end{bmatrix}\\] Expanding the right side and matching entries: \\[l_{11} = \\sqrt{a_{11}}\\] \\[l_{21} = \\frac{a_{12}}{l_{11}}\\] \\[l_{22} = \\sqrt{a_{22} - l_{21}^2}\\]","title":"4.3 The Cholesky Algorithm (for 2x2)"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#44-the-cholesky-algorithm-general","text":"For an \\(n \\times n\\) SPD matrix, the entries of \\(L\\) are computed as: \\[l_{jj} = \\sqrt{a_{jj} - \\sum_{k=1}^{j-1} l_{jk}^2}\\] \\[l_{ij} = \\frac{1}{l_{jj}} \\left( a_{ij} - \\sum_{k=1}^{j-1} l_{ik} l_{jk} \\right), \\quad \\text{for } i > j\\]","title":"4.4 The Cholesky Algorithm (general)"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#45-worked-example","text":"Find the Cholesky decomposition of: \\[A = \\begin{bmatrix} 4 & 2 \\\\ 2 & 5 \\end{bmatrix}\\] Step 1: Check that \\(A\\) is SPD. It is symmetric ( \\(A = A^T\\) ). Its eigenvalues are \\(\\lambda = \\frac{9 \\pm \\sqrt{1}}{2}\\) , giving \\(\\lambda_1 \\approx 5.56\\) and \\(\\lambda_2 \\approx 3.44\\) , both positive. (Alternatively, \\(\\det(A) = 16 > 0\\) and \\(a_{11} = 4 > 0\\) .) Step 2: Compute \\(L\\) . \\[l_{11} = \\sqrt{4} = 2\\] \\[l_{21} = \\frac{2}{2} = 1\\] \\[l_{22} = \\sqrt{5 - 1^2} = \\sqrt{4} = 2\\] Result: \\[L = \\begin{bmatrix} 2 & 0 \\\\ 1 & 2 \\end{bmatrix}\\] Verification: \\[LL^T = \\begin{bmatrix} 2 & 0 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 4 & 2 \\\\ 2 & 5 \\end{bmatrix} = A \\quad \\checkmark\\]","title":"4.5 Worked Example"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#46-why-cholesky-decomposition-matters","text":"Application How It Helps Solving linear systems \\(Ax = b\\) becomes two easy triangular solves: \\(Ly = b\\) , then \\(L^Tx = y\\) Sampling from multivariate Gaussians If \\(\\Sigma = LL^T\\) , generate \\(\\mathbf{x} = L\\mathbf{z} + \\boldsymbol{\\mu}\\) where \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\) Numerical stability More stable and about twice as fast as general LU decomposition for SPD matrices","title":"4.6 Why Cholesky Decomposition Matters"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#part-5-eigendecomposition","text":"","title":"Part 5: Eigendecomposition"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#51-definition","text":"If a square matrix \\(A\\) has \\(n\\) linearly independent eigenvectors, then \\(A\\) can be factored as: \\[A = PDP^{-1}\\] where: - \\(P\\) is the matrix whose columns are the eigenvectors of \\(A\\) : \\(P = [\\mathbf{v}_1 \\mid \\mathbf{v}_2 \\mid \\cdots \\mid \\mathbf{v}_n]\\) - \\(D\\) is a diagonal matrix with the corresponding eigenvalues on the diagonal: \\[D = \\begin{bmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{bmatrix}\\] This factorization is called the eigendecomposition or spectral decomposition (for symmetric matrices).","title":"5.1 Definition"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#52-when-does-eigendecomposition-exist","text":"Condition Eigendecomposition Exists? \\(A\\) has \\(n\\) distinct eigenvalues Always yes \\(A\\) is symmetric ( \\(A = A^T\\) ) Always yes, and \\(P\\) is orthogonal ( \\(P^{-1} = P^T\\) ) \\(A\\) has repeated eigenvalues Sometimes (depends on geometric multiplicity) \\(A\\) is defective (not enough independent eigenvectors) No For symmetric matrices , the decomposition simplifies to: \\[A = PDP^T\\] because the eigenvectors are orthogonal.","title":"5.2 When Does Eigendecomposition Exist?"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#53-worked-example","text":"Diagonalize the matrix from Part 3: \\[A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix}\\] We already found \\(\\lambda_1 = 5\\) , \\(\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) and \\(\\lambda_2 = 2\\) , \\(\\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}\\) . \\[P = \\begin{bmatrix} 1 & 1 \\\\ 1 & -2 \\end{bmatrix}, \\quad D = \\begin{bmatrix} 5 & 0 \\\\ 0 & 2 \\end{bmatrix}\\] Compute \\(P^{-1}\\) . For a \\(2 \\times 2\\) matrix \\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) , the inverse is \\(\\frac{1}{ad - bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\) : \\[P^{-1} = \\frac{1}{(1)(-2) - (1)(1)} \\begin{bmatrix} -2 & -1 \\\\ -1 & 1 \\end{bmatrix} = \\frac{1}{-3} \\begin{bmatrix} -2 & -1 \\\\ -1 & 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & -\\frac{1}{3} \\end{bmatrix}\\] Verification: \\[PDP^{-1} = \\begin{bmatrix} 1 & 1 \\\\ 1 & -2 \\end{bmatrix} \\begin{bmatrix} 5 & 0 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & -\\frac{1}{3} \\end{bmatrix}\\] \\[= \\begin{bmatrix} 5 & 2 \\\\ 5 & -4 \\end{bmatrix} \\begin{bmatrix} \\frac{2}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & -\\frac{1}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{10}{3} + \\frac{2}{3} & \\frac{5}{3} - \\frac{2}{3} \\\\ \\frac{10}{3} - \\frac{4}{3} & \\frac{5}{3} + \\frac{4}{3} \\end{bmatrix} = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix} = A \\quad \\checkmark\\]","title":"5.3 Worked Example"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#54-why-eigendecomposition-is-useful","text":"Computing matrix powers: If \\(A = PDP^{-1}\\) , then: \\[A^k = PD^kP^{-1}\\] Since \\(D\\) is diagonal, \\(D^k\\) is just each diagonal entry raised to the \\(k\\) -th power: \\[D^k = \\begin{bmatrix} \\lambda_1^k & 0 \\\\ 0 & \\lambda_2^k \\end{bmatrix}\\] This makes computing \\(A^{100}\\) just as easy as computing \\(A^2\\) .","title":"5.4 Why Eigendecomposition is Useful"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#part-6-singular-value-decomposition-svd","text":"","title":"Part 6: Singular Value Decomposition (SVD)"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#61-definition","text":"Every matrix \\(A\\) (of any shape) can be decomposed as: \\[A = U \\Sigma V^T\\] where: - \\(A\\) is \\(m \\times n\\) - \\(U\\) is \\(m \\times m\\) orthogonal matrix (columns are left singular vectors ) - \\(\\Sigma\\) is \\(m \\times n\\) diagonal matrix (diagonal entries are singular values \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq 0\\) ) - \\(V\\) is \\(n \\times n\\) orthogonal matrix (columns are right singular vectors ) This is the most important matrix decomposition in applied mathematics and machine learning.","title":"6.1 Definition"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#62-geometric-interpretation","text":"Any linear transformation \\(A\\) can be broken down into three simple steps: \\(V^T\\) : Rotate (in the input space) \\(\\Sigma\\) : Scale along each axis (possibly changing dimensions) \\(U\\) : Rotate (in the output space) In plain English: every matrix transformation is just a rotation, followed by a stretch, followed by another rotation.","title":"6.2 Geometric Interpretation"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#63-relationship-to-eigendecomposition","text":"The singular values and vectors are connected to eigenvalues: SVD Component Obtained From \\(V\\) (right singular vectors) Eigenvectors of \\(A^T A\\) \\(U\\) (left singular vectors) Eigenvectors of \\(A A^T\\) \\(\\sigma_i\\) (singular values) \\(\\sigma_i = \\sqrt{\\lambda_i}\\) where \\(\\lambda_i\\) are eigenvalues of \\(A^T A\\)","title":"6.3 Relationship to Eigendecomposition"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#64-worked-example-computing-the-svd","text":"Find the SVD of: \\[A = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix}\\] This is already a diagonal matrix, so the SVD is straightforward. Step 1: Compute \\(A^T A\\) . \\[A^T A = \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 9 & 0 \\\\ 0 & 4 \\end{bmatrix}\\] Step 2: Find eigenvalues of \\(A^T A\\) . Eigenvalues: \\(\\lambda_1 = 9\\) , \\(\\lambda_2 = 4\\) . Singular values: \\(\\sigma_1 = \\sqrt{9} = 3\\) , \\(\\sigma_2 = \\sqrt{4} = 2\\) . Step 3: Find \\(V\\) (eigenvectors of \\(A^T A\\) ). \\[V = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I\\] Step 4: Find \\(U\\) (eigenvectors of \\(A A^T\\) ). \\[A A^T = \\begin{bmatrix} 9 & 0 \\\\ 0 & 4 \\end{bmatrix}\\] \\[U = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I\\] Result: \\[A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 3 & 0 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I \\Sigma I = \\Sigma\\]","title":"6.4 Worked Example: Computing the SVD"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#65-worked-example-non-diagonal-matrix","text":"Find the SVD of: \\[A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\] Step 1: Compute \\(A^T A\\) . \\[A^T A = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\end{bmatrix}\\] Step 2: Find eigenvalues of \\(A^T A\\) . \\[\\det(A^T A - \\lambda I) = (1 - \\lambda)(2 - \\lambda) - 1 = \\lambda^2 - 3\\lambda + 1 = 0\\] \\[\\lambda = \\frac{3 \\pm \\sqrt{5}}{2}\\] \\[\\lambda_1 = \\frac{3 + \\sqrt{5}}{2} \\approx 2.618, \\quad \\lambda_2 = \\frac{3 - \\sqrt{5}}{2} \\approx 0.382\\] Singular values: \\[\\sigma_1 = \\sqrt{\\frac{3 + \\sqrt{5}}{2}} \\approx 1.618, \\quad \\sigma_2 = \\sqrt{\\frac{3 - \\sqrt{5}}{2}} \\approx 0.618\\] (Notice: \\(\\sigma_1 \\approx \\phi\\) , the golden ratio, and \\(\\sigma_2 \\approx 1/\\phi\\) .) Step 3: Find \\(V\\) . For \\(\\lambda_1 = \\frac{3+\\sqrt{5}}{2}\\) , solve \\((A^TA - \\lambda_1 I)\\mathbf{v} = 0\\) : \\[\\begin{bmatrix} 1 - \\lambda_1 & 1 \\\\ 1 & 2 - \\lambda_1 \\end{bmatrix}\\mathbf{v} = 0\\] This gives \\(v_2 = (\\lambda_1 - 1) v_1\\) . Normalizing: \\[\\mathbf{v}_1 = \\frac{1}{\\sqrt{1 + (\\lambda_1 - 1)^2}}\\begin{bmatrix} 1 \\\\ \\lambda_1 - 1 \\end{bmatrix}\\] Similarly for \\(\\lambda_2\\) . Then \\(V = [\\mathbf{v}_1 \\mid \\mathbf{v}_2]\\) . Step 4: Find \\(U\\) . Compute \\(\\mathbf{u}_i = \\frac{1}{\\sigma_i} A \\mathbf{v}_i\\) for each singular vector. The full numerical result gives \\(A = U\\Sigma V^T\\) as desired.","title":"6.5 Worked Example: Non-Diagonal Matrix"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#66-key-properties-of-svd","text":"Property Statement Existence Every matrix has an SVD (unlike eigendecomposition) Rank \\(\\text{rank}(A) =\\) number of nonzero singular values Norm \\(\\|A\\|_2 = \\sigma_1\\) (largest singular value) Frobenius norm \\(\\|A\\|_F = \\sqrt{\\sigma_1^2 + \\sigma_2^2 + \\cdots + \\sigma_r^2}\\) Condition number \\(\\kappa(A) = \\sigma_1 / \\sigma_n\\) (ratio of largest to smallest)","title":"6.6 Key Properties of SVD"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#part-7-matrix-approximation","text":"","title":"Part 7: Matrix Approximation"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#71-truncated-svd","text":"Given the full SVD \\(A = U\\Sigma V^T\\) with \\(r\\) nonzero singular values, we can approximate \\(A\\) by keeping only the \\(k\\) largest singular values (where \\(k < r\\) ): \\[A_k = U_k \\Sigma_k V_k^T = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T\\] where: - \\(U_k\\) is the first \\(k\\) columns of \\(U\\) - \\(\\Sigma_k\\) is the top-left \\(k \\times k\\) block of \\(\\Sigma\\) - \\(V_k\\) is the first \\(k\\) columns of \\(V\\) In plain English: keep the \\(k\\) most important \"layers\" of the matrix and throw away the rest. Each layer is a rank-1 matrix \\(\\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T\\) weighted by its singular value.","title":"7.1 Truncated SVD"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#72-the-eckart-young-theorem","text":"The truncated SVD gives the best rank- \\(k\\) approximation to \\(A\\) : \\[A_k = \\arg\\min_{\\text{rank}(B) \\leq k} \\|A - B\\|_F\\] In other words, among all matrices with rank at most \\(k\\) , \\(A_k\\) is the closest to \\(A\\) in the Frobenius norm. The approximation error is: \\[\\|A - A_k\\|_F = \\sqrt{\\sigma_{k+1}^2 + \\sigma_{k+2}^2 + \\cdots + \\sigma_r^2}\\] This is a remarkable result: the best low-rank approximation is obtained simply by discarding the smallest singular values.","title":"7.2 The Eckart-Young Theorem"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#73-worked-example-rank-1-approximation","text":"Find the best rank-1 approximation to: \\[A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}\\] Step 1: Compute the SVD. \\[A^T A = \\begin{bmatrix} 10 & 6 \\\\ 6 & 10 \\end{bmatrix}\\] Eigenvalues of \\(A^T A\\) : \\(\\lambda_1 = 16\\) , \\(\\lambda_2 = 4\\) . Singular values: \\(\\sigma_1 = 4\\) , \\(\\sigma_2 = 2\\) . Eigenvectors of \\(A^T A\\) : For \\(\\lambda_1 = 16\\) : \\(\\mathbf{v}_1 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) For \\(\\lambda_2 = 4\\) : \\(\\mathbf{v}_2 = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\) Left singular vectors: \\(\\mathbf{u}_i = \\frac{1}{\\sigma_i}A\\mathbf{v}_i\\) \\[\\mathbf{u}_1 = \\frac{1}{4} \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{4\\sqrt{2}} \\begin{bmatrix} 4 \\\\ 4 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\] \\[\\mathbf{u}_2 = \\frac{1}{2} \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\frac{1}{2\\sqrt{2}} \\begin{bmatrix} 2 \\\\ -2 \\end{bmatrix} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\] Step 2: Compute the rank-1 approximation. \\[A_1 = \\sigma_1 \\mathbf{u}_1 \\mathbf{v}_1^T = 4 \\cdot \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\cdot \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\end{bmatrix} = 4 \\cdot \\frac{1}{2}\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix}\\] Step 3: Check the error. \\[\\|A - A_1\\|_F = \\sigma_2 = 2\\] \\[A - A_1 = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} - \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix} = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}\\] \\[\\|A - A_1\\|_F = \\sqrt{1 + 1 + 1 + 1} = \\sqrt{4} = 2 \\quad \\checkmark\\]","title":"7.3 Worked Example: Rank-1 Approximation"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#74-applications-of-low-rank-approximation","text":"Application How Truncated SVD Helps Image compression An \\(m \\times n\\) image stored as a matrix can be approximated with rank \\(k\\) , requiring only \\(k(m + n + 1)\\) numbers instead of \\(mn\\) Dimensionality reduction (PCA) Principal Component Analysis keeps the top \\(k\\) singular vectors to reduce feature dimensions Recommender systems User-item rating matrices are approximated at low rank to predict missing ratings Noise reduction Small singular values often correspond to noise; discarding them denoises the data Latent semantic analysis In NLP, document-term matrices are approximated to find semantic structure Image compression example: Suppose you have a \\(1000 \\times 1000\\) grayscale image (1,000,000 pixel values). A rank-50 SVD approximation stores only \\(50 \\times (1000 + 1000 + 1) = 100{,}050\\) numbers, which is about 10% of the original, yet often captures the essential visual content.","title":"7.4 Applications of Low-Rank Approximation"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#summary-key-takeaways","text":"","title":"Summary: Key Takeaways"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#matrix-scalars","text":"Determinant \\(\\det(A)\\) : tells you invertibility and volume scaling Trace \\(\\text{tr}(A)\\) : sum of diagonal entries = sum of eigenvalues","title":"Matrix Scalars"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#eigenvalues-and-eigenvectors","text":"\\(A\\mathbf{v} = \\lambda \\mathbf{v}\\) : eigenvectors are special directions, eigenvalues are scale factors Found by solving \\(\\det(A - \\lambda I) = 0\\)","title":"Eigenvalues and Eigenvectors"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#matrix-decompositions","text":"Cholesky: \\(A = LL^T\\) for symmetric positive definite matrices Eigendecomposition: \\(A = PDP^{-1}\\) for diagonalizable square matrices SVD: \\(A = U\\Sigma V^T\\) for any matrix (the universal decomposition)","title":"Matrix Decompositions"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#matrix-approximation","text":"Truncated SVD gives the best rank- \\(k\\) approximation (Eckart-Young theorem) Central to PCA, image compression, recommender systems, and denoising","title":"Matrix Approximation"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#practice-problems","text":"","title":"Practice Problems"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#problem-1","text":"Compute the determinant: \\[A = \\begin{bmatrix} 1 & 3 & 2 \\\\ 4 & 1 & 3 \\\\ 2 & 5 & 2 \\end{bmatrix}\\]","title":"Problem 1"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#problem-2","text":"Let \\(A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\\) and \\(B = \\begin{bmatrix} 0 & 4 \\\\ 1 & 2 \\end{bmatrix}\\) . Verify that \\(\\text{tr}(AB) = \\text{tr}(BA)\\) .","title":"Problem 2"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#problem-3","text":"Find the eigenvalues and eigenvectors of: \\[C = \\begin{bmatrix} 5 & 4 \\\\ 2 & 3 \\end{bmatrix}\\]","title":"Problem 3"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#problem-4","text":"Find the Cholesky decomposition of: \\[A = \\begin{bmatrix} 9 & 6 \\\\ 6 & 8 \\end{bmatrix}\\]","title":"Problem 4"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#problem-5","text":"Diagonalize the matrix from Problem 3. That is, find \\(P\\) and \\(D\\) such that \\(C = PDP^{-1}\\) .","title":"Problem 5"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#problem-6","text":"The singular values of a \\(3 \\times 3\\) matrix \\(M\\) are \\(\\sigma_1 = 10\\) , \\(\\sigma_2 = 5\\) , \\(\\sigma_3 = 1\\) . (a) What is \\(\\|M\\|_F\\) ? (b) What is the Frobenius-norm error of the best rank-2 approximation \\(M_2\\) ? (c) What percentage of \\(\\|M\\|_F^2\\) is captured by \\(M_2\\) ?","title":"Problem 6"},{"location":"tutorials/Tutorial_04_Matrix_Decomposition/#solutions","text":"Solution 1: Using cofactor expansion along the first row: \\[\\det(A) = 1 \\begin{vmatrix} 1 & 3 \\\\ 5 & 2 \\end{vmatrix} - 3 \\begin{vmatrix} 4 & 3 \\\\ 2 & 2 \\end{vmatrix} + 2 \\begin{vmatrix} 4 & 1 \\\\ 2 & 5 \\end{vmatrix}\\] \\[= 1(1 \\cdot 2 - 3 \\cdot 5) - 3(4 \\cdot 2 - 3 \\cdot 2) + 2(4 \\cdot 5 - 1 \\cdot 2)\\] \\[= 1(2 - 15) - 3(8 - 6) + 2(20 - 2)\\] \\[= 1(-13) - 3(2) + 2(18)\\] \\[= -13 - 6 + 36 = 17\\] Solution 2: \\[AB = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\\begin{bmatrix} 0 & 4 \\\\ 1 & 2 \\end{bmatrix} = \\begin{bmatrix} 2(0)+1(1) & 2(4)+1(2) \\\\ 1(0)+3(1) & 1(4)+3(2) \\end{bmatrix} = \\begin{bmatrix} 1 & 10 \\\\ 3 & 10 \\end{bmatrix}\\] \\[\\text{tr}(AB) = 1 + 10 = 11\\] \\[BA = \\begin{bmatrix} 0 & 4 \\\\ 1 & 2 \\end{bmatrix}\\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix} = \\begin{bmatrix} 0(2)+4(1) & 0(1)+4(3) \\\\ 1(2)+2(1) & 1(1)+2(3) \\end{bmatrix} = \\begin{bmatrix} 4 & 12 \\\\ 4 & 7 \\end{bmatrix}\\] \\[\\text{tr}(BA) = 4 + 7 = 11\\] Both traces equal 11, confirming \\(\\text{tr}(AB) = \\text{tr}(BA)\\) . Solution 3: Characteristic polynomial: \\[\\det(C - \\lambda I) = (5 - \\lambda)(3 - \\lambda) - (4)(2) = \\lambda^2 - 8\\lambda + 15 - 8 = \\lambda^2 - 8\\lambda + 7\\] \\[(\\lambda - 7)(\\lambda - 1) = 0\\] \\[\\lambda_1 = 7, \\quad \\lambda_2 = 1\\] Eigenvector for \\(\\lambda_1 = 7\\) : \\[(C - 7I)\\mathbf{v} = \\begin{bmatrix} -2 & 4 \\\\ 2 & -4 \\end{bmatrix}\\mathbf{v} = \\mathbf{0}\\] From the first row: \\(-2v_1 + 4v_2 = 0\\) , so \\(v_1 = 2v_2\\) . Choosing \\(v_2 = 1\\) : \\[\\mathbf{v}_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\] Eigenvector for \\(\\lambda_2 = 1\\) : \\[(C - I)\\mathbf{v} = \\begin{bmatrix} 4 & 4 \\\\ 2 & 2 \\end{bmatrix}\\mathbf{v} = \\mathbf{0}\\] From the first row: \\(4v_1 + 4v_2 = 0\\) , so \\(v_1 = -v_2\\) . Choosing \\(v_2 = 1\\) : \\[\\mathbf{v}_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\] Check: \\(\\text{tr}(C) = 5 + 3 = 8 = 7 + 1\\) and \\(\\det(C) = 15 - 8 = 7 = 7 \\times 1\\) . Both pass. Solution 4: \\[l_{11} = \\sqrt{9} = 3\\] \\[l_{21} = \\frac{6}{3} = 2\\] \\[l_{22} = \\sqrt{8 - 2^2} = \\sqrt{4} = 2\\] \\[L = \\begin{bmatrix} 3 & 0 \\\\ 2 & 2 \\end{bmatrix}\\] Verification: \\[LL^T = \\begin{bmatrix} 3 & 0 \\\\ 2 & 2 \\end{bmatrix}\\begin{bmatrix} 3 & 2 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 9 & 6 \\\\ 6 & 8 \\end{bmatrix} = A \\quad \\checkmark\\] Solution 5: From Solution 3: \\(\\lambda_1 = 7\\) , \\(\\mathbf{v}_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\) , \\(\\lambda_2 = 1\\) , \\(\\mathbf{v}_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\) . \\[P = \\begin{bmatrix} 2 & -1 \\\\ 1 & 1 \\end{bmatrix}, \\quad D = \\begin{bmatrix} 7 & 0 \\\\ 0 & 1 \\end{bmatrix}\\] \\[P^{-1} = \\frac{1}{2(1) - (-1)(1)} \\begin{bmatrix} 1 & 1 \\\\ -1 & 2 \\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix} 1 & 1 \\\\ -1 & 2 \\end{bmatrix}\\] Verification: \\[PDP^{-1} = \\begin{bmatrix} 2 & -1 \\\\ 1 & 1 \\end{bmatrix}\\begin{bmatrix} 7 & 0 \\\\ 0 & 1 \\end{bmatrix} \\cdot \\frac{1}{3}\\begin{bmatrix} 1 & 1 \\\\ -1 & 2 \\end{bmatrix}\\] \\[= \\begin{bmatrix} 14 & -1 \\\\ 7 & 1 \\end{bmatrix} \\cdot \\frac{1}{3}\\begin{bmatrix} 1 & 1 \\\\ -1 & 2 \\end{bmatrix}\\] \\[= \\frac{1}{3}\\begin{bmatrix} 14(1)+(-1)(-1) & 14(1)+(-1)(2) \\\\ 7(1)+1(-1) & 7(1)+1(2) \\end{bmatrix} = \\frac{1}{3}\\begin{bmatrix} 15 & 12 \\\\ 6 & 9 \\end{bmatrix} = \\begin{bmatrix} 5 & 4 \\\\ 2 & 3 \\end{bmatrix} = C \\quad \\checkmark\\] Solution 6: (a) The Frobenius norm is: \\[\\|M\\|_F = \\sqrt{\\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2} = \\sqrt{100 + 25 + 1} = \\sqrt{126} = 3\\sqrt{14} \\approx 11.22\\] (b) The best rank-2 approximation discards \\(\\sigma_3 = 1\\) . The error is: \\[\\|M - M_2\\|_F = \\sqrt{\\sigma_3^2} = \\sigma_3 = 1\\] (c) The squared Frobenius norm captured by \\(M_2\\) is: \\[\\frac{\\sigma_1^2 + \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2 + \\sigma_3^2} = \\frac{100 + 25}{100 + 25 + 1} = \\frac{125}{126} \\approx 99.2\\%\\] So the rank-2 approximation captures about 99.2% of the total \"energy\" of \\(M\\) . Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Next: Tutorial 4 - Matrix Calculus and Optimization","title":"Solutions"},{"location":"tutorials/Tutorial_05_Vector_Calculus/","text":"Tutorial 4: Vector Calculus \u00b6 Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari \ud83d\udcda Learning Objectives \u00b6 By the end of this tutorial, you will understand: Differentiation of univariate functions and basic derivative rules Taylor series and polynomial approximation Partial derivatives and gradients Jacobians for vector-valued functions Matrix calculus rules and gradient identities The chain rule in single-variable and multivariate settings Backpropagation and computation graphs Higher-order derivatives and the Hessian matrix Useful gradient identities for machine learning Part 1: Differentiation of Univariate Functions \u00b6 1.1 Definition of the Derivative \u00b6 The derivative of a function \\(f(x)\\) measures the instantaneous rate of change of \\(f\\) with respect to \\(x\\) . \\[\\frac{df}{dx} = f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\\] Geometric interpretation: The derivative at a point gives the slope of the tangent line to the curve at that point. 1.2 Basic Derivative Rules \u00b6 Rule Function \\(f(x)\\) Derivative \\(f'(x)\\) Example Constant \\(c\\) \\(0\\) \\(\\frac{d}{dx}(5) = 0\\) Power Rule \\(x^n\\) \\(nx^{n-1}\\) \\(\\frac{d}{dx}(x^3) = 3x^2\\) Exponential \\(e^x\\) \\(e^x\\) \\(\\frac{d}{dx}(e^x) = e^x\\) Logarithm \\(\\ln(x)\\) \\(\\frac{1}{x}\\) \\(\\frac{d}{dx}(\\ln x) = \\frac{1}{x}\\) Sine \\(\\sin(x)\\) \\(\\cos(x)\\) \\(\\frac{d}{dx}(\\sin x) = \\cos x\\) Cosine \\(\\cos(x)\\) \\(-\\sin(x)\\) \\(\\frac{d}{dx}(\\cos x) = -\\sin x\\) 1.3 Combination Rules \u00b6 Sum Rule: $ \\(\\frac{d}{dx}\\left[f(x) + g(x)\\right] = f'(x) + g'(x)\\) $ Product Rule: $ \\(\\frac{d}{dx}\\left[f(x) \\cdot g(x)\\right] = f'(x) \\cdot g(x) + f(x) \\cdot g'(x)\\) $ Quotient Rule: $ \\(\\frac{d}{dx}\\left[\\frac{f(x)}{g(x)}\\right] = \\frac{f'(x) \\cdot g(x) - f(x) \\cdot g'(x)}{\\left[g(x)\\right]^2}\\) $ Chain Rule (single variable): $ \\(\\frac{d}{dx}\\left[f(g(x))\\right] = f'(g(x)) \\cdot g'(x)\\) $ 1.4 Worked Examples \u00b6 Example 1 (Product Rule): Find \\(\\frac{d}{dx}\\left[x^2 \\cdot e^x\\right]\\) . \\[\\frac{d}{dx}\\left[x^2 \\cdot e^x\\right] = 2x \\cdot e^x + x^2 \\cdot e^x = e^x(2x + x^2)\\] Example 2 (Chain Rule): Find \\(\\frac{d}{dx}\\left[e^{-x^2}\\right]\\) . Let \\(u = -x^2\\) , so \\(f(u) = e^u\\) . \\[\\frac{d}{dx}\\left[e^{-x^2}\\right] = e^{-x^2} \\cdot (-2x) = -2x \\, e^{-x^2}\\] Example 3 (Quotient Rule): Find the derivative of the sigmoid function \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) . \\[\\sigma'(x) = \\frac{0 \\cdot (1 + e^{-x}) - 1 \\cdot (-e^{-x})}{(1 + e^{-x})^2} = \\frac{e^{-x}}{(1 + e^{-x})^2}\\] This simplifies to the elegant identity: \\[\\sigma'(x) = \\sigma(x)\\left(1 - \\sigma(x)\\right)\\] Part 2: Taylor Series \u00b6 2.1 Taylor Series Definition \u00b6 A Taylor series expands a smooth function \\(f(x)\\) around a point \\(x_0\\) as an infinite polynomial: \\[f(x) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(x_0)}{k!}(x - x_0)^k\\] \\[= f(x_0) + f'(x_0)(x - x_0) + \\frac{f''(x_0)}{2!}(x - x_0)^2 + \\frac{f'''(x_0)}{3!}(x - x_0)^3 + \\cdots\\] 2.2 Taylor Polynomial Approximations \u00b6 In machine learning, we often use truncated Taylor polynomials for local approximation. First-order (linear) approximation: $ \\(f(x) \\approx f(x_0) + f'(x_0)(x - x_0)\\) $ Second-order (quadratic) approximation: $ \\(f(x) \\approx f(x_0) + f'(x_0)(x - x_0) + \\frac{f''(x_0)}{2}(x - x_0)^2\\) $ 2.3 Why Taylor Series Matter in ML \u00b6 Approximation Order Use in Machine Learning First-order Gradient descent (linear approximation of loss function) Second-order Newton's method (quadratic approximation of loss function) 2.4 Worked Example \u00b6 Example: Approximate \\(e^x\\) around \\(x_0 = 0\\) to second order. We need \\(f(0)\\) , \\(f'(0)\\) , and \\(f''(0)\\) . Since \\(f(x) = e^x\\) , all derivatives are \\(e^x\\) , so \\(f(0) = f'(0) = f''(0) = 1\\) . \\[e^x \\approx 1 + x + \\frac{x^2}{2}\\] Checking: at \\(x = 0.1\\) , the true value is \\(e^{0.1} = 1.10517...\\) \\[1 + 0.1 + \\frac{0.01}{2} = 1.105\\] The approximation is excellent near \\(x_0\\) ! Part 3: Partial Derivatives \u00b6 3.1 Definition \u00b6 For a function of multiple variables \\(f(x_1, x_2, \\ldots, x_n)\\) , the partial derivative with respect to \\(x_i\\) measures how \\(f\\) changes when only \\(x_i\\) varies, with all other variables held constant. \\[\\frac{\\partial f}{\\partial x_i} = \\lim_{h \\to 0} \\frac{f(x_1, \\ldots, x_i + h, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}\\] 3.2 Notation \u00b6 Partial derivatives have several equivalent notations: Notation Meaning \\(\\frac{\\partial f}{\\partial x}\\) Partial derivative of \\(f\\) with respect to \\(x\\) \\(f_x\\) Shorthand for \\(\\frac{\\partial f}{\\partial x}\\) \\(\\partial_x f\\) Another shorthand \\(D_x f\\) Differential operator notation 3.3 How to Compute Partial Derivatives \u00b6 Rule: To find \\(\\frac{\\partial f}{\\partial x_i}\\) , treat every variable except \\(x_i\\) as a constant, then differentiate with respect to \\(x_i\\) using the standard rules. Example 1: Let \\(f(x, y) = x^2 y + 3xy^2 - 2y\\) . \\[\\frac{\\partial f}{\\partial x} = 2xy + 3y^2\\] \\[\\frac{\\partial f}{\\partial y} = x^2 + 6xy - 2\\] Example 2: Let \\(f(x, y) = e^{xy} + \\sin(x)\\) . \\[\\frac{\\partial f}{\\partial x} = y \\, e^{xy} + \\cos(x)\\] \\[\\frac{\\partial f}{\\partial y} = x \\, e^{xy}\\] Part 4: Gradients \u00b6 4.1 Definition \u00b6 The gradient of a scalar-valued function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is a vector of all its partial derivatives: \\[\\nabla f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} \\in \\mathbb{R}^n\\] The gradient \"lives\" in the same space as the input \\(\\mathbf{x}\\) . 4.2 Gradient as Direction of Steepest Ascent \u00b6 The gradient has a fundamental geometric meaning: \\(\\nabla f(\\mathbf{x})\\) points in the direction of steepest ascent of \\(f\\) at \\(\\mathbf{x}\\) \\(-\\nabla f(\\mathbf{x})\\) points in the direction of steepest descent \\(\\|\\nabla f(\\mathbf{x})\\|\\) gives the rate of steepest ascent This is why gradient descent updates parameters as: \\[\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\, \\nabla f(\\mathbf{x}_t)\\] where \\(\\eta > 0\\) is the learning rate. 4.3 Worked Example \u00b6 Example: Find the gradient of \\(f(x_1, x_2, x_3) = x_1^2 + 2x_1 x_2 + x_3^3\\) . \\[\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\frac{\\partial f}{\\partial x_3} \\end{bmatrix} = \\begin{bmatrix} 2x_1 + 2x_2 \\\\ 2x_1 \\\\ 3x_3^2 \\end{bmatrix}\\] At the point \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}\\) : \\[\\nabla f\\big|_{\\mathbf{x}} = \\begin{bmatrix} 2(1) + 2(2) \\\\ 2(1) \\\\ 3(-1)^2 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 2 \\\\ 3 \\end{bmatrix}\\] The direction of steepest descent at this point is \\(-\\nabla f = \\begin{bmatrix} -6 \\\\ -2 \\\\ -3 \\end{bmatrix}\\) . Part 5: Jacobians \u00b6 5.1 Definition \u00b6 For a vector-valued function \\(\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m\\) with components \\(f_1, f_2, \\ldots, f_m\\) , the Jacobian is the \\(m \\times n\\) matrix of all first-order partial derivatives: \\[\\mathbf{J} = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\] Key observation: Each row of the Jacobian is the gradient (transposed) of one output component \\(f_i\\) . 5.2 Relationship to Gradients \u00b6 Object Input Output Derivative Gradient \\(\\nabla f\\) \\(\\mathbb{R}^n\\) \\(\\mathbb{R}\\) (scalar) Vector in \\(\\mathbb{R}^n\\) Jacobian \\(\\mathbf{J}\\) \\(\\mathbb{R}^n\\) \\(\\mathbb{R}^m\\) (vector) Matrix in \\(\\mathbb{R}^{m \\times n}\\) 5.3 Worked Example \u00b6 Example: Let \\(\\mathbf{f}: \\mathbb{R}^2 \\to \\mathbb{R}^3\\) be defined by: \\[\\mathbf{f}(x_1, x_2) = \\begin{bmatrix} x_1^2 x_2 \\\\ 5x_1 + \\sin(x_2) \\\\ x_2^2 \\end{bmatrix}\\] The Jacobian is: \\[\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} \\\\ \\frac{\\partial f_3}{\\partial x_1} & \\frac{\\partial f_3}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} 2x_1 x_2 & x_1^2 \\\\ 5 & \\cos(x_2) \\\\ 0 & 2x_2 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 2}\\] Part 6: Gradients of Matrices \u00b6 6.1 Matrix Calculus Rules \u00b6 When working with vectors and matrices, we need special differentiation rules. Gradient of a linear function: For \\(f(\\mathbf{x}) = \\mathbf{a}^T \\mathbf{x}\\) where \\(\\mathbf{a}, \\mathbf{x} \\in \\mathbb{R}^n\\) : \\[\\nabla_{\\mathbf{x}}(\\mathbf{a}^T \\mathbf{x}) = \\mathbf{a}\\] Gradient of a quadratic form: For \\(f(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) : \\[\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\] If \\(\\mathbf{A}\\) is symmetric ( \\(\\mathbf{A} = \\mathbf{A}^T\\) ), this simplifies to: \\[\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\] 6.2 Useful Matrix Calculus Identities \u00b6 Function \\(f(\\mathbf{x})\\) Gradient \\(\\nabla_{\\mathbf{x}} f\\) \\(\\mathbf{a}^T \\mathbf{x}\\) \\(\\mathbf{a}\\) \\(\\mathbf{x}^T \\mathbf{a}\\) \\(\\mathbf{a}\\) \\(\\mathbf{x}^T \\mathbf{x}\\) \\(2\\mathbf{x}\\) \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) \\((\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\) \\(\\|\\mathbf{x} - \\mathbf{b}\\|^2\\) \\(2(\\mathbf{x} - \\mathbf{b})\\) \\(\\mathbf{b}^T \\mathbf{A} \\mathbf{x}\\) \\(\\mathbf{A}^T \\mathbf{b}\\) 6.3 Worked Example \u00b6 Example: Find the gradient of the least-squares loss. The loss function is: \\[L(\\mathbf{w}) = \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|^2 = (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^T(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\\] Expanding: \\[L(\\mathbf{w}) = \\mathbf{w}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{w} - 2\\mathbf{y}^T \\mathbf{X} \\mathbf{w} + \\mathbf{y}^T \\mathbf{y}\\] Taking the gradient with respect to \\(\\mathbf{w}\\) : \\[\\nabla_{\\mathbf{w}} L = 2\\mathbf{X}^T \\mathbf{X} \\mathbf{w} - 2\\mathbf{X}^T \\mathbf{y}\\] Setting \\(\\nabla_{\\mathbf{w}} L = \\mathbf{0}\\) gives the normal equation : \\[\\mathbf{X}^T \\mathbf{X} \\mathbf{w}^* = \\mathbf{X}^T \\mathbf{y} \\quad \\Longrightarrow \\quad \\mathbf{w}^* = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\] Part 7: The Chain Rule \u00b6 7.1 Single Variable Chain Rule \u00b6 If \\(y = f(g(x))\\) , then: \\[\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\\] Example: \\(y = (3x + 1)^4\\) Let \\(g = 3x + 1\\) , so \\(y = g^4\\) . \\[\\frac{dy}{dx} = 4g^3 \\cdot 3 = 12(3x + 1)^3\\] 7.2 Multivariate Chain Rule \u00b6 If \\(f\\) depends on \\(\\mathbf{x}\\) through intermediate variables \\(\\mathbf{u}\\) : \\[\\mathbf{x} \\in \\mathbb{R}^n \\xrightarrow{\\mathbf{g}} \\mathbf{u} \\in \\mathbb{R}^m \\xrightarrow{f} y \\in \\mathbb{R}\\] Then: \\[\\frac{\\partial f}{\\partial x_i} = \\sum_{j=1}^{m} \\frac{\\partial f}{\\partial u_j} \\cdot \\frac{\\partial u_j}{\\partial x_i}\\] In matrix form (using Jacobians): \\[\\frac{\\partial f}{\\partial \\mathbf{x}} = \\frac{\\partial f}{\\partial \\mathbf{u}} \\cdot \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}\\] 7.3 Chain Rule for Neural Networks \u00b6 Consider a simple two-layer neural network: \\[\\mathbf{x} \\xrightarrow{\\mathbf{W}_1} \\mathbf{z}_1 \\xrightarrow{\\sigma} \\mathbf{a}_1 \\xrightarrow{\\mathbf{W}_2} \\mathbf{z}_2 \\xrightarrow{\\text{loss}} L\\] To find \\(\\frac{\\partial L}{\\partial \\mathbf{W}_1}\\) , we apply the chain rule through the entire computation: \\[\\frac{\\partial L}{\\partial \\mathbf{W}_1} = \\frac{\\partial L}{\\partial \\mathbf{z}_2} \\cdot \\frac{\\partial \\mathbf{z}_2}{\\partial \\mathbf{a}_1} \\cdot \\frac{\\partial \\mathbf{a}_1}{\\partial \\mathbf{z}_1} \\cdot \\frac{\\partial \\mathbf{z}_1}{\\partial \\mathbf{W}_1}\\] Each term in this product corresponds to a specific operation in the network. Part 8: Backpropagation \u00b6 8.1 Computation Graphs \u00b6 A computation graph represents a function as a directed acyclic graph (DAG) where: - Nodes represent operations or variables - Edges represent data flow Example: For \\(f(x, y) = (x + y) \\cdot (y + 1)\\) : x ---\\ (+) = a ---\\ y ---/ (*) = f y ---\\ / (+) = b --/ 1 ---/ Here \\(a = x + y\\) , \\(b = y + 1\\) , and \\(f = a \\cdot b\\) . 8.2 Forward Pass \u00b6 In the forward pass , we compute the output by evaluating the graph from inputs to output. Example: With \\(x = 2\\) , \\(y = 3\\) : Step Computation Value 1 \\(a = x + y\\) \\(a = 2 + 3 = 5\\) 2 \\(b = y + 1\\) \\(b = 3 + 1 = 4\\) 3 \\(f = a \\cdot b\\) \\(f = 5 \\cdot 4 = 20\\) 8.3 Backward Pass (Backpropagation) \u00b6 In the backward pass , we compute gradients by traversing the graph from output to inputs, applying the chain rule at each node. Starting from \\(\\frac{\\partial f}{\\partial f} = 1\\) : Step Gradient Computation Value 1 \\(\\frac{\\partial f}{\\partial a}\\) \\(b\\) \\(4\\) 2 \\(\\frac{\\partial f}{\\partial b}\\) \\(a\\) \\(5\\) 3 \\(\\frac{\\partial f}{\\partial x}\\) \\(\\frac{\\partial f}{\\partial a} \\cdot \\frac{\\partial a}{\\partial x} = b \\cdot 1\\) \\(4\\) 4 \\(\\frac{\\partial f}{\\partial y}\\) \\(\\frac{\\partial f}{\\partial a} \\cdot \\frac{\\partial a}{\\partial y} + \\frac{\\partial f}{\\partial b} \\cdot \\frac{\\partial b}{\\partial y} = b \\cdot 1 + a \\cdot 1\\) \\(4 + 5 = 9\\) Note: Since \\(y\\) appears in two paths ( \\(a\\) and \\(b\\) ), we sum the contributions from both paths. 8.4 General Backpropagation Algorithm \u00b6 For a computation graph with output \\(L\\) : Forward pass: Compute all intermediate values from inputs to output Initialize: Set \\(\\frac{\\partial L}{\\partial L} = 1\\) Backward pass: For each node \\(v\\) in reverse topological order: \\[\\frac{\\partial L}{\\partial v} = \\sum_{u \\in \\text{children}(v)} \\frac{\\partial L}{\\partial u} \\cdot \\frac{\\partial u}{\\partial v}\\] This is the foundation of training neural networks. Part 9: Higher-Order Derivatives \u00b6 9.1 Second-Order Partial Derivatives \u00b6 For a function \\(f(x_1, x_2, \\ldots, x_n)\\) , we can differentiate partial derivatives again: \\[\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial}{\\partial x_i}\\left(\\frac{\\partial f}{\\partial x_j}\\right)\\] Symmetry of mixed partials (Schwarz's theorem): If \\(f\\) has continuous second partial derivatives: \\[\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\\] 9.2 The Hessian Matrix \u00b6 The Hessian collects all second-order partial derivatives into a matrix: \\[\\mathbf{H} = \\nabla^2 f = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}\\] Properties: - The Hessian is symmetric (by Schwarz's theorem): \\(\\mathbf{H} = \\mathbf{H}^T\\) - If \\(\\mathbf{H}\\) is positive definite at a critical point, the point is a local minimum - If \\(\\mathbf{H}\\) is negative definite , the point is a local maximum - If \\(\\mathbf{H}\\) has both positive and negative eigenvalues, the point is a saddle point 9.3 Second-Order Taylor Expansion (Multivariate) \u00b6 The multivariate second-order Taylor expansion around \\(\\mathbf{x}_0\\) is: \\[f(\\mathbf{x}) \\approx f(\\mathbf{x}_0) + \\nabla f(\\mathbf{x}_0)^T (\\mathbf{x} - \\mathbf{x}_0) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_0)^T \\mathbf{H}(\\mathbf{x}_0)(\\mathbf{x} - \\mathbf{x}_0)\\] This is the basis for Newton's method in optimization. 9.4 Worked Example \u00b6 Example: Find the Hessian of \\(f(x_1, x_2) = x_1^3 + 2x_1 x_2^2 - x_2\\) . First, compute the gradient: \\[\\nabla f = \\begin{bmatrix} 3x_1^2 + 2x_2^2 \\\\ 4x_1 x_2 - 1 \\end{bmatrix}\\] Then, compute the Hessian: \\[\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix} = \\begin{bmatrix} 6x_1 & 4x_2 \\\\ 4x_2 & 4x_1 \\end{bmatrix}\\] Notice that \\(\\mathbf{H} = \\mathbf{H}^T\\) , confirming symmetry. Part 10: Useful Gradient Identities \u00b6 10.1 Reference Table \u00b6 These identities appear frequently in machine learning derivations. Here \\(\\mathbf{x}, \\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n\\) and \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) . # Function Gradient \\(\\nabla_{\\mathbf{x}}\\) 1 \\(\\mathbf{a}^T \\mathbf{x}\\) \\(\\mathbf{a}\\) 2 \\(\\mathbf{x}^T \\mathbf{x}\\) \\(2\\mathbf{x}\\) 3 \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) \\((\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\) 4 \\((\\mathbf{A}\\mathbf{x} - \\mathbf{b})^T(\\mathbf{A}\\mathbf{x} - \\mathbf{b})\\) \\(2\\mathbf{A}^T(\\mathbf{A}\\mathbf{x} - \\mathbf{b})\\) 5 \\(\\|\\mathbf{x}\\|^2 = \\mathbf{x}^T\\mathbf{x}\\) \\(2\\mathbf{x}\\) 6 \\(\\mathbf{b}^T \\mathbf{A} \\mathbf{x}\\) \\(\\mathbf{A}^T \\mathbf{b}\\) 10.2 Deriving Identity 3 \u00b6 Let us prove \\(\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\) . Write \\(f(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x} = \\sum_{i}\\sum_{j} x_i \\, A_{ij} \\, x_j\\) . Taking the partial derivative with respect to \\(x_k\\) : \\[\\frac{\\partial f}{\\partial x_k} = \\sum_{j} A_{kj} \\, x_j + \\sum_{i} x_i \\, A_{ik}\\] \\[= (\\mathbf{A}\\mathbf{x})_k + (\\mathbf{A}^T\\mathbf{x})_k\\] Collecting into a vector: \\[\\nabla_{\\mathbf{x}} f = \\mathbf{A}\\mathbf{x} + \\mathbf{A}^T\\mathbf{x} = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\] 10.3 When \\(\\mathbf{A}\\) is Symmetric \u00b6 If \\(\\mathbf{A} = \\mathbf{A}^T\\) , then \\(\\mathbf{A} + \\mathbf{A}^T = 2\\mathbf{A}\\) , so: \\[\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\] This is a very common case in machine learning, since covariance matrices and Hessians are symmetric. Summary: Key Takeaways \u00b6 Differentiation Fundamentals \u00b6 Derivatives measure rates of change; partial derivatives fix all variables except one The gradient \\(\\nabla f\\) collects all partial derivatives into a vector The Jacobian generalizes the gradient for vector-valued functions The Chain Rule and Backpropagation \u00b6 The multivariate chain rule composes Jacobians through multiplication Backpropagation applies the chain rule on a computation graph, working backward from the loss Gradients with respect to variables appearing in multiple paths are summed Higher-Order Information \u00b6 The Hessian matrix \\(\\mathbf{H}\\) captures second-order (curvature) information Positive definite Hessian at a critical point indicates a local minimum Matrix Calculus \u00b6 \\(\\nabla_{\\mathbf{x}}(\\mathbf{a}^T \\mathbf{x}) = \\mathbf{a}\\) \\(\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\) The normal equation for least squares: \\(\\mathbf{w}^* = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\) Practice Problems \u00b6 Problem 1 \u00b6 Find the derivative of \\(f(x) = x^3 e^{2x}\\) using the product and chain rules. Problem 2 \u00b6 Let \\(f(x, y) = x^2 y - 3xy^3 + 2x\\) . Find \\(\\frac{\\partial f}{\\partial x}\\) and \\(\\frac{\\partial f}{\\partial y}\\) , then compute the gradient at the point \\((1, -1)\\) . Problem 3 \u00b6 Compute the Jacobian of the function \\(\\mathbf{f}: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) defined by: \\[\\mathbf{f}(x, y) = \\begin{bmatrix} x^2 + y \\\\ xy - y^2 \\end{bmatrix}\\] Problem 4 \u00b6 Find the Hessian of \\(f(x_1, x_2) = x_1^2 + 4x_1 x_2 + x_2^2\\) . Is this Hessian positive definite? Problem 5 \u00b6 Consider the computation graph for \\(f(x) = (x + 2)^2\\) . Perform the forward pass with \\(x = 3\\) , then use backpropagation to compute \\(\\frac{df}{dx}\\) . Problem 6 \u00b6 Let \\(\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\) . Compute \\(\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x})\\) using the identity from Part 10, and verify by expanding \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) and differentiating directly. Solutions \u00b6 Solution 1: Using the product rule with \\(u = x^3\\) and \\(v = e^{2x}\\) : \\[f'(x) = \\frac{d}{dx}(x^3) \\cdot e^{2x} + x^3 \\cdot \\frac{d}{dx}(e^{2x})\\] \\[= 3x^2 \\cdot e^{2x} + x^3 \\cdot 2e^{2x}\\] \\[= e^{2x}(3x^2 + 2x^3) = x^2 e^{2x}(3 + 2x)\\] Solution 2: \\[\\frac{\\partial f}{\\partial x} = 2xy - 3y^3 + 2\\] \\[\\frac{\\partial f}{\\partial y} = x^2 - 9xy^2\\] At \\((1, -1)\\) : \\[\\frac{\\partial f}{\\partial x}\\bigg|_{(1,-1)} = 2(1)(-1) - 3(-1)^3 + 2 = -2 + 3 + 2 = 3\\] \\[\\frac{\\partial f}{\\partial y}\\bigg|_{(1,-1)} = (1)^2 - 9(1)(-1)^2 = 1 - 9 = -8\\] \\[\\nabla f\\big|_{(1,-1)} = \\begin{bmatrix} 3 \\\\ -8 \\end{bmatrix}\\] Solution 3: \\[\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\ \\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y} \\end{bmatrix}\\] For \\(f_1 = x^2 + y\\) : \\(\\frac{\\partial f_1}{\\partial x} = 2x\\) , \\(\\frac{\\partial f_1}{\\partial y} = 1\\) For \\(f_2 = xy - y^2\\) : \\(\\frac{\\partial f_2}{\\partial x} = y\\) , \\(\\frac{\\partial f_2}{\\partial y} = x - 2y\\) \\[\\mathbf{J} = \\begin{bmatrix} 2x & 1 \\\\ y & x - 2y \\end{bmatrix}\\] Solution 4: First, compute the gradient: \\[\\nabla f = \\begin{bmatrix} 2x_1 + 4x_2 \\\\ 4x_1 + 2x_2 \\end{bmatrix}\\] The Hessian (matrix of second derivatives): \\[\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix} = \\begin{bmatrix} 2 & 4 \\\\ 4 & 2 \\end{bmatrix}\\] To check positive definiteness, compute the eigenvalues. For a \\(2 \\times 2\\) matrix: \\[\\det(\\mathbf{H} - \\lambda \\mathbf{I}) = (2 - \\lambda)^2 - 16 = 0\\] \\[\\lambda^2 - 4\\lambda + 4 - 16 = 0 \\implies \\lambda^2 - 4\\lambda - 12 = 0\\] \\[\\lambda = \\frac{4 \\pm \\sqrt{16 + 48}}{2} = \\frac{4 \\pm 8}{2}\\] So \\(\\lambda_1 = 6\\) and \\(\\lambda_2 = -2\\) . Since one eigenvalue is negative, the Hessian is not positive definite . It is indefinite , meaning any critical point of \\(f\\) would be a saddle point. Solution 5: Decompose \\(f(x) = (x + 2)^2\\) into elementary steps: \\(a = x + 2\\) \\(f = a^2\\) Forward pass with \\(x = 3\\) : Step Computation Value 1 \\(a = x + 2\\) \\(a = 3 + 2 = 5\\) 2 \\(f = a^2\\) \\(f = 5^2 = 25\\) Backward pass: Step Gradient Computation Value 1 \\(\\frac{\\partial f}{\\partial f}\\) (seed) \\(1\\) 2 \\(\\frac{\\partial f}{\\partial a}\\) \\(2a\\) \\(2(5) = 10\\) 3 \\(\\frac{\\partial f}{\\partial x}\\) \\(\\frac{\\partial f}{\\partial a} \\cdot \\frac{\\partial a}{\\partial x} = 10 \\cdot 1\\) \\(10\\) Verification: \\(f'(x) = 2(x + 2)\\) , so \\(f'(3) = 2(5) = 10\\) . Correct! Solution 6: Using the identity: Since \\(\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\\) is symmetric ( \\(\\mathbf{A} = \\mathbf{A}^T\\) ): \\[\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 2\\mathbf{A}\\mathbf{x} = 2\\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 4x_1 + 2x_2 \\\\ 2x_1 + 6x_2 \\end{bmatrix}\\] Direct verification: Expand \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) : \\[\\mathbf{x}^T \\mathbf{A} \\mathbf{x} = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix}\\begin{bmatrix} 2x_1 + x_2 \\\\ x_1 + 3x_2 \\end{bmatrix} = 2x_1^2 + x_1 x_2 + x_1 x_2 + 3x_2^2 = 2x_1^2 + 2x_1 x_2 + 3x_2^2\\] Taking partial derivatives: \\[\\frac{\\partial}{\\partial x_1}(2x_1^2 + 2x_1 x_2 + 3x_2^2) = 4x_1 + 2x_2\\] \\[\\frac{\\partial}{\\partial x_2}(2x_1^2 + 2x_1 x_2 + 3x_2^2) = 2x_1 + 6x_2\\] \\[\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = \\begin{bmatrix} 4x_1 + 2x_2 \\\\ 2x_1 + 6x_2 \\end{bmatrix}\\] Both methods agree, confirming the identity. Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Next: Tutorial 5 - Probability and Distributions","title":"Tutorial 4: Vector Calculus"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#tutorial-4-vector-calculus","text":"Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari","title":"Tutorial 4: Vector Calculus"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#learning-objectives","text":"By the end of this tutorial, you will understand: Differentiation of univariate functions and basic derivative rules Taylor series and polynomial approximation Partial derivatives and gradients Jacobians for vector-valued functions Matrix calculus rules and gradient identities The chain rule in single-variable and multivariate settings Backpropagation and computation graphs Higher-order derivatives and the Hessian matrix Useful gradient identities for machine learning","title":"\ud83d\udcda Learning Objectives"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#part-1-differentiation-of-univariate-functions","text":"","title":"Part 1: Differentiation of Univariate Functions"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#11-definition-of-the-derivative","text":"The derivative of a function \\(f(x)\\) measures the instantaneous rate of change of \\(f\\) with respect to \\(x\\) . \\[\\frac{df}{dx} = f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\\] Geometric interpretation: The derivative at a point gives the slope of the tangent line to the curve at that point.","title":"1.1 Definition of the Derivative"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#12-basic-derivative-rules","text":"Rule Function \\(f(x)\\) Derivative \\(f'(x)\\) Example Constant \\(c\\) \\(0\\) \\(\\frac{d}{dx}(5) = 0\\) Power Rule \\(x^n\\) \\(nx^{n-1}\\) \\(\\frac{d}{dx}(x^3) = 3x^2\\) Exponential \\(e^x\\) \\(e^x\\) \\(\\frac{d}{dx}(e^x) = e^x\\) Logarithm \\(\\ln(x)\\) \\(\\frac{1}{x}\\) \\(\\frac{d}{dx}(\\ln x) = \\frac{1}{x}\\) Sine \\(\\sin(x)\\) \\(\\cos(x)\\) \\(\\frac{d}{dx}(\\sin x) = \\cos x\\) Cosine \\(\\cos(x)\\) \\(-\\sin(x)\\) \\(\\frac{d}{dx}(\\cos x) = -\\sin x\\)","title":"1.2 Basic Derivative Rules"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#13-combination-rules","text":"Sum Rule: $ \\(\\frac{d}{dx}\\left[f(x) + g(x)\\right] = f'(x) + g'(x)\\) $ Product Rule: $ \\(\\frac{d}{dx}\\left[f(x) \\cdot g(x)\\right] = f'(x) \\cdot g(x) + f(x) \\cdot g'(x)\\) $ Quotient Rule: $ \\(\\frac{d}{dx}\\left[\\frac{f(x)}{g(x)}\\right] = \\frac{f'(x) \\cdot g(x) - f(x) \\cdot g'(x)}{\\left[g(x)\\right]^2}\\) $ Chain Rule (single variable): $ \\(\\frac{d}{dx}\\left[f(g(x))\\right] = f'(g(x)) \\cdot g'(x)\\) $","title":"1.3 Combination Rules"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#14-worked-examples","text":"Example 1 (Product Rule): Find \\(\\frac{d}{dx}\\left[x^2 \\cdot e^x\\right]\\) . \\[\\frac{d}{dx}\\left[x^2 \\cdot e^x\\right] = 2x \\cdot e^x + x^2 \\cdot e^x = e^x(2x + x^2)\\] Example 2 (Chain Rule): Find \\(\\frac{d}{dx}\\left[e^{-x^2}\\right]\\) . Let \\(u = -x^2\\) , so \\(f(u) = e^u\\) . \\[\\frac{d}{dx}\\left[e^{-x^2}\\right] = e^{-x^2} \\cdot (-2x) = -2x \\, e^{-x^2}\\] Example 3 (Quotient Rule): Find the derivative of the sigmoid function \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) . \\[\\sigma'(x) = \\frac{0 \\cdot (1 + e^{-x}) - 1 \\cdot (-e^{-x})}{(1 + e^{-x})^2} = \\frac{e^{-x}}{(1 + e^{-x})^2}\\] This simplifies to the elegant identity: \\[\\sigma'(x) = \\sigma(x)\\left(1 - \\sigma(x)\\right)\\]","title":"1.4 Worked Examples"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#part-2-taylor-series","text":"","title":"Part 2: Taylor Series"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#21-taylor-series-definition","text":"A Taylor series expands a smooth function \\(f(x)\\) around a point \\(x_0\\) as an infinite polynomial: \\[f(x) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(x_0)}{k!}(x - x_0)^k\\] \\[= f(x_0) + f'(x_0)(x - x_0) + \\frac{f''(x_0)}{2!}(x - x_0)^2 + \\frac{f'''(x_0)}{3!}(x - x_0)^3 + \\cdots\\]","title":"2.1 Taylor Series Definition"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#22-taylor-polynomial-approximations","text":"In machine learning, we often use truncated Taylor polynomials for local approximation. First-order (linear) approximation: $ \\(f(x) \\approx f(x_0) + f'(x_0)(x - x_0)\\) $ Second-order (quadratic) approximation: $ \\(f(x) \\approx f(x_0) + f'(x_0)(x - x_0) + \\frac{f''(x_0)}{2}(x - x_0)^2\\) $","title":"2.2 Taylor Polynomial Approximations"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#23-why-taylor-series-matter-in-ml","text":"Approximation Order Use in Machine Learning First-order Gradient descent (linear approximation of loss function) Second-order Newton's method (quadratic approximation of loss function)","title":"2.3 Why Taylor Series Matter in ML"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#24-worked-example","text":"Example: Approximate \\(e^x\\) around \\(x_0 = 0\\) to second order. We need \\(f(0)\\) , \\(f'(0)\\) , and \\(f''(0)\\) . Since \\(f(x) = e^x\\) , all derivatives are \\(e^x\\) , so \\(f(0) = f'(0) = f''(0) = 1\\) . \\[e^x \\approx 1 + x + \\frac{x^2}{2}\\] Checking: at \\(x = 0.1\\) , the true value is \\(e^{0.1} = 1.10517...\\) \\[1 + 0.1 + \\frac{0.01}{2} = 1.105\\] The approximation is excellent near \\(x_0\\) !","title":"2.4 Worked Example"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#part-3-partial-derivatives","text":"","title":"Part 3: Partial Derivatives"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#31-definition","text":"For a function of multiple variables \\(f(x_1, x_2, \\ldots, x_n)\\) , the partial derivative with respect to \\(x_i\\) measures how \\(f\\) changes when only \\(x_i\\) varies, with all other variables held constant. \\[\\frac{\\partial f}{\\partial x_i} = \\lim_{h \\to 0} \\frac{f(x_1, \\ldots, x_i + h, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}\\]","title":"3.1 Definition"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#32-notation","text":"Partial derivatives have several equivalent notations: Notation Meaning \\(\\frac{\\partial f}{\\partial x}\\) Partial derivative of \\(f\\) with respect to \\(x\\) \\(f_x\\) Shorthand for \\(\\frac{\\partial f}{\\partial x}\\) \\(\\partial_x f\\) Another shorthand \\(D_x f\\) Differential operator notation","title":"3.2 Notation"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#33-how-to-compute-partial-derivatives","text":"Rule: To find \\(\\frac{\\partial f}{\\partial x_i}\\) , treat every variable except \\(x_i\\) as a constant, then differentiate with respect to \\(x_i\\) using the standard rules. Example 1: Let \\(f(x, y) = x^2 y + 3xy^2 - 2y\\) . \\[\\frac{\\partial f}{\\partial x} = 2xy + 3y^2\\] \\[\\frac{\\partial f}{\\partial y} = x^2 + 6xy - 2\\] Example 2: Let \\(f(x, y) = e^{xy} + \\sin(x)\\) . \\[\\frac{\\partial f}{\\partial x} = y \\, e^{xy} + \\cos(x)\\] \\[\\frac{\\partial f}{\\partial y} = x \\, e^{xy}\\]","title":"3.3 How to Compute Partial Derivatives"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#part-4-gradients","text":"","title":"Part 4: Gradients"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#41-definition","text":"The gradient of a scalar-valued function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is a vector of all its partial derivatives: \\[\\nabla f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} \\in \\mathbb{R}^n\\] The gradient \"lives\" in the same space as the input \\(\\mathbf{x}\\) .","title":"4.1 Definition"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#42-gradient-as-direction-of-steepest-ascent","text":"The gradient has a fundamental geometric meaning: \\(\\nabla f(\\mathbf{x})\\) points in the direction of steepest ascent of \\(f\\) at \\(\\mathbf{x}\\) \\(-\\nabla f(\\mathbf{x})\\) points in the direction of steepest descent \\(\\|\\nabla f(\\mathbf{x})\\|\\) gives the rate of steepest ascent This is why gradient descent updates parameters as: \\[\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\, \\nabla f(\\mathbf{x}_t)\\] where \\(\\eta > 0\\) is the learning rate.","title":"4.2 Gradient as Direction of Steepest Ascent"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#43-worked-example","text":"Example: Find the gradient of \\(f(x_1, x_2, x_3) = x_1^2 + 2x_1 x_2 + x_3^3\\) . \\[\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\frac{\\partial f}{\\partial x_3} \\end{bmatrix} = \\begin{bmatrix} 2x_1 + 2x_2 \\\\ 2x_1 \\\\ 3x_3^2 \\end{bmatrix}\\] At the point \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -1 \\end{bmatrix}\\) : \\[\\nabla f\\big|_{\\mathbf{x}} = \\begin{bmatrix} 2(1) + 2(2) \\\\ 2(1) \\\\ 3(-1)^2 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 2 \\\\ 3 \\end{bmatrix}\\] The direction of steepest descent at this point is \\(-\\nabla f = \\begin{bmatrix} -6 \\\\ -2 \\\\ -3 \\end{bmatrix}\\) .","title":"4.3 Worked Example"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#part-5-jacobians","text":"","title":"Part 5: Jacobians"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#51-definition","text":"For a vector-valued function \\(\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m\\) with components \\(f_1, f_2, \\ldots, f_m\\) , the Jacobian is the \\(m \\times n\\) matrix of all first-order partial derivatives: \\[\\mathbf{J} = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\] Key observation: Each row of the Jacobian is the gradient (transposed) of one output component \\(f_i\\) .","title":"5.1 Definition"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#52-relationship-to-gradients","text":"Object Input Output Derivative Gradient \\(\\nabla f\\) \\(\\mathbb{R}^n\\) \\(\\mathbb{R}\\) (scalar) Vector in \\(\\mathbb{R}^n\\) Jacobian \\(\\mathbf{J}\\) \\(\\mathbb{R}^n\\) \\(\\mathbb{R}^m\\) (vector) Matrix in \\(\\mathbb{R}^{m \\times n}\\)","title":"5.2 Relationship to Gradients"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#53-worked-example","text":"Example: Let \\(\\mathbf{f}: \\mathbb{R}^2 \\to \\mathbb{R}^3\\) be defined by: \\[\\mathbf{f}(x_1, x_2) = \\begin{bmatrix} x_1^2 x_2 \\\\ 5x_1 + \\sin(x_2) \\\\ x_2^2 \\end{bmatrix}\\] The Jacobian is: \\[\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} \\\\ \\frac{\\partial f_3}{\\partial x_1} & \\frac{\\partial f_3}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} 2x_1 x_2 & x_1^2 \\\\ 5 & \\cos(x_2) \\\\ 0 & 2x_2 \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 2}\\]","title":"5.3 Worked Example"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#part-6-gradients-of-matrices","text":"","title":"Part 6: Gradients of Matrices"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#61-matrix-calculus-rules","text":"When working with vectors and matrices, we need special differentiation rules. Gradient of a linear function: For \\(f(\\mathbf{x}) = \\mathbf{a}^T \\mathbf{x}\\) where \\(\\mathbf{a}, \\mathbf{x} \\in \\mathbb{R}^n\\) : \\[\\nabla_{\\mathbf{x}}(\\mathbf{a}^T \\mathbf{x}) = \\mathbf{a}\\] Gradient of a quadratic form: For \\(f(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) : \\[\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\] If \\(\\mathbf{A}\\) is symmetric ( \\(\\mathbf{A} = \\mathbf{A}^T\\) ), this simplifies to: \\[\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\]","title":"6.1 Matrix Calculus Rules"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#62-useful-matrix-calculus-identities","text":"Function \\(f(\\mathbf{x})\\) Gradient \\(\\nabla_{\\mathbf{x}} f\\) \\(\\mathbf{a}^T \\mathbf{x}\\) \\(\\mathbf{a}\\) \\(\\mathbf{x}^T \\mathbf{a}\\) \\(\\mathbf{a}\\) \\(\\mathbf{x}^T \\mathbf{x}\\) \\(2\\mathbf{x}\\) \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) \\((\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\) \\(\\|\\mathbf{x} - \\mathbf{b}\\|^2\\) \\(2(\\mathbf{x} - \\mathbf{b})\\) \\(\\mathbf{b}^T \\mathbf{A} \\mathbf{x}\\) \\(\\mathbf{A}^T \\mathbf{b}\\)","title":"6.2 Useful Matrix Calculus Identities"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#63-worked-example","text":"Example: Find the gradient of the least-squares loss. The loss function is: \\[L(\\mathbf{w}) = \\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|^2 = (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^T(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\\] Expanding: \\[L(\\mathbf{w}) = \\mathbf{w}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{w} - 2\\mathbf{y}^T \\mathbf{X} \\mathbf{w} + \\mathbf{y}^T \\mathbf{y}\\] Taking the gradient with respect to \\(\\mathbf{w}\\) : \\[\\nabla_{\\mathbf{w}} L = 2\\mathbf{X}^T \\mathbf{X} \\mathbf{w} - 2\\mathbf{X}^T \\mathbf{y}\\] Setting \\(\\nabla_{\\mathbf{w}} L = \\mathbf{0}\\) gives the normal equation : \\[\\mathbf{X}^T \\mathbf{X} \\mathbf{w}^* = \\mathbf{X}^T \\mathbf{y} \\quad \\Longrightarrow \\quad \\mathbf{w}^* = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\]","title":"6.3 Worked Example"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#part-7-the-chain-rule","text":"","title":"Part 7: The Chain Rule"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#71-single-variable-chain-rule","text":"If \\(y = f(g(x))\\) , then: \\[\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\\] Example: \\(y = (3x + 1)^4\\) Let \\(g = 3x + 1\\) , so \\(y = g^4\\) . \\[\\frac{dy}{dx} = 4g^3 \\cdot 3 = 12(3x + 1)^3\\]","title":"7.1 Single Variable Chain Rule"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#72-multivariate-chain-rule","text":"If \\(f\\) depends on \\(\\mathbf{x}\\) through intermediate variables \\(\\mathbf{u}\\) : \\[\\mathbf{x} \\in \\mathbb{R}^n \\xrightarrow{\\mathbf{g}} \\mathbf{u} \\in \\mathbb{R}^m \\xrightarrow{f} y \\in \\mathbb{R}\\] Then: \\[\\frac{\\partial f}{\\partial x_i} = \\sum_{j=1}^{m} \\frac{\\partial f}{\\partial u_j} \\cdot \\frac{\\partial u_j}{\\partial x_i}\\] In matrix form (using Jacobians): \\[\\frac{\\partial f}{\\partial \\mathbf{x}} = \\frac{\\partial f}{\\partial \\mathbf{u}} \\cdot \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}\\]","title":"7.2 Multivariate Chain Rule"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#73-chain-rule-for-neural-networks","text":"Consider a simple two-layer neural network: \\[\\mathbf{x} \\xrightarrow{\\mathbf{W}_1} \\mathbf{z}_1 \\xrightarrow{\\sigma} \\mathbf{a}_1 \\xrightarrow{\\mathbf{W}_2} \\mathbf{z}_2 \\xrightarrow{\\text{loss}} L\\] To find \\(\\frac{\\partial L}{\\partial \\mathbf{W}_1}\\) , we apply the chain rule through the entire computation: \\[\\frac{\\partial L}{\\partial \\mathbf{W}_1} = \\frac{\\partial L}{\\partial \\mathbf{z}_2} \\cdot \\frac{\\partial \\mathbf{z}_2}{\\partial \\mathbf{a}_1} \\cdot \\frac{\\partial \\mathbf{a}_1}{\\partial \\mathbf{z}_1} \\cdot \\frac{\\partial \\mathbf{z}_1}{\\partial \\mathbf{W}_1}\\] Each term in this product corresponds to a specific operation in the network.","title":"7.3 Chain Rule for Neural Networks"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#part-8-backpropagation","text":"","title":"Part 8: Backpropagation"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#81-computation-graphs","text":"A computation graph represents a function as a directed acyclic graph (DAG) where: - Nodes represent operations or variables - Edges represent data flow Example: For \\(f(x, y) = (x + y) \\cdot (y + 1)\\) : x ---\\ (+) = a ---\\ y ---/ (*) = f y ---\\ / (+) = b --/ 1 ---/ Here \\(a = x + y\\) , \\(b = y + 1\\) , and \\(f = a \\cdot b\\) .","title":"8.1 Computation Graphs"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#82-forward-pass","text":"In the forward pass , we compute the output by evaluating the graph from inputs to output. Example: With \\(x = 2\\) , \\(y = 3\\) : Step Computation Value 1 \\(a = x + y\\) \\(a = 2 + 3 = 5\\) 2 \\(b = y + 1\\) \\(b = 3 + 1 = 4\\) 3 \\(f = a \\cdot b\\) \\(f = 5 \\cdot 4 = 20\\)","title":"8.2 Forward Pass"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#83-backward-pass-backpropagation","text":"In the backward pass , we compute gradients by traversing the graph from output to inputs, applying the chain rule at each node. Starting from \\(\\frac{\\partial f}{\\partial f} = 1\\) : Step Gradient Computation Value 1 \\(\\frac{\\partial f}{\\partial a}\\) \\(b\\) \\(4\\) 2 \\(\\frac{\\partial f}{\\partial b}\\) \\(a\\) \\(5\\) 3 \\(\\frac{\\partial f}{\\partial x}\\) \\(\\frac{\\partial f}{\\partial a} \\cdot \\frac{\\partial a}{\\partial x} = b \\cdot 1\\) \\(4\\) 4 \\(\\frac{\\partial f}{\\partial y}\\) \\(\\frac{\\partial f}{\\partial a} \\cdot \\frac{\\partial a}{\\partial y} + \\frac{\\partial f}{\\partial b} \\cdot \\frac{\\partial b}{\\partial y} = b \\cdot 1 + a \\cdot 1\\) \\(4 + 5 = 9\\) Note: Since \\(y\\) appears in two paths ( \\(a\\) and \\(b\\) ), we sum the contributions from both paths.","title":"8.3 Backward Pass (Backpropagation)"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#84-general-backpropagation-algorithm","text":"For a computation graph with output \\(L\\) : Forward pass: Compute all intermediate values from inputs to output Initialize: Set \\(\\frac{\\partial L}{\\partial L} = 1\\) Backward pass: For each node \\(v\\) in reverse topological order: \\[\\frac{\\partial L}{\\partial v} = \\sum_{u \\in \\text{children}(v)} \\frac{\\partial L}{\\partial u} \\cdot \\frac{\\partial u}{\\partial v}\\] This is the foundation of training neural networks.","title":"8.4 General Backpropagation Algorithm"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#part-9-higher-order-derivatives","text":"","title":"Part 9: Higher-Order Derivatives"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#91-second-order-partial-derivatives","text":"For a function \\(f(x_1, x_2, \\ldots, x_n)\\) , we can differentiate partial derivatives again: \\[\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial}{\\partial x_i}\\left(\\frac{\\partial f}{\\partial x_j}\\right)\\] Symmetry of mixed partials (Schwarz's theorem): If \\(f\\) has continuous second partial derivatives: \\[\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\\]","title":"9.1 Second-Order Partial Derivatives"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#92-the-hessian-matrix","text":"The Hessian collects all second-order partial derivatives into a matrix: \\[\\mathbf{H} = \\nabla^2 f = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}\\] Properties: - The Hessian is symmetric (by Schwarz's theorem): \\(\\mathbf{H} = \\mathbf{H}^T\\) - If \\(\\mathbf{H}\\) is positive definite at a critical point, the point is a local minimum - If \\(\\mathbf{H}\\) is negative definite , the point is a local maximum - If \\(\\mathbf{H}\\) has both positive and negative eigenvalues, the point is a saddle point","title":"9.2 The Hessian Matrix"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#93-second-order-taylor-expansion-multivariate","text":"The multivariate second-order Taylor expansion around \\(\\mathbf{x}_0\\) is: \\[f(\\mathbf{x}) \\approx f(\\mathbf{x}_0) + \\nabla f(\\mathbf{x}_0)^T (\\mathbf{x} - \\mathbf{x}_0) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_0)^T \\mathbf{H}(\\mathbf{x}_0)(\\mathbf{x} - \\mathbf{x}_0)\\] This is the basis for Newton's method in optimization.","title":"9.3 Second-Order Taylor Expansion (Multivariate)"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#94-worked-example","text":"Example: Find the Hessian of \\(f(x_1, x_2) = x_1^3 + 2x_1 x_2^2 - x_2\\) . First, compute the gradient: \\[\\nabla f = \\begin{bmatrix} 3x_1^2 + 2x_2^2 \\\\ 4x_1 x_2 - 1 \\end{bmatrix}\\] Then, compute the Hessian: \\[\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix} = \\begin{bmatrix} 6x_1 & 4x_2 \\\\ 4x_2 & 4x_1 \\end{bmatrix}\\] Notice that \\(\\mathbf{H} = \\mathbf{H}^T\\) , confirming symmetry.","title":"9.4 Worked Example"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#part-10-useful-gradient-identities","text":"","title":"Part 10: Useful Gradient Identities"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#101-reference-table","text":"These identities appear frequently in machine learning derivations. Here \\(\\mathbf{x}, \\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n\\) and \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) . # Function Gradient \\(\\nabla_{\\mathbf{x}}\\) 1 \\(\\mathbf{a}^T \\mathbf{x}\\) \\(\\mathbf{a}\\) 2 \\(\\mathbf{x}^T \\mathbf{x}\\) \\(2\\mathbf{x}\\) 3 \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) \\((\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\) 4 \\((\\mathbf{A}\\mathbf{x} - \\mathbf{b})^T(\\mathbf{A}\\mathbf{x} - \\mathbf{b})\\) \\(2\\mathbf{A}^T(\\mathbf{A}\\mathbf{x} - \\mathbf{b})\\) 5 \\(\\|\\mathbf{x}\\|^2 = \\mathbf{x}^T\\mathbf{x}\\) \\(2\\mathbf{x}\\) 6 \\(\\mathbf{b}^T \\mathbf{A} \\mathbf{x}\\) \\(\\mathbf{A}^T \\mathbf{b}\\)","title":"10.1 Reference Table"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#102-deriving-identity-3","text":"Let us prove \\(\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\) . Write \\(f(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x} = \\sum_{i}\\sum_{j} x_i \\, A_{ij} \\, x_j\\) . Taking the partial derivative with respect to \\(x_k\\) : \\[\\frac{\\partial f}{\\partial x_k} = \\sum_{j} A_{kj} \\, x_j + \\sum_{i} x_i \\, A_{ik}\\] \\[= (\\mathbf{A}\\mathbf{x})_k + (\\mathbf{A}^T\\mathbf{x})_k\\] Collecting into a vector: \\[\\nabla_{\\mathbf{x}} f = \\mathbf{A}\\mathbf{x} + \\mathbf{A}^T\\mathbf{x} = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\]","title":"10.2 Deriving Identity 3"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#103-when-mathbfa-is-symmetric","text":"If \\(\\mathbf{A} = \\mathbf{A}^T\\) , then \\(\\mathbf{A} + \\mathbf{A}^T = 2\\mathbf{A}\\) , so: \\[\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 2\\mathbf{A}\\mathbf{x}\\] This is a very common case in machine learning, since covariance matrices and Hessians are symmetric.","title":"10.3 When \\(\\mathbf{A}\\) is Symmetric"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#summary-key-takeaways","text":"","title":"Summary: Key Takeaways"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#differentiation-fundamentals","text":"Derivatives measure rates of change; partial derivatives fix all variables except one The gradient \\(\\nabla f\\) collects all partial derivatives into a vector The Jacobian generalizes the gradient for vector-valued functions","title":"Differentiation Fundamentals"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#the-chain-rule-and-backpropagation","text":"The multivariate chain rule composes Jacobians through multiplication Backpropagation applies the chain rule on a computation graph, working backward from the loss Gradients with respect to variables appearing in multiple paths are summed","title":"The Chain Rule and Backpropagation"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#higher-order-information","text":"The Hessian matrix \\(\\mathbf{H}\\) captures second-order (curvature) information Positive definite Hessian at a critical point indicates a local minimum","title":"Higher-Order Information"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#matrix-calculus","text":"\\(\\nabla_{\\mathbf{x}}(\\mathbf{a}^T \\mathbf{x}) = \\mathbf{a}\\) \\(\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\\) The normal equation for least squares: \\(\\mathbf{w}^* = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\)","title":"Matrix Calculus"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#practice-problems","text":"","title":"Practice Problems"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#problem-1","text":"Find the derivative of \\(f(x) = x^3 e^{2x}\\) using the product and chain rules.","title":"Problem 1"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#problem-2","text":"Let \\(f(x, y) = x^2 y - 3xy^3 + 2x\\) . Find \\(\\frac{\\partial f}{\\partial x}\\) and \\(\\frac{\\partial f}{\\partial y}\\) , then compute the gradient at the point \\((1, -1)\\) .","title":"Problem 2"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#problem-3","text":"Compute the Jacobian of the function \\(\\mathbf{f}: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) defined by: \\[\\mathbf{f}(x, y) = \\begin{bmatrix} x^2 + y \\\\ xy - y^2 \\end{bmatrix}\\]","title":"Problem 3"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#problem-4","text":"Find the Hessian of \\(f(x_1, x_2) = x_1^2 + 4x_1 x_2 + x_2^2\\) . Is this Hessian positive definite?","title":"Problem 4"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#problem-5","text":"Consider the computation graph for \\(f(x) = (x + 2)^2\\) . Perform the forward pass with \\(x = 3\\) , then use backpropagation to compute \\(\\frac{df}{dx}\\) .","title":"Problem 5"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#problem-6","text":"Let \\(\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\\) and \\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\) . Compute \\(\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x})\\) using the identity from Part 10, and verify by expanding \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) and differentiating directly.","title":"Problem 6"},{"location":"tutorials/Tutorial_05_Vector_Calculus/#solutions","text":"Solution 1: Using the product rule with \\(u = x^3\\) and \\(v = e^{2x}\\) : \\[f'(x) = \\frac{d}{dx}(x^3) \\cdot e^{2x} + x^3 \\cdot \\frac{d}{dx}(e^{2x})\\] \\[= 3x^2 \\cdot e^{2x} + x^3 \\cdot 2e^{2x}\\] \\[= e^{2x}(3x^2 + 2x^3) = x^2 e^{2x}(3 + 2x)\\] Solution 2: \\[\\frac{\\partial f}{\\partial x} = 2xy - 3y^3 + 2\\] \\[\\frac{\\partial f}{\\partial y} = x^2 - 9xy^2\\] At \\((1, -1)\\) : \\[\\frac{\\partial f}{\\partial x}\\bigg|_{(1,-1)} = 2(1)(-1) - 3(-1)^3 + 2 = -2 + 3 + 2 = 3\\] \\[\\frac{\\partial f}{\\partial y}\\bigg|_{(1,-1)} = (1)^2 - 9(1)(-1)^2 = 1 - 9 = -8\\] \\[\\nabla f\\big|_{(1,-1)} = \\begin{bmatrix} 3 \\\\ -8 \\end{bmatrix}\\] Solution 3: \\[\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\ \\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y} \\end{bmatrix}\\] For \\(f_1 = x^2 + y\\) : \\(\\frac{\\partial f_1}{\\partial x} = 2x\\) , \\(\\frac{\\partial f_1}{\\partial y} = 1\\) For \\(f_2 = xy - y^2\\) : \\(\\frac{\\partial f_2}{\\partial x} = y\\) , \\(\\frac{\\partial f_2}{\\partial y} = x - 2y\\) \\[\\mathbf{J} = \\begin{bmatrix} 2x & 1 \\\\ y & x - 2y \\end{bmatrix}\\] Solution 4: First, compute the gradient: \\[\\nabla f = \\begin{bmatrix} 2x_1 + 4x_2 \\\\ 4x_1 + 2x_2 \\end{bmatrix}\\] The Hessian (matrix of second derivatives): \\[\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix} = \\begin{bmatrix} 2 & 4 \\\\ 4 & 2 \\end{bmatrix}\\] To check positive definiteness, compute the eigenvalues. For a \\(2 \\times 2\\) matrix: \\[\\det(\\mathbf{H} - \\lambda \\mathbf{I}) = (2 - \\lambda)^2 - 16 = 0\\] \\[\\lambda^2 - 4\\lambda + 4 - 16 = 0 \\implies \\lambda^2 - 4\\lambda - 12 = 0\\] \\[\\lambda = \\frac{4 \\pm \\sqrt{16 + 48}}{2} = \\frac{4 \\pm 8}{2}\\] So \\(\\lambda_1 = 6\\) and \\(\\lambda_2 = -2\\) . Since one eigenvalue is negative, the Hessian is not positive definite . It is indefinite , meaning any critical point of \\(f\\) would be a saddle point. Solution 5: Decompose \\(f(x) = (x + 2)^2\\) into elementary steps: \\(a = x + 2\\) \\(f = a^2\\) Forward pass with \\(x = 3\\) : Step Computation Value 1 \\(a = x + 2\\) \\(a = 3 + 2 = 5\\) 2 \\(f = a^2\\) \\(f = 5^2 = 25\\) Backward pass: Step Gradient Computation Value 1 \\(\\frac{\\partial f}{\\partial f}\\) (seed) \\(1\\) 2 \\(\\frac{\\partial f}{\\partial a}\\) \\(2a\\) \\(2(5) = 10\\) 3 \\(\\frac{\\partial f}{\\partial x}\\) \\(\\frac{\\partial f}{\\partial a} \\cdot \\frac{\\partial a}{\\partial x} = 10 \\cdot 1\\) \\(10\\) Verification: \\(f'(x) = 2(x + 2)\\) , so \\(f'(3) = 2(5) = 10\\) . Correct! Solution 6: Using the identity: Since \\(\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\\) is symmetric ( \\(\\mathbf{A} = \\mathbf{A}^T\\) ): \\[\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 2\\mathbf{A}\\mathbf{x} = 2\\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 4x_1 + 2x_2 \\\\ 2x_1 + 6x_2 \\end{bmatrix}\\] Direct verification: Expand \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) : \\[\\mathbf{x}^T \\mathbf{A} \\mathbf{x} = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix}\\begin{bmatrix} 2x_1 + x_2 \\\\ x_1 + 3x_2 \\end{bmatrix} = 2x_1^2 + x_1 x_2 + x_1 x_2 + 3x_2^2 = 2x_1^2 + 2x_1 x_2 + 3x_2^2\\] Taking partial derivatives: \\[\\frac{\\partial}{\\partial x_1}(2x_1^2 + 2x_1 x_2 + 3x_2^2) = 4x_1 + 2x_2\\] \\[\\frac{\\partial}{\\partial x_2}(2x_1^2 + 2x_1 x_2 + 3x_2^2) = 2x_1 + 6x_2\\] \\[\\nabla_{\\mathbf{x}}(\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = \\begin{bmatrix} 4x_1 + 2x_2 \\\\ 2x_1 + 6x_2 \\end{bmatrix}\\] Both methods agree, confirming the identity. Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Next: Tutorial 5 - Probability and Distributions","title":"Solutions"},{"location":"tutorials/Tutorial_06_Probability_Distributions/","text":"Tutorial 5: Probability and Distributions \u00b6 Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Learning Objectives \u00b6 By the end of this tutorial, you will understand: Probability spaces, sample spaces, events, and the Kolmogorov axioms Conditional probability and independence Bayes' theorem and how to apply it Discrete random variables and their distributions (Bernoulli, Binomial, Geometric) Continuous random variables and their distributions (Uniform, Exponential, Gaussian) Expected value and variance, including computation rules The Gaussian (Normal) distribution and its properties Joint and marginal distributions Covariance and correlation The sum rule and product rule of probability Part 1: Probability Space \u00b6 1.1 Core Definitions \u00b6 A probability space is a triple \\((\\Omega, \\mathcal{F}, P)\\) consisting of three components: Component Name Description \\(\\Omega\\) Sample space The set of all possible outcomes of an experiment \\(\\mathcal{F}\\) Event space A collection of subsets of \\(\\Omega\\) (the events we can assign probabilities to) \\(P\\) Probability function A function \\(P: \\mathcal{F} \\to [0, 1]\\) that assigns probabilities to events Example (Coin Flip): - Sample space: \\(\\Omega = \\{H, T\\}\\) - Event space: \\(\\mathcal{F} = \\{\\emptyset, \\{H\\}, \\{T\\}, \\{H, T\\}\\}\\) - Probability: \\(P(\\{H\\}) = 0.5\\) , \\(P(\\{T\\}) = 0.5\\) Example (Rolling a Die): - Sample space: \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\) - Event \"rolling an even number\": \\(A = \\{2, 4, 6\\}\\) - \\(P(A) = \\frac{3}{6} = \\frac{1}{2}\\) 1.2 Kolmogorov Axioms of Probability \u00b6 All of probability theory rests on three axioms, formalized by Andrey Kolmogorov: Axiom Statement Meaning Axiom 1 (Non-negativity) \\(P(A) \\geq 0\\) for every event \\(A\\) Probabilities are never negative Axiom 2 (Normalization) \\(P(\\Omega) = 1\\) Something must happen Axiom 3 (Additivity) If \\(A \\cap B = \\emptyset\\) , then \\(P(A \\cup B) = P(A) + P(B)\\) For mutually exclusive events, probabilities add Key consequences of the axioms: \\(P(\\emptyset) = 0\\) (the impossible event has probability zero) \\(P(A^c) = 1 - P(A)\\) (complement rule) \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) (inclusion-exclusion) If \\(A \\subseteq B\\) , then \\(P(A) \\leq P(B)\\) (monotonicity) Worked Example: Suppose \\(P(A) = 0.6\\) and \\(P(B) = 0.4\\) with \\(P(A \\cap B) = 0.2\\) . Find \\(P(A \\cup B)\\) . \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = 0.6 + 0.4 - 0.2 = 0.8\\] Part 2: Conditional Probability and Independence \u00b6 2.1 Conditional Probability \u00b6 The conditional probability of event \\(A\\) given that event \\(B\\) has occurred is: \\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}, \\quad \\text{provided } P(B) > 0\\] This reads: \"the probability of \\(A\\) given \\(B\\) .\" Intuition: Once we know \\(B\\) happened, the sample space effectively shrinks to \\(B\\) , and we ask how much of \\(B\\) is also in \\(A\\) . Worked Example: A standard deck of 52 cards. What is the probability a card is a King given it is a face card? \\(B\\) = face card: there are 12 face cards (J, Q, K of each suit), so \\(P(B) = \\frac{12}{52}\\) \\(A \\cap B\\) = King and face card = King: there are 4 Kings, so \\(P(A \\cap B) = \\frac{4}{52}\\) \\[P(\\text{King} \\mid \\text{Face card}) = \\frac{P(A \\cap B)}{P(B)} = \\frac{4/52}{12/52} = \\frac{4}{12} = \\frac{1}{3}\\] 2.2 Independence \u00b6 Two events \\(A\\) and \\(B\\) are independent if knowing one gives no information about the other. Formally: \\[P(A \\cap B) = P(A) \\cdot P(B)\\] Equivalently, if \\(A\\) and \\(B\\) are independent: \\[P(A \\mid B) = P(A) \\quad \\text{and} \\quad P(B \\mid A) = P(B)\\] Example: Rolling two fair dice. Let \\(A\\) = \"first die shows 3\" and \\(B\\) = \"second die shows 5.\" \\[P(A) = \\frac{1}{6}, \\quad P(B) = \\frac{1}{6}, \\quad P(A \\cap B) = \\frac{1}{36} = \\frac{1}{6} \\cdot \\frac{1}{6}\\] Since \\(P(A \\cap B) = P(A)P(B)\\) , the events are independent. Warning: Independence is not the same as mutual exclusivity. If \\(A\\) and \\(B\\) are mutually exclusive and both have positive probability, they are not independent (knowing one happened tells you the other did not). Part 3: Bayes' Theorem \u00b6 3.1 The Formula \u00b6 Bayes' theorem lets us \"reverse\" conditional probabilities: \\[\\boxed{P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}}\\] Term Name Interpretation \\(P(A \\mid B)\\) Posterior Updated belief about \\(A\\) after observing \\(B\\) \\(P(A)\\) Prior Initial belief about \\(A\\) before seeing evidence \\(P(B \\mid A)\\) Likelihood How probable the evidence \\(B\\) is if \\(A\\) is true \\(P(B)\\) Evidence (marginal likelihood) Total probability of observing \\(B\\) 3.2 The Law of Total Probability \u00b6 The denominator \\(P(B)\\) is often computed using the law of total probability . If \\(A_1, A_2, \\ldots, A_n\\) partition \\(\\Omega\\) : \\[P(B) = \\sum_{i=1}^{n} P(B \\mid A_i) \\, P(A_i)\\] For two complementary events \\(A\\) and \\(A^c\\) : \\[P(B) = P(B \\mid A) \\, P(A) + P(B \\mid A^c) \\, P(A^c)\\] 3.3 Worked Example: Medical Testing \u00b6 A disease affects 1% of the population. A test has: - Sensitivity (true positive rate): \\(P(\\text{Positive} \\mid \\text{Disease}) = 0.95\\) - Specificity (true negative rate): \\(P(\\text{Negative} \\mid \\text{No Disease}) = 0.90\\) If a person tests positive, what is \\(P(\\text{Disease} \\mid \\text{Positive})\\) ? Step 1: Define events and assign values. - \\(P(D) = 0.01\\) , \\(P(D^c) = 0.99\\) - \\(P(+ \\mid D) = 0.95\\) , \\(P(+ \\mid D^c) = 1 - 0.90 = 0.10\\) Step 2: Compute \\(P(+)\\) using the law of total probability. \\[P(+) = P(+ \\mid D) \\, P(D) + P(+ \\mid D^c) \\, P(D^c)$$ $$P(+) = (0.95)(0.01) + (0.10)(0.99) = 0.0095 + 0.099 = 0.1085\\] Step 3: Apply Bayes' theorem. \\[P(D \\mid +) = \\frac{P(+ \\mid D) \\, P(D)}{P(+)} = \\frac{(0.95)(0.01)}{0.1085} = \\frac{0.0095}{0.1085} \\approx 0.0876\\] Interpretation: Even with a positive test, there is only about an 8.8% chance the person actually has the disease. This counterintuitive result arises because the disease is rare (low prior), so false positives outnumber true positives. Part 4: Discrete Random Variables \u00b6 4.1 Definitions \u00b6 A random variable \\(X\\) is a function that maps outcomes in the sample space to real numbers: \\[X: \\Omega \\to \\mathbb{R}\\] A random variable is discrete if it takes values from a countable set (e.g., \\(\\{0, 1, 2, \\ldots\\}\\) ). The probability mass function (PMF) of a discrete random variable \\(X\\) is: \\[p(x) = P(X = x)\\] Properties of a valid PMF: \\(p(x) \\geq 0\\) for all \\(x\\) \\(\\displaystyle\\sum_{\\text{all } x} p(x) = 1\\) 4.2 Bernoulli Distribution \u00b6 A single trial with two outcomes: success ( \\(X = 1\\) ) with probability \\(p\\) , or failure ( \\(X = 0\\) ) with probability \\(1 - p\\) . \\[X \\sim \\text{Bernoulli}(p)\\] \\[P(X = x) = p^x (1-p)^{1-x}, \\quad x \\in \\{0, 1\\}\\] Property Value Mean \\(E[X] = p\\) Variance \\(\\text{Var}(X) = p(1-p)\\) Example: A coin flip with \\(P(\\text{Heads}) = 0.6\\) . Then \\(X \\sim \\text{Bernoulli}(0.6)\\) . 4.3 Binomial Distribution \u00b6 The number of successes in \\(n\\) independent Bernoulli trials, each with success probability \\(p\\) . \\[X \\sim \\text{Binomial}(n, p)\\] \\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0, 1, 2, \\ldots, n\\] where \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\) is the binomial coefficient. Property Value Mean \\(E[X] = np\\) Variance \\(\\text{Var}(X) = np(1-p)\\) Worked Example: A fair coin is flipped 10 times. What is the probability of getting exactly 4 heads? \\[P(X = 4) = \\binom{10}{4} (0.5)^4 (0.5)^{6} = \\binom{10}{4} (0.5)^{10}$$ $$= \\frac{10!}{4! \\cdot 6!} \\cdot \\frac{1}{1024} = \\frac{210}{1024} \\approx 0.2051\\] 4.4 Geometric Distribution \u00b6 The number of trials until the first success in a sequence of independent Bernoulli trials. \\[X \\sim \\text{Geometric}(p)\\] \\[P(X = k) = (1-p)^{k-1} p, \\quad k = 1, 2, 3, \\ldots\\] Property Value Mean \\(E[X] = \\frac{1}{p}\\) Variance \\(\\text{Var}(X) = \\frac{1-p}{p^2}\\) Worked Example: You roll a fair die until you get a 6. What is the probability it takes exactly 3 rolls? \\[P(X = 3) = \\left(\\frac{5}{6}\\right)^{2} \\cdot \\frac{1}{6} = \\frac{25}{36} \\cdot \\frac{1}{6} = \\frac{25}{216} \\approx 0.1157\\] The expected number of rolls: \\(E[X] = \\frac{1}{1/6} = 6\\) . Part 5: Continuous Random Variables \u00b6 5.1 Definitions \u00b6 A random variable is continuous if it can take any value in an interval (or union of intervals). The probability density function (PDF) \\(f(x)\\) satisfies: \\(f(x) \\geq 0\\) for all \\(x\\) \\(\\displaystyle\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\) \\(\\displaystyle P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx\\) Important: For a continuous random variable, \\(P(X = x) = 0\\) for any specific value \\(x\\) . Only intervals have nonzero probability. The cumulative distribution function (CDF) is: \\[F(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f(t) \\, dt\\] Properties of the CDF: - \\(F(-\\infty) = 0\\) and \\(F(\\infty) = 1\\) - \\(F\\) is non-decreasing - \\(f(x) = \\frac{d}{dx} F(x)\\) (the PDF is the derivative of the CDF) 5.2 Uniform Distribution \u00b6 A random variable is equally likely to take any value in the interval \\([a, b]\\) . \\[X \\sim \\text{Uniform}(a, b)\\] \\[f(x) = \\begin{cases} \\frac{1}{b - a} & \\text{if } a \\leq x \\leq b \\\\ 0 & \\text{otherwise} \\end{cases}\\] Property Value Mean \\(E[X] = \\frac{a + b}{2}\\) Variance \\(\\text{Var}(X) = \\frac{(b - a)^2}{12}\\) Example: If \\(X \\sim \\text{Uniform}(0, 10)\\) , then \\(E[X] = 5\\) and \\(P(2 \\leq X \\leq 5) = \\frac{5-2}{10-0} = 0.3\\) . 5.3 Exponential Distribution \u00b6 Models the time between events in a Poisson process. The parameter \\(\\lambda > 0\\) is the rate. \\[X \\sim \\text{Exponential}(\\lambda)\\] \\[f(x) = \\begin{cases} \\lambda e^{-\\lambda x} & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}\\] \\[F(x) = 1 - e^{-\\lambda x}, \\quad x \\geq 0\\] Property Value Mean \\(E[X] = \\frac{1}{\\lambda}\\) Variance \\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\) Key property (Memoryless): \\[P(X > s + t \\mid X > s) = P(X > t)\\] Worked Example: Light bulbs fail at a rate of \\(\\lambda = 0.01\\) per hour. What is the probability a bulb lasts more than 200 hours? \\[P(X > 200) = 1 - F(200) = e^{-0.01 \\cdot 200} = e^{-2} \\approx 0.1353\\] 5.4 Gaussian (Normal) Distribution \u00b6 The most important distribution in statistics and machine learning. \\[X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\] \\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\] Property Value Mean \\(E[X] = \\mu\\) Variance \\(\\text{Var}(X) = \\sigma^2\\) Full details on the Gaussian are in Part 7 below. Part 6: Expected Value and Variance \u00b6 6.1 Expected Value (Mean) \u00b6 The expected value is the long-run average of a random variable. Discrete case: \\[E[X] = \\sum_{x} x \\, p(x)\\] Continuous case: \\[E[X] = \\int_{-\\infty}^{\\infty} x \\, f(x) \\, dx\\] Expected value of a function \\(g(X)\\) : \\[E[g(X)] = \\sum_{x} g(x) \\, p(x) \\quad \\text{(discrete)}\\] \\[E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) \\, f(x) \\, dx \\quad \\text{(continuous)}\\] 6.2 Properties of Expected Value \u00b6 Property Formula Linearity \\(E[aX + b] = aE[X] + b\\) Sum \\(E[X + Y] = E[X] + E[Y]\\) (always, even if dependent) Product (independent) \\(E[XY] = E[X] \\cdot E[Y]\\) (only if \\(X, Y\\) are independent) Constant \\(E[c] = c\\) Worked Example: Let \\(X\\) be a die roll. Compute \\(E[X]\\) . \\[E[X] = \\sum_{x=1}^{6} x \\cdot \\frac{1}{6} = \\frac{1}{6}(1 + 2 + 3 + 4 + 5 + 6) = \\frac{21}{6} = 3.5\\] 6.3 Variance \u00b6 Variance measures how spread out a distribution is around its mean. \\[\\text{Var}(X) = E\\left[(X - E[X])^2\\right]\\] Shortcut formula (very useful for computation): \\[\\boxed{\\text{Var}(X) = E[X^2] - (E[X])^2}\\] The standard deviation is \\(\\sigma = \\sqrt{\\text{Var}(X)}\\) . 6.4 Properties of Variance \u00b6 Property Formula Scaling \\(\\text{Var}(aX) = a^2 \\text{Var}(X)\\) Shift \\(\\text{Var}(X + b) = \\text{Var}(X)\\) Affine \\(\\text{Var}(aX + b) = a^2 \\text{Var}(X)\\) Sum (independent) \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\) (only if independent) Constant \\(\\text{Var}(c) = 0\\) Worked Example: Let \\(X\\) be a die roll. Compute \\(\\text{Var}(X)\\) . First, compute \\(E[X^2]\\) : \\[E[X^2] = \\frac{1}{6}(1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2) = \\frac{1}{6}(1 + 4 + 9 + 16 + 25 + 36) = \\frac{91}{6} \\approx 15.167\\] We already know \\(E[X] = 3.5\\) , so \\((E[X])^2 = 12.25\\) . \\[\\text{Var}(X) = E[X^2] - (E[X])^2 = \\frac{91}{6} - \\frac{49}{4} = \\frac{182}{12} - \\frac{147}{12} = \\frac{35}{12} \\approx 2.917\\] Part 7: Gaussian (Normal) Distribution in Depth \u00b6 7.1 Definition \u00b6 The Gaussian distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) has PDF: \\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right), \\quad x \\in \\mathbb{R}\\] We write \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) . 7.2 The Standard Normal Distribution \u00b6 When \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\) , we get the standard normal \\(Z \\sim \\mathcal{N}(0, 1)\\) : \\[\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)\\] Standardization: Any normal random variable can be converted to a standard normal: \\[Z = \\frac{X - \\mu}{\\sigma}\\] 7.3 Key Properties \u00b6 Property Description Symmetry The PDF is symmetric about \\(\\mu\\) 68-95-99.7 Rule ~68% of values fall within \\(\\mu \\pm \\sigma\\) , ~95% within \\(\\mu \\pm 2\\sigma\\) , ~99.7% within \\(\\mu \\pm 3\\sigma\\) Linear closure If \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) , then \\(aX + b \\sim \\mathcal{N}(a\\mu + b, \\, a^2\\sigma^2)\\) Sum of normals If \\(X \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)\\) and \\(Y \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)\\) are independent, then \\(X + Y \\sim \\mathcal{N}(\\mu_1 + \\mu_2, \\, \\sigma_1^2 + \\sigma_2^2)\\) Worked Example: Exam scores are distributed as \\(X \\sim \\mathcal{N}(75, 100)\\) (mean 75, standard deviation 10). What fraction of students score above 90? Standardize: \\[Z = \\frac{90 - 75}{10} = 1.5\\] \\[P(X > 90) = P(Z > 1.5) = 1 - \\Phi(1.5) \\approx 1 - 0.9332 = 0.0668\\] About 6.7% of students score above 90. 7.4 Why the Gaussian Matters in Machine Learning \u00b6 The Central Limit Theorem states that the sum of many independent random variables tends toward a Gaussian, regardless of their individual distributions. Many ML algorithms assume Gaussian noise (linear regression, Gaussian processes). The multivariate Gaussian is fundamental to dimensionality reduction (PCA) and generative models. Part 8: Joint and Marginal Distributions \u00b6 8.1 Joint Distribution \u00b6 The joint distribution describes the probability behavior of two (or more) random variables simultaneously. Discrete (Joint PMF): \\[p(x, y) = P(X = x, Y = y)\\] Properties: - \\(p(x, y) \\geq 0\\) for all \\(x, y\\) - \\(\\displaystyle\\sum_{x}\\sum_{y} p(x, y) = 1\\) Continuous (Joint PDF): \\[f(x, y) \\geq 0 \\quad \\text{and} \\quad \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(x, y) \\, dx \\, dy = 1\\] 8.2 Marginal Distribution \u00b6 The marginal distribution of one variable is obtained by summing (or integrating) over the other variable. Discrete: \\[p_X(x) = \\sum_{y} p(x, y) \\qquad \\text{and} \\qquad p_Y(y) = \\sum_{x} p(x, y)\\] Continuous: \\[f_X(x) = \\int_{-\\infty}^{\\infty} f(x, y) \\, dy \\qquad \\text{and} \\qquad f_Y(y) = \\int_{-\\infty}^{\\infty} f(x, y) \\, dx\\] Worked Example (Discrete): Consider two discrete random variables \\(X\\) and \\(Y\\) with the following joint PMF table: \\(Y = 0\\) \\(Y = 1\\) \\(p_X(x)\\) \\(X = 0\\) 0.1 0.2 0.3 \\(X = 1\\) 0.3 0.4 0.7 \\(p_Y(y)\\) 0.4 0.6 1.0 Marginals are computed by summing each row or column: - \\(p_X(0) = 0.1 + 0.2 = 0.3\\) - \\(p_X(1) = 0.3 + 0.4 = 0.7\\) - \\(p_Y(0) = 0.1 + 0.3 = 0.4\\) - \\(p_Y(1) = 0.2 + 0.4 = 0.6\\) 8.3 Independence of Random Variables \u00b6 \\(X\\) and \\(Y\\) are independent if and only if: \\[p(x, y) = p_X(x) \\cdot p_Y(y) \\quad \\text{for all } x, y\\] Check the example above: \\(p(0, 0) = 0.1\\) but \\(p_X(0) \\cdot p_Y(0) = 0.3 \\times 0.4 = 0.12 \\neq 0.1\\) . So \\(X\\) and \\(Y\\) are not independent. Part 9: Covariance and Correlation \u00b6 9.1 Covariance \u00b6 Covariance measures how two random variables vary together: \\[\\text{Cov}(X, Y) = E\\left[(X - E[X])(Y - E[Y])\\right]\\] Shortcut formula: \\[\\boxed{\\text{Cov}(X, Y) = E[XY] - E[X] \\cdot E[Y]}\\] Value Interpretation \\(\\text{Cov}(X,Y) > 0\\) \\(X\\) and \\(Y\\) tend to increase together \\(\\text{Cov}(X,Y) < 0\\) When \\(X\\) increases, \\(Y\\) tends to decrease \\(\\text{Cov}(X,Y) = 0\\) No linear relationship (uncorrelated) Properties of Covariance: \\(\\text{Cov}(X, X) = \\text{Var}(X)\\) \\(\\text{Cov}(X, Y) = \\text{Cov}(Y, X)\\) (symmetric) \\(\\text{Cov}(aX + b, \\, cY + d) = ac \\, \\text{Cov}(X, Y)\\) If \\(X\\) and \\(Y\\) are independent, then \\(\\text{Cov}(X, Y) = 0\\) (the converse is not always true) 9.2 Correlation Coefficient \u00b6 The Pearson correlation coefficient normalizes covariance to the range \\([-1, 1]\\) : \\[\\boxed{\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X)} \\cdot \\sqrt{\\text{Var}(Y)}} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}}\\] Value Interpretation \\(\\rho = 1\\) Perfect positive linear relationship \\(\\rho = -1\\) Perfect negative linear relationship \\(\\rho = 0\\) No linear relationship (uncorrelated) \\(0 < \\rho < 1\\) Positive linear tendency \\(-1 < \\rho < 0\\) Negative linear tendency 9.3 Variance of a Sum (General Case) \u00b6 \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\,\\text{Cov}(X, Y)\\] If \\(X\\) and \\(Y\\) are independent (so \\(\\text{Cov}(X,Y) = 0\\) ): \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\] Worked Example: Using the joint PMF from Part 8, compute \\(\\text{Cov}(X, Y)\\) . From the table: \\(E[X] = 0(0.3) + 1(0.7) = 0.7\\) and \\(E[Y] = 0(0.4) + 1(0.6) = 0.6\\) . \\[E[XY] = \\sum_x \\sum_y xy \\, p(x,y) = (0)(0)(0.1) + (0)(1)(0.2) + (1)(0)(0.3) + (1)(1)(0.4) = 0.4\\] \\[\\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = 0.4 - (0.7)(0.6) = 0.4 - 0.42 = -0.02\\] The small negative covariance indicates a very slight negative association. Part 10: Sum Rule and Product Rule \u00b6 10.1 The Two Fundamental Rules \u00b6 These two rules form the foundation of all probabilistic reasoning. Product Rule (Chain Rule): \\[P(A, B) = P(A \\mid B) \\, P(B) = P(B \\mid A) \\, P(A)\\] This generalizes to multiple variables: \\[P(A, B, C) = P(A \\mid B, C) \\, P(B \\mid C) \\, P(C)\\] Sum Rule (Marginalization): \\[P(A) = \\sum_{B} P(A, B) = \\sum_{B} P(A \\mid B) \\, P(B)\\] 10.2 Discrete Case \u00b6 For discrete random variables \\(X\\) and \\(Y\\) : Product rule: \\[p(x, y) = p(x \\mid y) \\, p(y) = p(y \\mid x) \\, p(x)\\] Sum rule (marginalization): \\[p(x) = \\sum_{y} p(x, y) = \\sum_{y} p(x \\mid y) \\, p(y)\\] 10.3 Continuous Case \u00b6 For continuous random variables \\(X\\) and \\(Y\\) : Product rule: \\[f(x, y) = f(x \\mid y) \\, f(y) = f(y \\mid x) \\, f(x)\\] Sum rule (marginalization): \\[f(x) = \\int_{-\\infty}^{\\infty} f(x, y) \\, dy = \\int_{-\\infty}^{\\infty} f(x \\mid y) \\, f(y) \\, dy\\] 10.4 Connection to Bayes' Theorem \u00b6 Bayes' theorem is a direct consequence of applying the product rule in both directions and then dividing: \\[f(y \\mid x) = \\frac{f(x \\mid y) \\, f(y)}{f(x)} = \\frac{f(x \\mid y) \\, f(y)}{\\int f(x \\mid y') \\, f(y') \\, dy'}\\] The denominator uses the sum rule to compute the marginal \\(f(x)\\) . Worked Example: Suppose \\(Y \\in \\{0, 1\\}\\) with \\(P(Y=1) = 0.3\\) and \\(P(Y=0) = 0.7\\) . Also: - \\(P(X = 1 \\mid Y = 1) = 0.9\\) - \\(P(X = 1 \\mid Y = 0) = 0.2\\) Find \\(P(Y = 1 \\mid X = 1)\\) using the sum and product rules. Step 1 (Sum rule): Compute \\(P(X = 1)\\) . \\[P(X=1) = P(X=1 \\mid Y=1)P(Y=1) + P(X=1 \\mid Y=0)P(Y=0)$$ $$= (0.9)(0.3) + (0.2)(0.7) = 0.27 + 0.14 = 0.41\\] Step 2 (Bayes via product rule): \\[P(Y=1 \\mid X=1) = \\frac{P(X=1 \\mid Y=1) P(Y=1)}{P(X=1)} = \\frac{(0.9)(0.3)}{0.41} = \\frac{0.27}{0.41} \\approx 0.6585\\] Reference: Table of Common Distributions \u00b6 Discrete Distributions \u00b6 Distribution PMF \\(P(X = k)\\) Mean \\(E[X]\\) Variance \\(\\text{Var}(X)\\) \\(\\text{Bernoulli}(p)\\) \\(p^k(1-p)^{1-k}\\) , \\(k \\in \\{0,1\\}\\) \\(p\\) \\(p(1-p)\\) \\(\\text{Binomial}(n, p)\\) \\(\\binom{n}{k}p^k(1-p)^{n-k}\\) , \\(k = 0,\\ldots,n\\) \\(np\\) \\(np(1-p)\\) \\(\\text{Geometric}(p)\\) \\((1-p)^{k-1}p\\) , \\(k = 1, 2, \\ldots\\) \\(\\frac{1}{p}\\) \\(\\frac{1-p}{p^2}\\) \\(\\text{Poisson}(\\lambda)\\) \\(\\frac{\\lambda^k e^{-\\lambda}}{k!}\\) , \\(k = 0, 1, 2, \\ldots\\) \\(\\lambda\\) \\(\\lambda\\) Continuous Distributions \u00b6 Distribution PDF \\(f(x)\\) Mean \\(E[X]\\) Variance \\(\\text{Var}(X)\\) \\(\\text{Uniform}(a,b)\\) \\(\\frac{1}{b-a}\\) for \\(x \\in [a,b]\\) \\(\\frac{a+b}{2}\\) \\(\\frac{(b-a)^2}{12}\\) \\(\\text{Exponential}(\\lambda)\\) \\(\\lambda e^{-\\lambda x}\\) for \\(x \\geq 0\\) \\(\\frac{1}{\\lambda}\\) \\(\\frac{1}{\\lambda^2}\\) \\(\\mathcal{N}(\\mu, \\sigma^2)\\) \\(\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(x-\\mu)^2/(2\\sigma^2)}\\) \\(\\mu\\) \\(\\sigma^2\\) \\(\\text{Beta}(\\alpha, \\beta)\\) \\(\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\) for \\(x \\in [0,1]\\) \\(\\frac{\\alpha}{\\alpha+\\beta}\\) \\(\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\) Summary: Key Takeaways \u00b6 Probability Foundations \u00b6 A probability space is \\((\\Omega, \\mathcal{F}, P)\\) satisfying the Kolmogorov axioms Conditional probability: \\(P(A \\mid B) = P(A \\cap B) / P(B)\\) Bayes' theorem: \\(P(A \\mid B) = P(B \\mid A) P(A) / P(B)\\) Random Variables \u00b6 Discrete: described by PMFs; Continuous: described by PDFs CDF: \\(F(x) = P(X \\leq x)\\) works for both types Key Formulas \u00b6 Expected value: \\(E[X] = \\sum x \\, p(x)\\) or \\(\\int x \\, f(x) \\, dx\\) Variance shortcut: \\(\\text{Var}(X) = E[X^2] - (E[X])^2\\) Covariance: \\(\\text{Cov}(X,Y) = E[XY] - E[X]E[Y]\\) Correlation: \\(\\rho_{XY} = \\text{Cov}(X,Y) / (\\sigma_X \\sigma_Y)\\) Fundamental Rules \u00b6 Product rule: \\(P(A, B) = P(A \\mid B) P(B)\\) Sum rule: \\(P(A) = \\sum_B P(A, B)\\) Practice Problems \u00b6 Problem 1 \u00b6 A bag contains 5 red balls and 3 blue balls. Two balls are drawn without replacement. What is the probability that both are red? Problem 2 \u00b6 A factory has two machines. Machine A produces 60% of items and Machine B produces 40%. The defect rate is 3% for Machine A and 5% for Machine B. If an item is found to be defective, what is the probability it came from Machine A? Problem 3 \u00b6 Let \\(X \\sim \\text{Binomial}(8, 0.3)\\) . Compute \\(P(X = 2)\\) , \\(E[X]\\) , and \\(\\text{Var}(X)\\) . Problem 4 \u00b6 Let \\(X \\sim \\mathcal{N}(50, 25)\\) (mean 50, variance 25, so \\(\\sigma = 5\\) ). Find: - (a) \\(P(X > 60)\\) - (b) \\(P(40 < X < 55)\\) Problem 5 \u00b6 Random variables \\(X\\) and \\(Y\\) have \\(E[X] = 3\\) , \\(E[Y] = 5\\) , \\(E[X^2] = 13\\) , \\(E[Y^2] = 30\\) , and \\(E[XY] = 16\\) . Compute \\(\\text{Cov}(X,Y)\\) , \\(\\text{Var}(X)\\) , \\(\\text{Var}(Y)\\) , and the correlation \\(\\rho_{XY}\\) . Problem 6 \u00b6 Consider the continuous random variable \\(X\\) with PDF: \\[f(x) = \\begin{cases} cx^2 & \\text{if } 0 \\leq x \\leq 2 \\\\ 0 & \\text{otherwise} \\end{cases}\\] (a) Find the constant \\(c\\) so that \\(f\\) is a valid PDF. (b) Compute \\(E[X]\\) . (c) Compute \\(\\text{Var}(X)\\) . Solutions \u00b6 Solution 1: Use the product rule (chain rule) for drawing without replacement. \\[P(\\text{both red}) = P(R_1) \\cdot P(R_2 \\mid R_1) = \\frac{5}{8} \\cdot \\frac{4}{7} = \\frac{20}{56} = \\frac{5}{14} \\approx 0.3571\\] Solution 2: Apply Bayes' theorem. Let \\(A\\) = \"from Machine A,\" \\(B\\) = \"from Machine B,\" and \\(D\\) = \"defective.\" \\(P(A) = 0.6\\) , \\(P(B) = 0.4\\) \\(P(D \\mid A) = 0.03\\) , \\(P(D \\mid B) = 0.05\\) First, compute \\(P(D)\\) using the law of total probability: \\[P(D) = P(D \\mid A)P(A) + P(D \\mid B)P(B) = (0.03)(0.6) + (0.05)(0.4) = 0.018 + 0.02 = 0.038\\] Then: \\[P(A \\mid D) = \\frac{P(D \\mid A) P(A)}{P(D)} = \\frac{(0.03)(0.6)}{0.038} = \\frac{0.018}{0.038} \\approx 0.4737\\] There is about a 47.4% chance the defective item came from Machine A. Solution 3: \\(X \\sim \\text{Binomial}(8, 0.3)\\) . \\[P(X = 2) = \\binom{8}{2}(0.3)^2(0.7)^6 = 28 \\cdot 0.09 \\cdot 0.117649 = 28 \\cdot 0.01058841 \\approx 0.2965\\] \\[E[X] = np = 8 \\times 0.3 = 2.4\\] \\[\\text{Var}(X) = np(1-p) = 8 \\times 0.3 \\times 0.7 = 1.68\\] Solution 4: \\(X \\sim \\mathcal{N}(50, 25)\\) , so \\(\\mu = 50\\) and \\(\\sigma = 5\\) . (a) Standardize: \\[Z = \\frac{60 - 50}{5} = 2.0\\] \\[P(X > 60) = P(Z > 2.0) = 1 - \\Phi(2.0) \\approx 1 - 0.9772 = 0.0228\\] About 2.3% of the distribution lies above 60. (b) Standardize both bounds: \\[Z_1 = \\frac{40 - 50}{5} = -2.0, \\quad Z_2 = \\frac{55 - 50}{5} = 1.0\\] \\[P(40 < X < 55) = \\Phi(1.0) - \\Phi(-2.0) \\approx 0.8413 - 0.0228 = 0.8185\\] About 81.9% of the distribution falls between 40 and 55. Solution 5: Covariance: \\[\\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = 16 - (3)(5) = 16 - 15 = 1\\] Variance of \\(X\\) : \\[\\text{Var}(X) = E[X^2] - (E[X])^2 = 13 - 9 = 4 \\quad \\Rightarrow \\quad \\sigma_X = 2\\] Variance of \\(Y\\) : \\[\\text{Var}(Y) = E[Y^2] - (E[Y])^2 = 30 - 25 = 5 \\quad \\Rightarrow \\quad \\sigma_Y = \\sqrt{5}\\] Correlation: \\[\\rho_{XY} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{1}{2\\sqrt{5}} = \\frac{1}{2\\sqrt{5}} \\cdot \\frac{\\sqrt{5}}{\\sqrt{5}} = \\frac{\\sqrt{5}}{10} \\approx 0.2236\\] A moderate positive linear association. Solution 6: (a) For \\(f\\) to be a valid PDF, the total area must equal 1: \\[\\int_0^2 cx^2 \\, dx = c \\left[\\frac{x^3}{3}\\right]_0^2 = c \\cdot \\frac{8}{3} = 1\\] \\[c = \\frac{3}{8}\\] (b) Expected value: \\[E[X] = \\int_0^2 x \\cdot \\frac{3}{8}x^2 \\, dx = \\frac{3}{8}\\int_0^2 x^3 \\, dx = \\frac{3}{8}\\left[\\frac{x^4}{4}\\right]_0^2 = \\frac{3}{8} \\cdot \\frac{16}{4} = \\frac{3}{8} \\cdot 4 = \\frac{3}{2} = 1.5\\] (c) First compute \\(E[X^2]\\) : \\[E[X^2] = \\int_0^2 x^2 \\cdot \\frac{3}{8}x^2 \\, dx = \\frac{3}{8}\\int_0^2 x^4 \\, dx = \\frac{3}{8}\\left[\\frac{x^5}{5}\\right]_0^2 = \\frac{3}{8} \\cdot \\frac{32}{5} = \\frac{96}{40} = \\frac{12}{5} = 2.4\\] Then apply the variance shortcut: \\[\\text{Var}(X) = E[X^2] - (E[X])^2 = 2.4 - (1.5)^2 = 2.4 - 2.25 = 0.15\\] Equivalently, \\(\\text{Var}(X) = \\frac{3}{20}\\) . Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Previous: Tutorial 4 - Matrix Decompositions Next: Tutorial 6 - Optimization and Gradient Descent","title":"Tutorial 5: Probability and Distributions"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#tutorial-5-probability-and-distributions","text":"Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari","title":"Tutorial 5: Probability and Distributions"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#learning-objectives","text":"By the end of this tutorial, you will understand: Probability spaces, sample spaces, events, and the Kolmogorov axioms Conditional probability and independence Bayes' theorem and how to apply it Discrete random variables and their distributions (Bernoulli, Binomial, Geometric) Continuous random variables and their distributions (Uniform, Exponential, Gaussian) Expected value and variance, including computation rules The Gaussian (Normal) distribution and its properties Joint and marginal distributions Covariance and correlation The sum rule and product rule of probability","title":"Learning Objectives"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#part-1-probability-space","text":"","title":"Part 1: Probability Space"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#11-core-definitions","text":"A probability space is a triple \\((\\Omega, \\mathcal{F}, P)\\) consisting of three components: Component Name Description \\(\\Omega\\) Sample space The set of all possible outcomes of an experiment \\(\\mathcal{F}\\) Event space A collection of subsets of \\(\\Omega\\) (the events we can assign probabilities to) \\(P\\) Probability function A function \\(P: \\mathcal{F} \\to [0, 1]\\) that assigns probabilities to events Example (Coin Flip): - Sample space: \\(\\Omega = \\{H, T\\}\\) - Event space: \\(\\mathcal{F} = \\{\\emptyset, \\{H\\}, \\{T\\}, \\{H, T\\}\\}\\) - Probability: \\(P(\\{H\\}) = 0.5\\) , \\(P(\\{T\\}) = 0.5\\) Example (Rolling a Die): - Sample space: \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\) - Event \"rolling an even number\": \\(A = \\{2, 4, 6\\}\\) - \\(P(A) = \\frac{3}{6} = \\frac{1}{2}\\)","title":"1.1 Core Definitions"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#12-kolmogorov-axioms-of-probability","text":"All of probability theory rests on three axioms, formalized by Andrey Kolmogorov: Axiom Statement Meaning Axiom 1 (Non-negativity) \\(P(A) \\geq 0\\) for every event \\(A\\) Probabilities are never negative Axiom 2 (Normalization) \\(P(\\Omega) = 1\\) Something must happen Axiom 3 (Additivity) If \\(A \\cap B = \\emptyset\\) , then \\(P(A \\cup B) = P(A) + P(B)\\) For mutually exclusive events, probabilities add Key consequences of the axioms: \\(P(\\emptyset) = 0\\) (the impossible event has probability zero) \\(P(A^c) = 1 - P(A)\\) (complement rule) \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) (inclusion-exclusion) If \\(A \\subseteq B\\) , then \\(P(A) \\leq P(B)\\) (monotonicity) Worked Example: Suppose \\(P(A) = 0.6\\) and \\(P(B) = 0.4\\) with \\(P(A \\cap B) = 0.2\\) . Find \\(P(A \\cup B)\\) . \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = 0.6 + 0.4 - 0.2 = 0.8\\]","title":"1.2 Kolmogorov Axioms of Probability"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#part-2-conditional-probability-and-independence","text":"","title":"Part 2: Conditional Probability and Independence"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#21-conditional-probability","text":"The conditional probability of event \\(A\\) given that event \\(B\\) has occurred is: \\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}, \\quad \\text{provided } P(B) > 0\\] This reads: \"the probability of \\(A\\) given \\(B\\) .\" Intuition: Once we know \\(B\\) happened, the sample space effectively shrinks to \\(B\\) , and we ask how much of \\(B\\) is also in \\(A\\) . Worked Example: A standard deck of 52 cards. What is the probability a card is a King given it is a face card? \\(B\\) = face card: there are 12 face cards (J, Q, K of each suit), so \\(P(B) = \\frac{12}{52}\\) \\(A \\cap B\\) = King and face card = King: there are 4 Kings, so \\(P(A \\cap B) = \\frac{4}{52}\\) \\[P(\\text{King} \\mid \\text{Face card}) = \\frac{P(A \\cap B)}{P(B)} = \\frac{4/52}{12/52} = \\frac{4}{12} = \\frac{1}{3}\\]","title":"2.1 Conditional Probability"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#22-independence","text":"Two events \\(A\\) and \\(B\\) are independent if knowing one gives no information about the other. Formally: \\[P(A \\cap B) = P(A) \\cdot P(B)\\] Equivalently, if \\(A\\) and \\(B\\) are independent: \\[P(A \\mid B) = P(A) \\quad \\text{and} \\quad P(B \\mid A) = P(B)\\] Example: Rolling two fair dice. Let \\(A\\) = \"first die shows 3\" and \\(B\\) = \"second die shows 5.\" \\[P(A) = \\frac{1}{6}, \\quad P(B) = \\frac{1}{6}, \\quad P(A \\cap B) = \\frac{1}{36} = \\frac{1}{6} \\cdot \\frac{1}{6}\\] Since \\(P(A \\cap B) = P(A)P(B)\\) , the events are independent. Warning: Independence is not the same as mutual exclusivity. If \\(A\\) and \\(B\\) are mutually exclusive and both have positive probability, they are not independent (knowing one happened tells you the other did not).","title":"2.2 Independence"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#part-3-bayes-theorem","text":"","title":"Part 3: Bayes' Theorem"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#31-the-formula","text":"Bayes' theorem lets us \"reverse\" conditional probabilities: \\[\\boxed{P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}}\\] Term Name Interpretation \\(P(A \\mid B)\\) Posterior Updated belief about \\(A\\) after observing \\(B\\) \\(P(A)\\) Prior Initial belief about \\(A\\) before seeing evidence \\(P(B \\mid A)\\) Likelihood How probable the evidence \\(B\\) is if \\(A\\) is true \\(P(B)\\) Evidence (marginal likelihood) Total probability of observing \\(B\\)","title":"3.1 The Formula"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#32-the-law-of-total-probability","text":"The denominator \\(P(B)\\) is often computed using the law of total probability . If \\(A_1, A_2, \\ldots, A_n\\) partition \\(\\Omega\\) : \\[P(B) = \\sum_{i=1}^{n} P(B \\mid A_i) \\, P(A_i)\\] For two complementary events \\(A\\) and \\(A^c\\) : \\[P(B) = P(B \\mid A) \\, P(A) + P(B \\mid A^c) \\, P(A^c)\\]","title":"3.2 The Law of Total Probability"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#33-worked-example-medical-testing","text":"A disease affects 1% of the population. A test has: - Sensitivity (true positive rate): \\(P(\\text{Positive} \\mid \\text{Disease}) = 0.95\\) - Specificity (true negative rate): \\(P(\\text{Negative} \\mid \\text{No Disease}) = 0.90\\) If a person tests positive, what is \\(P(\\text{Disease} \\mid \\text{Positive})\\) ? Step 1: Define events and assign values. - \\(P(D) = 0.01\\) , \\(P(D^c) = 0.99\\) - \\(P(+ \\mid D) = 0.95\\) , \\(P(+ \\mid D^c) = 1 - 0.90 = 0.10\\) Step 2: Compute \\(P(+)\\) using the law of total probability. \\[P(+) = P(+ \\mid D) \\, P(D) + P(+ \\mid D^c) \\, P(D^c)$$ $$P(+) = (0.95)(0.01) + (0.10)(0.99) = 0.0095 + 0.099 = 0.1085\\] Step 3: Apply Bayes' theorem. \\[P(D \\mid +) = \\frac{P(+ \\mid D) \\, P(D)}{P(+)} = \\frac{(0.95)(0.01)}{0.1085} = \\frac{0.0095}{0.1085} \\approx 0.0876\\] Interpretation: Even with a positive test, there is only about an 8.8% chance the person actually has the disease. This counterintuitive result arises because the disease is rare (low prior), so false positives outnumber true positives.","title":"3.3 Worked Example: Medical Testing"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#part-4-discrete-random-variables","text":"","title":"Part 4: Discrete Random Variables"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#41-definitions","text":"A random variable \\(X\\) is a function that maps outcomes in the sample space to real numbers: \\[X: \\Omega \\to \\mathbb{R}\\] A random variable is discrete if it takes values from a countable set (e.g., \\(\\{0, 1, 2, \\ldots\\}\\) ). The probability mass function (PMF) of a discrete random variable \\(X\\) is: \\[p(x) = P(X = x)\\] Properties of a valid PMF: \\(p(x) \\geq 0\\) for all \\(x\\) \\(\\displaystyle\\sum_{\\text{all } x} p(x) = 1\\)","title":"4.1 Definitions"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#42-bernoulli-distribution","text":"A single trial with two outcomes: success ( \\(X = 1\\) ) with probability \\(p\\) , or failure ( \\(X = 0\\) ) with probability \\(1 - p\\) . \\[X \\sim \\text{Bernoulli}(p)\\] \\[P(X = x) = p^x (1-p)^{1-x}, \\quad x \\in \\{0, 1\\}\\] Property Value Mean \\(E[X] = p\\) Variance \\(\\text{Var}(X) = p(1-p)\\) Example: A coin flip with \\(P(\\text{Heads}) = 0.6\\) . Then \\(X \\sim \\text{Bernoulli}(0.6)\\) .","title":"4.2 Bernoulli Distribution"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#43-binomial-distribution","text":"The number of successes in \\(n\\) independent Bernoulli trials, each with success probability \\(p\\) . \\[X \\sim \\text{Binomial}(n, p)\\] \\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0, 1, 2, \\ldots, n\\] where \\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\) is the binomial coefficient. Property Value Mean \\(E[X] = np\\) Variance \\(\\text{Var}(X) = np(1-p)\\) Worked Example: A fair coin is flipped 10 times. What is the probability of getting exactly 4 heads? \\[P(X = 4) = \\binom{10}{4} (0.5)^4 (0.5)^{6} = \\binom{10}{4} (0.5)^{10}$$ $$= \\frac{10!}{4! \\cdot 6!} \\cdot \\frac{1}{1024} = \\frac{210}{1024} \\approx 0.2051\\]","title":"4.3 Binomial Distribution"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#44-geometric-distribution","text":"The number of trials until the first success in a sequence of independent Bernoulli trials. \\[X \\sim \\text{Geometric}(p)\\] \\[P(X = k) = (1-p)^{k-1} p, \\quad k = 1, 2, 3, \\ldots\\] Property Value Mean \\(E[X] = \\frac{1}{p}\\) Variance \\(\\text{Var}(X) = \\frac{1-p}{p^2}\\) Worked Example: You roll a fair die until you get a 6. What is the probability it takes exactly 3 rolls? \\[P(X = 3) = \\left(\\frac{5}{6}\\right)^{2} \\cdot \\frac{1}{6} = \\frac{25}{36} \\cdot \\frac{1}{6} = \\frac{25}{216} \\approx 0.1157\\] The expected number of rolls: \\(E[X] = \\frac{1}{1/6} = 6\\) .","title":"4.4 Geometric Distribution"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#part-5-continuous-random-variables","text":"","title":"Part 5: Continuous Random Variables"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#51-definitions","text":"A random variable is continuous if it can take any value in an interval (or union of intervals). The probability density function (PDF) \\(f(x)\\) satisfies: \\(f(x) \\geq 0\\) for all \\(x\\) \\(\\displaystyle\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\) \\(\\displaystyle P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx\\) Important: For a continuous random variable, \\(P(X = x) = 0\\) for any specific value \\(x\\) . Only intervals have nonzero probability. The cumulative distribution function (CDF) is: \\[F(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f(t) \\, dt\\] Properties of the CDF: - \\(F(-\\infty) = 0\\) and \\(F(\\infty) = 1\\) - \\(F\\) is non-decreasing - \\(f(x) = \\frac{d}{dx} F(x)\\) (the PDF is the derivative of the CDF)","title":"5.1 Definitions"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#52-uniform-distribution","text":"A random variable is equally likely to take any value in the interval \\([a, b]\\) . \\[X \\sim \\text{Uniform}(a, b)\\] \\[f(x) = \\begin{cases} \\frac{1}{b - a} & \\text{if } a \\leq x \\leq b \\\\ 0 & \\text{otherwise} \\end{cases}\\] Property Value Mean \\(E[X] = \\frac{a + b}{2}\\) Variance \\(\\text{Var}(X) = \\frac{(b - a)^2}{12}\\) Example: If \\(X \\sim \\text{Uniform}(0, 10)\\) , then \\(E[X] = 5\\) and \\(P(2 \\leq X \\leq 5) = \\frac{5-2}{10-0} = 0.3\\) .","title":"5.2 Uniform Distribution"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#53-exponential-distribution","text":"Models the time between events in a Poisson process. The parameter \\(\\lambda > 0\\) is the rate. \\[X \\sim \\text{Exponential}(\\lambda)\\] \\[f(x) = \\begin{cases} \\lambda e^{-\\lambda x} & \\text{if } x \\geq 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}\\] \\[F(x) = 1 - e^{-\\lambda x}, \\quad x \\geq 0\\] Property Value Mean \\(E[X] = \\frac{1}{\\lambda}\\) Variance \\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\) Key property (Memoryless): \\[P(X > s + t \\mid X > s) = P(X > t)\\] Worked Example: Light bulbs fail at a rate of \\(\\lambda = 0.01\\) per hour. What is the probability a bulb lasts more than 200 hours? \\[P(X > 200) = 1 - F(200) = e^{-0.01 \\cdot 200} = e^{-2} \\approx 0.1353\\]","title":"5.3 Exponential Distribution"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#54-gaussian-normal-distribution","text":"The most important distribution in statistics and machine learning. \\[X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\] \\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\] Property Value Mean \\(E[X] = \\mu\\) Variance \\(\\text{Var}(X) = \\sigma^2\\) Full details on the Gaussian are in Part 7 below.","title":"5.4 Gaussian (Normal) Distribution"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#part-6-expected-value-and-variance","text":"","title":"Part 6: Expected Value and Variance"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#61-expected-value-mean","text":"The expected value is the long-run average of a random variable. Discrete case: \\[E[X] = \\sum_{x} x \\, p(x)\\] Continuous case: \\[E[X] = \\int_{-\\infty}^{\\infty} x \\, f(x) \\, dx\\] Expected value of a function \\(g(X)\\) : \\[E[g(X)] = \\sum_{x} g(x) \\, p(x) \\quad \\text{(discrete)}\\] \\[E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) \\, f(x) \\, dx \\quad \\text{(continuous)}\\]","title":"6.1 Expected Value (Mean)"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#62-properties-of-expected-value","text":"Property Formula Linearity \\(E[aX + b] = aE[X] + b\\) Sum \\(E[X + Y] = E[X] + E[Y]\\) (always, even if dependent) Product (independent) \\(E[XY] = E[X] \\cdot E[Y]\\) (only if \\(X, Y\\) are independent) Constant \\(E[c] = c\\) Worked Example: Let \\(X\\) be a die roll. Compute \\(E[X]\\) . \\[E[X] = \\sum_{x=1}^{6} x \\cdot \\frac{1}{6} = \\frac{1}{6}(1 + 2 + 3 + 4 + 5 + 6) = \\frac{21}{6} = 3.5\\]","title":"6.2 Properties of Expected Value"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#63-variance","text":"Variance measures how spread out a distribution is around its mean. \\[\\text{Var}(X) = E\\left[(X - E[X])^2\\right]\\] Shortcut formula (very useful for computation): \\[\\boxed{\\text{Var}(X) = E[X^2] - (E[X])^2}\\] The standard deviation is \\(\\sigma = \\sqrt{\\text{Var}(X)}\\) .","title":"6.3 Variance"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#64-properties-of-variance","text":"Property Formula Scaling \\(\\text{Var}(aX) = a^2 \\text{Var}(X)\\) Shift \\(\\text{Var}(X + b) = \\text{Var}(X)\\) Affine \\(\\text{Var}(aX + b) = a^2 \\text{Var}(X)\\) Sum (independent) \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\) (only if independent) Constant \\(\\text{Var}(c) = 0\\) Worked Example: Let \\(X\\) be a die roll. Compute \\(\\text{Var}(X)\\) . First, compute \\(E[X^2]\\) : \\[E[X^2] = \\frac{1}{6}(1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2) = \\frac{1}{6}(1 + 4 + 9 + 16 + 25 + 36) = \\frac{91}{6} \\approx 15.167\\] We already know \\(E[X] = 3.5\\) , so \\((E[X])^2 = 12.25\\) . \\[\\text{Var}(X) = E[X^2] - (E[X])^2 = \\frac{91}{6} - \\frac{49}{4} = \\frac{182}{12} - \\frac{147}{12} = \\frac{35}{12} \\approx 2.917\\]","title":"6.4 Properties of Variance"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#part-7-gaussian-normal-distribution-in-depth","text":"","title":"Part 7: Gaussian (Normal) Distribution in Depth"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#71-definition","text":"The Gaussian distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) has PDF: \\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right), \\quad x \\in \\mathbb{R}\\] We write \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) .","title":"7.1 Definition"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#72-the-standard-normal-distribution","text":"When \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\) , we get the standard normal \\(Z \\sim \\mathcal{N}(0, 1)\\) : \\[\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)\\] Standardization: Any normal random variable can be converted to a standard normal: \\[Z = \\frac{X - \\mu}{\\sigma}\\]","title":"7.2 The Standard Normal Distribution"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#73-key-properties","text":"Property Description Symmetry The PDF is symmetric about \\(\\mu\\) 68-95-99.7 Rule ~68% of values fall within \\(\\mu \\pm \\sigma\\) , ~95% within \\(\\mu \\pm 2\\sigma\\) , ~99.7% within \\(\\mu \\pm 3\\sigma\\) Linear closure If \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) , then \\(aX + b \\sim \\mathcal{N}(a\\mu + b, \\, a^2\\sigma^2)\\) Sum of normals If \\(X \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)\\) and \\(Y \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)\\) are independent, then \\(X + Y \\sim \\mathcal{N}(\\mu_1 + \\mu_2, \\, \\sigma_1^2 + \\sigma_2^2)\\) Worked Example: Exam scores are distributed as \\(X \\sim \\mathcal{N}(75, 100)\\) (mean 75, standard deviation 10). What fraction of students score above 90? Standardize: \\[Z = \\frac{90 - 75}{10} = 1.5\\] \\[P(X > 90) = P(Z > 1.5) = 1 - \\Phi(1.5) \\approx 1 - 0.9332 = 0.0668\\] About 6.7% of students score above 90.","title":"7.3 Key Properties"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#74-why-the-gaussian-matters-in-machine-learning","text":"The Central Limit Theorem states that the sum of many independent random variables tends toward a Gaussian, regardless of their individual distributions. Many ML algorithms assume Gaussian noise (linear regression, Gaussian processes). The multivariate Gaussian is fundamental to dimensionality reduction (PCA) and generative models.","title":"7.4 Why the Gaussian Matters in Machine Learning"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#part-8-joint-and-marginal-distributions","text":"","title":"Part 8: Joint and Marginal Distributions"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#81-joint-distribution","text":"The joint distribution describes the probability behavior of two (or more) random variables simultaneously. Discrete (Joint PMF): \\[p(x, y) = P(X = x, Y = y)\\] Properties: - \\(p(x, y) \\geq 0\\) for all \\(x, y\\) - \\(\\displaystyle\\sum_{x}\\sum_{y} p(x, y) = 1\\) Continuous (Joint PDF): \\[f(x, y) \\geq 0 \\quad \\text{and} \\quad \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} f(x, y) \\, dx \\, dy = 1\\]","title":"8.1 Joint Distribution"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#82-marginal-distribution","text":"The marginal distribution of one variable is obtained by summing (or integrating) over the other variable. Discrete: \\[p_X(x) = \\sum_{y} p(x, y) \\qquad \\text{and} \\qquad p_Y(y) = \\sum_{x} p(x, y)\\] Continuous: \\[f_X(x) = \\int_{-\\infty}^{\\infty} f(x, y) \\, dy \\qquad \\text{and} \\qquad f_Y(y) = \\int_{-\\infty}^{\\infty} f(x, y) \\, dx\\] Worked Example (Discrete): Consider two discrete random variables \\(X\\) and \\(Y\\) with the following joint PMF table: \\(Y = 0\\) \\(Y = 1\\) \\(p_X(x)\\) \\(X = 0\\) 0.1 0.2 0.3 \\(X = 1\\) 0.3 0.4 0.7 \\(p_Y(y)\\) 0.4 0.6 1.0 Marginals are computed by summing each row or column: - \\(p_X(0) = 0.1 + 0.2 = 0.3\\) - \\(p_X(1) = 0.3 + 0.4 = 0.7\\) - \\(p_Y(0) = 0.1 + 0.3 = 0.4\\) - \\(p_Y(1) = 0.2 + 0.4 = 0.6\\)","title":"8.2 Marginal Distribution"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#83-independence-of-random-variables","text":"\\(X\\) and \\(Y\\) are independent if and only if: \\[p(x, y) = p_X(x) \\cdot p_Y(y) \\quad \\text{for all } x, y\\] Check the example above: \\(p(0, 0) = 0.1\\) but \\(p_X(0) \\cdot p_Y(0) = 0.3 \\times 0.4 = 0.12 \\neq 0.1\\) . So \\(X\\) and \\(Y\\) are not independent.","title":"8.3 Independence of Random Variables"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#part-9-covariance-and-correlation","text":"","title":"Part 9: Covariance and Correlation"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#91-covariance","text":"Covariance measures how two random variables vary together: \\[\\text{Cov}(X, Y) = E\\left[(X - E[X])(Y - E[Y])\\right]\\] Shortcut formula: \\[\\boxed{\\text{Cov}(X, Y) = E[XY] - E[X] \\cdot E[Y]}\\] Value Interpretation \\(\\text{Cov}(X,Y) > 0\\) \\(X\\) and \\(Y\\) tend to increase together \\(\\text{Cov}(X,Y) < 0\\) When \\(X\\) increases, \\(Y\\) tends to decrease \\(\\text{Cov}(X,Y) = 0\\) No linear relationship (uncorrelated) Properties of Covariance: \\(\\text{Cov}(X, X) = \\text{Var}(X)\\) \\(\\text{Cov}(X, Y) = \\text{Cov}(Y, X)\\) (symmetric) \\(\\text{Cov}(aX + b, \\, cY + d) = ac \\, \\text{Cov}(X, Y)\\) If \\(X\\) and \\(Y\\) are independent, then \\(\\text{Cov}(X, Y) = 0\\) (the converse is not always true)","title":"9.1 Covariance"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#92-correlation-coefficient","text":"The Pearson correlation coefficient normalizes covariance to the range \\([-1, 1]\\) : \\[\\boxed{\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X)} \\cdot \\sqrt{\\text{Var}(Y)}} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}}\\] Value Interpretation \\(\\rho = 1\\) Perfect positive linear relationship \\(\\rho = -1\\) Perfect negative linear relationship \\(\\rho = 0\\) No linear relationship (uncorrelated) \\(0 < \\rho < 1\\) Positive linear tendency \\(-1 < \\rho < 0\\) Negative linear tendency","title":"9.2 Correlation Coefficient"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#93-variance-of-a-sum-general-case","text":"\\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\,\\text{Cov}(X, Y)\\] If \\(X\\) and \\(Y\\) are independent (so \\(\\text{Cov}(X,Y) = 0\\) ): \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\] Worked Example: Using the joint PMF from Part 8, compute \\(\\text{Cov}(X, Y)\\) . From the table: \\(E[X] = 0(0.3) + 1(0.7) = 0.7\\) and \\(E[Y] = 0(0.4) + 1(0.6) = 0.6\\) . \\[E[XY] = \\sum_x \\sum_y xy \\, p(x,y) = (0)(0)(0.1) + (0)(1)(0.2) + (1)(0)(0.3) + (1)(1)(0.4) = 0.4\\] \\[\\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = 0.4 - (0.7)(0.6) = 0.4 - 0.42 = -0.02\\] The small negative covariance indicates a very slight negative association.","title":"9.3 Variance of a Sum (General Case)"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#part-10-sum-rule-and-product-rule","text":"","title":"Part 10: Sum Rule and Product Rule"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#101-the-two-fundamental-rules","text":"These two rules form the foundation of all probabilistic reasoning. Product Rule (Chain Rule): \\[P(A, B) = P(A \\mid B) \\, P(B) = P(B \\mid A) \\, P(A)\\] This generalizes to multiple variables: \\[P(A, B, C) = P(A \\mid B, C) \\, P(B \\mid C) \\, P(C)\\] Sum Rule (Marginalization): \\[P(A) = \\sum_{B} P(A, B) = \\sum_{B} P(A \\mid B) \\, P(B)\\]","title":"10.1 The Two Fundamental Rules"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#102-discrete-case","text":"For discrete random variables \\(X\\) and \\(Y\\) : Product rule: \\[p(x, y) = p(x \\mid y) \\, p(y) = p(y \\mid x) \\, p(x)\\] Sum rule (marginalization): \\[p(x) = \\sum_{y} p(x, y) = \\sum_{y} p(x \\mid y) \\, p(y)\\]","title":"10.2 Discrete Case"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#103-continuous-case","text":"For continuous random variables \\(X\\) and \\(Y\\) : Product rule: \\[f(x, y) = f(x \\mid y) \\, f(y) = f(y \\mid x) \\, f(x)\\] Sum rule (marginalization): \\[f(x) = \\int_{-\\infty}^{\\infty} f(x, y) \\, dy = \\int_{-\\infty}^{\\infty} f(x \\mid y) \\, f(y) \\, dy\\]","title":"10.3 Continuous Case"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#104-connection-to-bayes-theorem","text":"Bayes' theorem is a direct consequence of applying the product rule in both directions and then dividing: \\[f(y \\mid x) = \\frac{f(x \\mid y) \\, f(y)}{f(x)} = \\frac{f(x \\mid y) \\, f(y)}{\\int f(x \\mid y') \\, f(y') \\, dy'}\\] The denominator uses the sum rule to compute the marginal \\(f(x)\\) . Worked Example: Suppose \\(Y \\in \\{0, 1\\}\\) with \\(P(Y=1) = 0.3\\) and \\(P(Y=0) = 0.7\\) . Also: - \\(P(X = 1 \\mid Y = 1) = 0.9\\) - \\(P(X = 1 \\mid Y = 0) = 0.2\\) Find \\(P(Y = 1 \\mid X = 1)\\) using the sum and product rules. Step 1 (Sum rule): Compute \\(P(X = 1)\\) . \\[P(X=1) = P(X=1 \\mid Y=1)P(Y=1) + P(X=1 \\mid Y=0)P(Y=0)$$ $$= (0.9)(0.3) + (0.2)(0.7) = 0.27 + 0.14 = 0.41\\] Step 2 (Bayes via product rule): \\[P(Y=1 \\mid X=1) = \\frac{P(X=1 \\mid Y=1) P(Y=1)}{P(X=1)} = \\frac{(0.9)(0.3)}{0.41} = \\frac{0.27}{0.41} \\approx 0.6585\\]","title":"10.4 Connection to Bayes' Theorem"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#reference-table-of-common-distributions","text":"","title":"Reference: Table of Common Distributions"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#discrete-distributions","text":"Distribution PMF \\(P(X = k)\\) Mean \\(E[X]\\) Variance \\(\\text{Var}(X)\\) \\(\\text{Bernoulli}(p)\\) \\(p^k(1-p)^{1-k}\\) , \\(k \\in \\{0,1\\}\\) \\(p\\) \\(p(1-p)\\) \\(\\text{Binomial}(n, p)\\) \\(\\binom{n}{k}p^k(1-p)^{n-k}\\) , \\(k = 0,\\ldots,n\\) \\(np\\) \\(np(1-p)\\) \\(\\text{Geometric}(p)\\) \\((1-p)^{k-1}p\\) , \\(k = 1, 2, \\ldots\\) \\(\\frac{1}{p}\\) \\(\\frac{1-p}{p^2}\\) \\(\\text{Poisson}(\\lambda)\\) \\(\\frac{\\lambda^k e^{-\\lambda}}{k!}\\) , \\(k = 0, 1, 2, \\ldots\\) \\(\\lambda\\) \\(\\lambda\\)","title":"Discrete Distributions"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#continuous-distributions","text":"Distribution PDF \\(f(x)\\) Mean \\(E[X]\\) Variance \\(\\text{Var}(X)\\) \\(\\text{Uniform}(a,b)\\) \\(\\frac{1}{b-a}\\) for \\(x \\in [a,b]\\) \\(\\frac{a+b}{2}\\) \\(\\frac{(b-a)^2}{12}\\) \\(\\text{Exponential}(\\lambda)\\) \\(\\lambda e^{-\\lambda x}\\) for \\(x \\geq 0\\) \\(\\frac{1}{\\lambda}\\) \\(\\frac{1}{\\lambda^2}\\) \\(\\mathcal{N}(\\mu, \\sigma^2)\\) \\(\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(x-\\mu)^2/(2\\sigma^2)}\\) \\(\\mu\\) \\(\\sigma^2\\) \\(\\text{Beta}(\\alpha, \\beta)\\) \\(\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\) for \\(x \\in [0,1]\\) \\(\\frac{\\alpha}{\\alpha+\\beta}\\) \\(\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)","title":"Continuous Distributions"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#summary-key-takeaways","text":"","title":"Summary: Key Takeaways"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#probability-foundations","text":"A probability space is \\((\\Omega, \\mathcal{F}, P)\\) satisfying the Kolmogorov axioms Conditional probability: \\(P(A \\mid B) = P(A \\cap B) / P(B)\\) Bayes' theorem: \\(P(A \\mid B) = P(B \\mid A) P(A) / P(B)\\)","title":"Probability Foundations"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#random-variables","text":"Discrete: described by PMFs; Continuous: described by PDFs CDF: \\(F(x) = P(X \\leq x)\\) works for both types","title":"Random Variables"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#key-formulas","text":"Expected value: \\(E[X] = \\sum x \\, p(x)\\) or \\(\\int x \\, f(x) \\, dx\\) Variance shortcut: \\(\\text{Var}(X) = E[X^2] - (E[X])^2\\) Covariance: \\(\\text{Cov}(X,Y) = E[XY] - E[X]E[Y]\\) Correlation: \\(\\rho_{XY} = \\text{Cov}(X,Y) / (\\sigma_X \\sigma_Y)\\)","title":"Key Formulas"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#fundamental-rules","text":"Product rule: \\(P(A, B) = P(A \\mid B) P(B)\\) Sum rule: \\(P(A) = \\sum_B P(A, B)\\)","title":"Fundamental Rules"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#practice-problems","text":"","title":"Practice Problems"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#problem-1","text":"A bag contains 5 red balls and 3 blue balls. Two balls are drawn without replacement. What is the probability that both are red?","title":"Problem 1"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#problem-2","text":"A factory has two machines. Machine A produces 60% of items and Machine B produces 40%. The defect rate is 3% for Machine A and 5% for Machine B. If an item is found to be defective, what is the probability it came from Machine A?","title":"Problem 2"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#problem-3","text":"Let \\(X \\sim \\text{Binomial}(8, 0.3)\\) . Compute \\(P(X = 2)\\) , \\(E[X]\\) , and \\(\\text{Var}(X)\\) .","title":"Problem 3"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#problem-4","text":"Let \\(X \\sim \\mathcal{N}(50, 25)\\) (mean 50, variance 25, so \\(\\sigma = 5\\) ). Find: - (a) \\(P(X > 60)\\) - (b) \\(P(40 < X < 55)\\)","title":"Problem 4"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#problem-5","text":"Random variables \\(X\\) and \\(Y\\) have \\(E[X] = 3\\) , \\(E[Y] = 5\\) , \\(E[X^2] = 13\\) , \\(E[Y^2] = 30\\) , and \\(E[XY] = 16\\) . Compute \\(\\text{Cov}(X,Y)\\) , \\(\\text{Var}(X)\\) , \\(\\text{Var}(Y)\\) , and the correlation \\(\\rho_{XY}\\) .","title":"Problem 5"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#problem-6","text":"Consider the continuous random variable \\(X\\) with PDF: \\[f(x) = \\begin{cases} cx^2 & \\text{if } 0 \\leq x \\leq 2 \\\\ 0 & \\text{otherwise} \\end{cases}\\] (a) Find the constant \\(c\\) so that \\(f\\) is a valid PDF. (b) Compute \\(E[X]\\) . (c) Compute \\(\\text{Var}(X)\\) .","title":"Problem 6"},{"location":"tutorials/Tutorial_06_Probability_Distributions/#solutions","text":"Solution 1: Use the product rule (chain rule) for drawing without replacement. \\[P(\\text{both red}) = P(R_1) \\cdot P(R_2 \\mid R_1) = \\frac{5}{8} \\cdot \\frac{4}{7} = \\frac{20}{56} = \\frac{5}{14} \\approx 0.3571\\] Solution 2: Apply Bayes' theorem. Let \\(A\\) = \"from Machine A,\" \\(B\\) = \"from Machine B,\" and \\(D\\) = \"defective.\" \\(P(A) = 0.6\\) , \\(P(B) = 0.4\\) \\(P(D \\mid A) = 0.03\\) , \\(P(D \\mid B) = 0.05\\) First, compute \\(P(D)\\) using the law of total probability: \\[P(D) = P(D \\mid A)P(A) + P(D \\mid B)P(B) = (0.03)(0.6) + (0.05)(0.4) = 0.018 + 0.02 = 0.038\\] Then: \\[P(A \\mid D) = \\frac{P(D \\mid A) P(A)}{P(D)} = \\frac{(0.03)(0.6)}{0.038} = \\frac{0.018}{0.038} \\approx 0.4737\\] There is about a 47.4% chance the defective item came from Machine A. Solution 3: \\(X \\sim \\text{Binomial}(8, 0.3)\\) . \\[P(X = 2) = \\binom{8}{2}(0.3)^2(0.7)^6 = 28 \\cdot 0.09 \\cdot 0.117649 = 28 \\cdot 0.01058841 \\approx 0.2965\\] \\[E[X] = np = 8 \\times 0.3 = 2.4\\] \\[\\text{Var}(X) = np(1-p) = 8 \\times 0.3 \\times 0.7 = 1.68\\] Solution 4: \\(X \\sim \\mathcal{N}(50, 25)\\) , so \\(\\mu = 50\\) and \\(\\sigma = 5\\) . (a) Standardize: \\[Z = \\frac{60 - 50}{5} = 2.0\\] \\[P(X > 60) = P(Z > 2.0) = 1 - \\Phi(2.0) \\approx 1 - 0.9772 = 0.0228\\] About 2.3% of the distribution lies above 60. (b) Standardize both bounds: \\[Z_1 = \\frac{40 - 50}{5} = -2.0, \\quad Z_2 = \\frac{55 - 50}{5} = 1.0\\] \\[P(40 < X < 55) = \\Phi(1.0) - \\Phi(-2.0) \\approx 0.8413 - 0.0228 = 0.8185\\] About 81.9% of the distribution falls between 40 and 55. Solution 5: Covariance: \\[\\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = 16 - (3)(5) = 16 - 15 = 1\\] Variance of \\(X\\) : \\[\\text{Var}(X) = E[X^2] - (E[X])^2 = 13 - 9 = 4 \\quad \\Rightarrow \\quad \\sigma_X = 2\\] Variance of \\(Y\\) : \\[\\text{Var}(Y) = E[Y^2] - (E[Y])^2 = 30 - 25 = 5 \\quad \\Rightarrow \\quad \\sigma_Y = \\sqrt{5}\\] Correlation: \\[\\rho_{XY} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{1}{2\\sqrt{5}} = \\frac{1}{2\\sqrt{5}} \\cdot \\frac{\\sqrt{5}}{\\sqrt{5}} = \\frac{\\sqrt{5}}{10} \\approx 0.2236\\] A moderate positive linear association. Solution 6: (a) For \\(f\\) to be a valid PDF, the total area must equal 1: \\[\\int_0^2 cx^2 \\, dx = c \\left[\\frac{x^3}{3}\\right]_0^2 = c \\cdot \\frac{8}{3} = 1\\] \\[c = \\frac{3}{8}\\] (b) Expected value: \\[E[X] = \\int_0^2 x \\cdot \\frac{3}{8}x^2 \\, dx = \\frac{3}{8}\\int_0^2 x^3 \\, dx = \\frac{3}{8}\\left[\\frac{x^4}{4}\\right]_0^2 = \\frac{3}{8} \\cdot \\frac{16}{4} = \\frac{3}{8} \\cdot 4 = \\frac{3}{2} = 1.5\\] (c) First compute \\(E[X^2]\\) : \\[E[X^2] = \\int_0^2 x^2 \\cdot \\frac{3}{8}x^2 \\, dx = \\frac{3}{8}\\int_0^2 x^4 \\, dx = \\frac{3}{8}\\left[\\frac{x^5}{5}\\right]_0^2 = \\frac{3}{8} \\cdot \\frac{32}{5} = \\frac{96}{40} = \\frac{12}{5} = 2.4\\] Then apply the variance shortcut: \\[\\text{Var}(X) = E[X^2] - (E[X])^2 = 2.4 - (1.5)^2 = 2.4 - 2.25 = 0.15\\] Equivalently, \\(\\text{Var}(X) = \\frac{3}{20}\\) . Course: Mathematics for Machine Learning Instructor: Mohammed Alnemari Previous: Tutorial 4 - Matrix Decompositions Next: Tutorial 6 - Optimization and Gradient Descent","title":"Solutions"}]}